%
%  ███████╗██╗   ██╗███╗   ███╗███╗   ███╗███████╗████████╗██████╗ ██╗ ██████╗
%  ██╔════╝╚██╗ ██╔╝████╗ ████║████╗ ████║██╔════╝╚══██╔══╝██╔══██╗██║██╔════╝
%  ███████╗ ╚████╔╝ ██╔████╔██║██╔████╔██║█████╗     ██║   ██████╔╝██║██║     
%  ╚════██║  ╚██╔╝  ██║╚██╔╝██║██║╚██╔╝██║██╔══╝     ██║   ██╔══██╗██║██║     
%  ███████║   ██║   ██║ ╚═╝ ██║██║ ╚═╝ ██║███████╗   ██║   ██║  ██║██║╚██████╗
%  ╚══════╝   ╚═╝   ╚═╝     ╚═╝╚═╝     ╚═╝╚══════╝   ╚═╝   ╚═╝  ╚═╝╚═╝ ╚═════╝
%
%  ███████╗██╗   ██╗███╗   ██╗ ██████╗████████╗██╗ ██████╗ ███╗   ██╗███████╗ 
%  ██╔════╝██║   ██║████╗  ██║██╔════╝╚══██╔══╝██║██╔═══██╗████╗  ██║██╔════╝ 
%  █████╗  ██║   ██║██╔██╗ ██║██║        ██║   ██║██║   ██║██╔██╗ ██║███████╗ 
%  ██╔══╝  ██║   ██║██║╚██╗██║██║        ██║   ██║██║   ██║██║╚██╗██║╚════██║ 
%  ██║     ╚██████╔╝██║ ╚████║╚██████╗   ██║   ██║╚██████╔╝██║ ╚████║███████║ 
%  ╚═╝      ╚═════╝ ╚═╝  ╚═══╝ ╚═════╝   ╚═╝   ╚═╝ ╚═════╝ ╚═╝  ╚═══╝╚══════╝ 
%
% Welcome! This is the FIRST .tex file of this scale I've ever done, so please
% excuse any newbie mistakes.


% ===== begin preamble =========================================================
\documentclass{article}


% ----- begin includes ---------------------------------------------------------
% See https://github.com/Jasper-Ty/dotfiles
\usepackage[garamond, tableau]{jaspercommon}
\usepackage{algorithm2e}
\usetikzlibrary{arrows.meta}
\setcounter{MaxMatrixCols}{20}

% ----- end includes -----------------------------------------------------------


% ----- begin newcommands ------------------------------------------------------
% Environments 
\barenv[][subsection]{theorem}{Theorem}
\barenv[bartheorem]{lemma}{Lemma}
\barenv[bartheorem]{corollary}{Corollary}
\barenv[bartheorem]{proposition}{Proposition}
\barenv[bartheorem]{definition}{Definition}
\barenv[bartheorem]{example}{Example}
\barenv[bartheorem]{remark}{Remark}
\barenv[bartheorem]{convention}{Convention}

\SetKw{KwAnd}{and}

% Formal Power Series
\newcommand{\fps}[2]{#1 [\kern-1.2pt[ {#2} ]\kern-1.2pt]}
% Polynomial Ring
\newcommand{\pring}[2]{#1 [ {#2} ]}
% Big Comma (yes, a big comma)
\newcommand{\bigcomma}{\ensuremath{\mathop{\scalebox{3}{\:,\:}}\limits}}
% Inner product
\newcommand{\InnerProduct}[1]{
    \ensuremath{
        \left\langle
            {#1}
        \right\rangle
    }
}
\newcommand{\IP}[1]{\InnerProduct{#1}}

% Robinson-Schensted-Knuth correspondence
\DeclareMathOperator{\RSK}{RSK}
\DeclareMathOperator{\RowInsert}{RowInsert}

% Type
\DeclareMathOperator*\Maybe{Maybe}
\DeclareMathOperator*\Just{Just}
\DeclareMathOperator*\Nothing{Nothing}

% Basic operator names
\DeclareMathOperator{\row}{row}
\DeclareMathOperator{\col}{col}
\DeclareMathOperator{\size}{size}
\DeclareMathOperator{\shape}{shape}
\DeclareMathOperator{\sh}{sh}
\DeclareMathOperator{\content}{content}
\DeclareMathOperator{\ct}{ct}
\DeclareMathOperator{\len}{len}
\DeclareMathOperator{\opht}{ht}
\DeclareMathOperator{\opheight}{height}
\DeclareMathOperator{\type}{type}
\DeclareMathOperator{\rowword}{rowword}
\DeclareMathOperator{\sgn}{sgn}
\DeclareMathOperator{\sort}{sort}

% The set of all partitions
\DeclareMathOperator{\Par}{Par}
% The set of all compositions 
\DeclareMathOperator{\Com}{Comp}
% The set of Young tableaux
\DeclareMathOperator{\YT}{YT}
% The set of standard Young tableaux
\DeclareMathOperator{\SYT}{SYT}
% The set of semistandard Young tableaux
\DeclareMathOperator{\SSYT}{SSYT}
% The set of semistandard Young tableaux
\DeclareMathOperator{\biSSYT}{biSSYT}

\DeclareMathOperator{\Dom}{Dom}
\DeclareMathOperator{\Lex}{Lex}
\DeclareMathOperator{\cyc}{cyc}
\DeclareMathOperator{\tr}{t}
\DeclareMathOperator{\s}{s}
\DeclareMathOperator{\rw}{rw}
\DeclareMathOperator{\oln}{oln}

% The simple reflection r_i
\newcommand*\refl{\ensuremath{\text{r}}}

\DeclareMathOperator{\Mat}{Mat}
\DeclareMathOperator{\alt}{alt}
\DeclareMathOperator{\biword}{biword}
\DeclareMathOperator{\coeff}{coeff}
\DeclareMathOperator{\id}{id}

% The symmetric group
\DeclareMathOperator{\Sym}{Sym}

% Bender-Knuth involution
\DeclareMathOperator{\BK}{BK}

%   RSK
% ------->
\newcommand{\rskarrow}{\ensuremath\xrightarrow{\RSK}}

% ----- end newcommands --------------------------------------------------------


\title{Symmetric functions}
\author{Jasper Ty}
\date{}

\titleauthorhead


% ===== end preamble ==========================================================

\begin{document}

\maketitle

\section*{What is this?}

These are perpetually growing notes based on my self-study of Chapter 7 in Richard P. Stanley's ``Enumerative Combinatorics'', William Fulton's ``Young Tableaux'', Ian G. Macdonald's ``Symmetric Functions and Hall Polynomials'', and various other expositions. 

\iffalse 
I've learned to \textit{love} this subject! At first, I thought ``Functions that remain the same change under interchange of variables? What's so interesting about that?'', but at some point between now and the end of my undergraduate life, I took it on myself to \textit{compute} with these things, to hold them with my bare hands, and lo--- I suddenly found myself baptized in the waters of symmetric polynomials.

I'm not entirely sure how to write for an audience yet, so certain things might be over or under explained, and this might happen all over the place!
I guess, one has to have had some combinatorics, knowing about posets, partitions, coming up with bijections, and so on. 
Also experience with working with formal power series probably helps.

I'll be honest and say algebra is not my strong suit, so I apologize in advance if that manifests particularly clearly in some sections.
\fi


\tableofcontents

\section*{Notation, conventions, some facts}

\iffalse
As much as I hate to admit it, I think about notation and proof minutiae \textit{a lot}.
I had a short ``previous life'' as a software engineer, and I always enjoyed thinking over and over again about how to rewrite code--- and the same goes for proofs.

This is my best attempt at synthesizing notation in this subject that is much more to my taste, but hopefully isn't too idiosyncratic at the same time.
I take some inspiration from Darij Grinberg's algebraic combinatorics lecture notes \cite{DarijAC} and from various other sources.

There's a level of redundancy going on--- often notation will be defined in the parts where they're used despite being already defined here.
This is intentional.
\fi

\addcontentsline{toc}{section}{Notation and conventions}

\subsection*{Distinguished sets}

We take $\NN$ to be the set of natural numbers \textit{including} zero,
\[
    \NN
    \coloneq
    \{0,1,2,\ldots\}.
\]
We take $\PP$ to be the set of \textit{positive integers},
\[
    \PP
    \coloneq
    \{1,2,\ldots\}.
\]
$\ZZ,\QQ,\RR,\CC$ are defined as usual.

We define
\[
    [n]
    \coloneq
    \{1,2,\ldots,n\}.
\]

\subsection*{Lists, tuples}

Generically, \textit{any} tuple will be written in the form
\[
    (a_1, a_2, \ldots, a_n)
\]
I have this crazy notation for lists and sequences--- I put
\[
    \bigcomma_{1\leq i \leq n}a_i
    \coloneq
    a_1,\ldots,a_n.
\]
Often, a tuple will be packed into a symbol, into which we index by appending a subscript, so the above tuple can just be written $a$.

When it is possible (read: unambiguous), we will often elide brackets and commas. 
For example, $(1,2,3,4)$ can be written as $1234$ instead.

Given a tuple $(a_1,\ldots,a_n)$, we will denote \textit{omission} of entries by slashing out the entry.
This means
\[
    a_1,\ldots,\bcancel{a_i},\ldots,a_n
    \coloneq
    a_1,\ldots,a_{i-1},a_{i+1},\ldots a_n.
\]

\subsection*{Sequences, compositions, and partitions}

We will often deal with infinite tuples, which look like
\[
    (a_i)_{i=1}^\infty
    \coloneq
    \left(
        \bigcomma_{i=1}^\infty \alpha_i
    \right)
    =
    (\alpha_1, \alpha_2, \ldots).
\]
We will be loose about infinite and finite tuples--- specifically, finite tuples can always be extended to an infinite tuple padded with infinitely many entries that are some natural ``null'' element for the context.

A \textit{weak composition} $\alpha$ of $n \in \NN$ is an infinite tuple of nonnegative integers 
such that $\sum_i \alpha_i = n$. 
We define $|\alpha| = \sum_i \alpha_i$ to have notation for recovering $n$ given $\alpha$.

When more convenient (most of the time), we will omit brackets and commas, and write compositions as strings of digits. For example we will write $(1,1,4,3)$ as $1143$.

A \textit{partition} $\lambda$ of $n$ is a weak composition whose entries are \textit{weakly decreasing}. 
That a particular partition $\lambda$ is a partition of a particular $n$ is denoted $\lambda \vdash n$. 

I use English notation when drawing diagrams and tableaux, meaning, increasing row index means going north to south, and increasing column index means going east to west.

\subsection*{Rings, polynomials, and formal power series}

All rings considered are commutative and unital. An arbitrary ring will be denoted $\KK$. 

$\pring{\KK}{x}$ will denote the polynomial ring over $\KK$ in the indeterminate $t$, similarly $\fps{\KK}{t}$ will denote the formal power series ring over $\KK$ in the indeterminate $t$.

We will fix notation for sets of indeterminates:

\begin{enumerate}[label=(\alph*)]
    \item $\bfx_N \coloneq (x_1, x_2, \ldots x_N)$ for a set of $N$ indeterminates.
    \item $\bfx_\infty \coloneq (x_1, x_2, \ldots)$ for a set of countably many indeterminates.
    \item $\bfx$ for either the finite or countable case when it is clear from context.
    \item $\bfy$, $\bfy_N$, $\bfy_\infty$, $\bfz$, $\bfz_N$, $\bfz_\infty$, and so on are defined similarly.
\end{enumerate}

With compositions, partitions, or otherwise any finitely supported tuple of nonnegative integers $\alpha$, we define \textit{multi-index notation} for compactly writing down monomials in a set of variables.
\[
    \bfx^\alpha
    \coloneq
    x_1^{\alpha_1}x_2^{\alpha_2}x_3^{\alpha_3}\cdots.
\]
As a notation in between, we will also write
\[
    \bfx^{\alpha_1,\alpha_2,\ldots}
\]
for the monomial $\bfx^\alpha$.
This allows us to ``anonymously'' use multi-indices.

In the context of multi-index notation, $\alpha$ will also be called $\bfx$'s \textit{exponent vector}.
We will let $[\bfx^\alpha]f$ denote the coefficient of $[\bfx^\alpha]$ in the formal power series $f$.
Sometimes, this is written as $\InnerProduct{f,\bfx^\alpha}$.

\subsection*{Permutations and the symmetric group}

$S_n$ will denote the symmetric group on $n$ letters.
In general, $S_A$ will denote the group of permutations of the set $A$.
In this case, we have defined $S_n \coloneq S_{[n]} = S_{\{1,2,\ldots,n\}}$.

I will use the standard one-line and two-line notation, generally, for permutations.

I use the $\cyc_{a_1a_2\ldots a_k}$ to refer to a cycle that sends $a_1$ to $a_2$, $a_2$ to $a_3$, and so on. 
As an example, the cycle that sends $1$ to $7$, $7$ to $4$, and $4$ to $1$ will be written as $\cyc_{174}$.
Commas will be introduced and elided as appropriate.

A transposition that swaps $i$ and $j$ will be denoted $\tr_{ij}$.
For example, the transposition that swaps 4 and 8 will be written as $\tr_{48}$.

The simple transpositions $\tr_{i,i+1}$ will be denoted $\refl_i$.

Permutations will act on functions (somewhat incorrectly!) by permuting places, so if $w \in S_n$ and $f$ is a function of $n$ variables, then
\[
    wf(x_1,\ldots,x_n) \coloneq f(x_{w(1)},\ldots,x_{w(n)}).
\]

\hide{
    The \textit{one-line notation} of a permutation will be denoted $\underline{a_1\cdots a_n}_{\pi}$

    \[
        \underline{a_1 \cdots a_n}_{\text{w}}
    \]
}


\subsection*{Iverson brackets}

I use the \defstyle{Iverson bracket}, which is defined to be
\[
    [\psi]^? \coloneq \begin{cases}
        1 & \psi\text{ is true} \\
        0 & \text{otherwise}
    \end{cases}
\]
for any statement $\psi$ that can be true or false.
I add the question mark, since square brackets are \textit{terribly overloaded} as it is, and because it's pointless to exponentiate Iverson brackets anyway--- there's no mistaking what it's for.

It's an important remark that the Iverson bracket is a \textit{function of the free variables in $\psi$}.

\section{Symmetric functions}

\subsection{Symmetric polynomials}

We first define \textit{symmetric polynomials}.

\begin{definition}
    Fix $n \in \NN$, and let $\KK$ be a commutative ring.
    We call $f \in \pring{\KK}{x_1,\ldots,x_n}$ a \defstyle{symmetric polynomial} if, for all permutations $w \in S_n$, we have that
    \[
        wf = f.
    \]
    That is, if
    \[
        f(x_{w(1)},x_{w(2)},\ldots,x_{w(n)})
        =
        f(x_1,x_2,\ldots,x_n)
    \]
    for all $w \in S_n$.

    We will denote the set of all such polynomials $\pring{\KK}{\bfx_n}^{S_n} = \pring{\KK}{x_1,\ldots,x_n}^{S_n}$.
\end{definition}

This is a specific case of a more general idea of the \defstyle{ring of invariants} of a group action on a ring $G \curvearrowright R$, which is denoted $R^G$.

\begin{example}
    The polynomial
    \[
        p(x,y)
        =
        x + y 
    \]
    is symmetric, and is an element of $\pring{\KK}{x,y}$.
\end{example}

\begin{remark}
    The sum and product of two symmetric polynomials is again symmetric.
    Also, $0$ and $1$ are clearly symmetric.
    Hence, $\pring{\KK}{\bfx}^{S_n}$ is a \textit{subring} of $\pring{\KK}{\bfx}$.
\end{remark}

\subsection{Formal power series in infinitely many variables}

Next, we want to give a formal definition for ``the ring of symmetric functions'', which we will call $\Lambda$.

More precisely, we construct the ring of symmetric functions as a certain subring of formal power series in infinitely many variables.
The reason for this mouthful is that many interesting families of symmetric polynomials typically enjoy a \textit{stability} property: that
\[
    \sfk^{(n)}(x_1,\ldots,x_n)
    =
    \sfk^{(n+1)}(x_1,\ldots,x_n, 0),
\]
where $\sfk^{(n)} \in \pring{\KK}{\bfx_n}^{S_n}$ and $\sfk^{(n+1)} \in \pring{\KK}{\bfx_{n+1}}^{S_n}$ are meant to be defined in the ``same'' way, differing only in the number of variables.

This motivates the idea that there must be \textit{some} limit $\sfk(\bfx_\infty)$ which $\sfk^{(n)}$ stabilizes to.
Unfortunately, this means we \textit{have} to do some paperwork with regards to where this object $\sfk$ lives.

\begin{definition}
    Fix a ground ring $\KK$.
    The ring $\fps{\KK}{\bfx_\infty}$ of \defstyle{formal power series in countably many variables} $\bfx_\infty \coloneq (x_1, x_2, \ldots)$ is defined to be the set of all formal linear combinations 
    \[
        \sum_{\alpha\in\Com} c_\alpha \bfx^\alpha,
    \]
    where $c_a \in \KK$, and $\Com$ is the set of all weak compositions, or equivalently, all finitely supported $\NN$-sequences, and where $\bfx^\alpha \coloneq \bfx_\infty^\alpha = x_1^{\alpha_1}x_2^{\alpha_2}\cdots$.

    The ring operations on are defined in the morally correct way:
    if $f = \sum_\alpha a_\alpha \bfx^\alpha$ and $g = \sum_\alpha b_\alpha \bfx^\alpha$, then $f + g$ and $fg$ are defined to be
    \[
        f+g
        \coloneq
        \sum_\alpha
        (a_\alpha+b_\alpha)\bfx^\alpha
    \]
    and
    \[
        fg
        \coloneq
        \sum_\gamma \prod_{\alpha+\beta=\gamma}
        (a_\alpha b_\beta) \bfx^\gamma.
    \]
\end{definition}

Indexing over all \textit{weak compositions} $\alpha$ means that all the monomials that appear are ``honest'', and this makes the definition work nicely with our existing intuition for working with formal power series.

\begin{example}
    The formal power series
    \[
        f(\bfx)
        =
        x_1 + x_2 + x_3 + \cdots
    \]
    is an element of $\fps{\KK}{\bfx_\infty}$.
\end{example}

\subsection{Homogeneous components}

To begin our construction, we will begin by taking a ``slice'' of the full ring.

\begin{definition}
    A \defstyle{homogeneous symmetric function of degree $n$} over a ring $\KK$ is a formal power series
    \[
        \sum_{\alpha \in \Com_n}
        c_\alpha \bfx^\alpha
        \in \fps{\KK}{\bfx_\infty},
    \]
    where $\Com_n$ means we are summing over all weak compositions $\alpha$ of $n$, and every $c_\alpha$ is a scalar such that $c_\alpha = c_\beta$ whenever $\beta$ can be obtained by permuting the parts of $\alpha$.
    
    We denote the set of all such formal power series by $\Lambda_\KK^n$.
\end{definition}

These form a $\KK$-module, as a submodule of $\fps{\KK}{\bfx_\infty}$.
Moreover, these are in fact defined correctly, meaning that these are symmetric ``functions''.

\begin{definition}
    Let $S_\infty$ denote the subgroup of $S_\NN$ consisting of permutations of $\NN$ with ``finite support''.
    That is,
    \[
        S_\infty
        \coloneq
        \Big\{
            w \in S_\NN: w(t) = t\text{ for all but finitely many }t
        \Big\}.
    \]
\end{definition}

This contains as a subgroup $S_n$ for all $n \in \NN$.
Importantly, every permutation in $S_\infty$ is an extension of a permutation that lives in some finite symmetric group.

\begin{remark}
    Consider the action $S_\infty \curvearrowright \fps{\KK}{\bfx_\infty}$ given by
    \[
        wf(x_1,x_2,\ldots)
        =
        f(x_{w(1)},x_{w(2)},\ldots), \quad \forall w \in S_\infty.
    \]
    Equivalently, the action is also given by corresponding each $w \in S_\infty$ to the automorphism of $\fps{\KK}{\bfx_\infty}$ which replaces each indeterminate $x_i$ with $x_{w(i)}$.

    $\Lambda_{\KK}^n$ is precisely all the elements of $\fps{\KK}{\bfx_\infty}$ invariant under $S_\infty$.
\end{remark}

The following is a simple example of an element of $\Lambda_\KK^n$:

\begin{example}
    The formal power series 
    \[
        f(\bfx_\infty)
        =
        \sum_i
        x_i^2 
        + 10\sum_{i < j}
        x_ix_j
    \]
    is a symmetric function that is homogeneous of degree $2$. In this case $c_\alpha = 1$ whenever $\alpha = (0,\ldots,0, 2, 0,\ldots)$, and $c_\alpha = 10$ whenever $\alpha = (0,\ldots, 0, 1, 0, \ldots, 0, 1, 0, \ldots)$.
    In every other case, $c_\alpha = 0$.
\end{example}

\subsection{The ring $\Lambda$ of symmetric functions}

We note that multiplying (inside $\fps{\KK}{\bfx_\infty}$) any two homogeneous symmetric functions $f$, $g$ of degrees $m$ and $n$ respectively give us a homogeneous symmetric function of degree $m+n$.

This allows us to finally define $\Lambda$ as the following graded algebra:

\begin{definition}
    The \defstyle{ring of symmetric functions} $\Lambda_\KK$ is the infinite direct sum
    \[
        \Lambda_\KK \coloneq \Lambda_\KK^0 \oplus\Lambda_\KK^1 \oplus \cdots.
    \]
    In the case when $\KK = \QQ$, we will suppress the subscript and refer to $\Lambda_\QQ^n$ and $\Lambda_\QQ$ as $\Lambda^n$ and $\Lambda$ respectively.
\end{definition}

This greatly broadens the possible definitions for symmetric functions. 
For example, the following demonstrates a symmetric function that arises from an infinite product, which evidently contains many monomials of different degrees and is not at all homogeneous.

\begin{example}
    The formal power series 
    \[
        f = \prod_{i\geq1} (1 + 3x_i^2 + 7x_i^5)
    \]
    is a symmetric function.
\end{example}

\section{Partitions, compositions and tableaux}

\subsection{Definitions}

A partition is, as it's well known, just a way of writing down $n$ as a sum of positive integers.
And, a composition is an integer partition in which we care about the particular order the positive integers are summed.

We give formalizations of these ideas that are convenient in developing the theory of symmetric functions.

\begin{definition}
    A \defstyle{weak composition}, which we will often refer to simply as a \textit{composition}, $\alpha$ of $n \in \NN$ is a sequence of nonnegative integers 
    \[
        (\alpha_1,\alpha_2,\ldots)
    \]
    such that 
    \[
        |\lambda|
        \coloneq
        \sum_{i\in\NN}\alpha
        =
        n.
    \]
    The nonzero entries of $\alpha$ are called the \defstyle{parts} of $\alpha$.
\end{definition}

A partition can be seen as an \textit{equivalence class} of a certain class of compositions--- namely those which have the same multiset of parts.
One can explicitly compute a representative simply by sorting the parts of a composition.
This is what the following definition reflects.

\begin{definition}
    A \defstyle{partition}, also sometimes called an \defstyle{integer partition}, $\lambda$ of $n \in \NN$ is a sequence of nonnegative integers 
    \[
        (\lambda_1,\lambda_2,\ldots)
    \]
    such that 
    \[
        |\lambda| = \sum_{i\in\NN}\lambda_i = n
    \]
    and 
    \[
        \lambda_j \leq \lambda_k
    \]
    for all $j\geq k$.

    We say that $n$ is the \defstyle{size} of $\lambda$.
    The \defstyle{parts} of $\lambda$ are the nonzero entries of $\lambda$.
    The \defstyle{length} of a partition $\lambda$, $\len \lambda$ or $\ell(\lambda)$, is the number of nonzero parts of $\lambda$.
\end{definition}

Put yet another way, $\lambda_k$ is a composition that is weakly decreasing and has only finitely many nonzero entries.

We put notation for talking about partitions and compositions.

\begin{convention}
    We write $\lambda \vdash n$ to say ``$\lambda$ is a partition of $n$.''
    Similarly, we will write $\alpha \models n$ to say ``$\alpha$ is a composition of $n$.''

    We denote the set of all partitions by $\Par$.
    The set of all partitions $\lambda$ such that $\lambda \vdash n$ will be denoted $\Par(n)$.
    The set of all partitions with $n$ parts will be denoted $\Par_{n}$.

    We denote the set of all compositions by $\Com$.
    If $\lambda \in \Par$, we denote the set of all compositions with parts $\lambda$ to be $\Com(\lambda)$.

\end{convention}


\begin{example}
    The sequence
    \[
        54432111
    \]
    is a partition of $21$.
\end{example}


\subsection{Diagrams}
Partitions can be drawn as \defstyle{Ferrers diagrams} and \defstyle{Young diagrams}. 

Both have the same underlying data structure: they encode the partition $\lambda$ as a subset of $\NN^2$, with a particularly simple definition:

\begin{definition}
    Let $\lambda \vdash n$. The \defstyle{diagram} of $\lambda$, which we will denote $\boxplus \lambda$, is the set
    \[
        \boxplus \lambda \coloneq \{(i,j) \in \NN^2 : 1 \leq j \leq \lambda_i\}.
    \]
\end{definition}

Then, we can define Young and Ferrers diagrams.

\begin{definition}
    Let $\lambda \vdash n$.
    The \defstyle{Young diagram} of $\lambda$ is obtained by drawing a box at location $(i,j)$ for each $(i,j)$ in $\boxplus\lambda$.
    Similarly, its \defstyle{diagram} is obtained by plotting dots rather than boxes.
\end{definition}

\newpage

\begin{example}
    The Young diagram of the partition $\lambda = 54432111$ is
    \[
        \ydiagram{5,4,4,3,2,1,1,1}.
    \]
\end{example}
\begin{example}
    The Ferrers diagram of the partition $\lambda = 54432111$ is
    \[
        \begin{matrix}
            . & . & . & . & . \\
            . & . & . & . &  \\
            . & . & . &   &  \\
            . & . &   &   &  \\
            . &   &   &   &  \\
            . &   &   &   &  \\
            . &   &   &   & 
        \end{matrix}.
    \]
\end{example}

\subsection{Tableaux}

The fact that Young diagrams are made up of boxes is nice--- because we can put things in the boxes. 

\begin{definition}
    Let $\lambda \vdash n$. A Young diagram of $\lambda$ whose boxes are filled in with elements from a set is called a \defstyle{Young tableau}, which will often be denoted with a capital letter, say $T$. 

    Formally, it is a function $\boxplus \lambda \to X$, where $X$ is some set.

    $\lambda$ is referred to as the \defstyle{shape} of the tableau, denoted $\shape T$ or $\sh T$.

    We will define $\boxplus T \coloneq \boxplus \sh T$ as an easy unambiguous shorthand.

    The elements filled in are called the \defstyle{entries} of the tableau, and the entry at box $(i,j)$ is indexed as $T_{ij}$.
\end{definition}

\begin{example}
    The following is a Young tableau, filled in with positive integers:
    \[
        \ytableaushort{123,45}
    \]
\end{example}

Typically, our entries will either be positive integers or elements of a family indexed by positive integers.

This allows us to encode the entries in a convenient way

\begin{definition}
    Let $T$ be a Young tableau filled in with \textit{positive integers}.
    Consider the multiset $\{T_{ij}\}_{ij \in \boxplus T}$ of all its entries, counted with multiplicities.
    The \defstyle{content} of $T$, written $\content T$ or $\ct T$, is the sequence defined to be
    \[
        \content T
        \coloneq
        \left(
            \bigcomma_{n \in \NN}
            \text{multiplicity of }n\text{ in }T\text{'s entries}
        \right).
    \]
    This is also sometimes referred to as the \defstyle{weight} of $T$.
\end{definition}

\begin{example}
    The Young tableau
    \[
        \ytableaushort{1112,33,4}
    \]
    has content $3121$, as its entries consist of three $1$'s, one $2$, two $3$'s and one $4$.
\end{example}

More formally, Young tableau are functions whose domain is a partition's diagram.
A partition's diagram has an order induced on it by being a subset of $\NN^2$--- the product order on $\NN^2$.
If the entries are filled in with something that is also ordered, which in our case is almost always $\NN$, the order on the boxes can interact with the order of the entries in several ways.

\begin{definition}
    We define a few important constraints on a Young tableau $T$.
    \begin{enumerate}[label=(\alph*)]
        \item That the \textit{rows} of $T$ are \defstyle{weakly increasing} means that
            \[
                T_{ni} \leq T_{nj} \text{ whenever } i < j \text{ for all }n.
            \]
        \item That the \textit{columns} of $T$ are \defstyle{weakly increasing} means that
            \[
                T_{im} \leq T_{jm} \text{ whenever } i < j \text{ for all }m.
            \]
        \item That the \textit{rows} of $T$ are \defstyle{strongly increasing} means that
            \[
                T_{ni} < T_{nj} \text{ whenever } i < j \text{ for all }n.
            \]
        \item That the \textit{columns} of $T$ are \defstyle{strongly increasing} means that
            \[
                T_{im} < T_{jm} \text{ whenever } i < j \text{ for all }m.
            \]
    \end{enumerate}
\end{definition} 

These have really obvious meanings on the level of ``filling numbers in boxes''.

With this, we can define two important classes of Young tableau.

\begin{definition}
    A Young tableau $T$ is called \defstyle{standard} if both its rows and columns are strongly increasing. $T$ is called \defstyle{semistandard} if its rows are weakly increasing and its columns are strongly increasing.

    We will denote the set of \textit{all} standard and semistandard Young tableaux by $\SYT$ and $\SSYT$ respectively.
\end{definition}

Semistandard Young tableau are explored in more detail in Section \ref{ch:schurs}, as the \textit{Schur functions} enumerate them.

\subsection{Orders on partitions}

We have several orders on \textit{partitions themselves}.
The first one, \textit{containment}, is defined on all partitions.

\begin{definition}
    Young diagrams, as subsets of $\NN^2$, have a partial order induced by the subset relation. 
    \defstyle{Containment order} for partitions is precisely this order that diagrams induce on partitions.

    We will use $\subseteq$ to denote this order, so we have defined
    \[
        \lambda \subseteq \mu \text{ whenever } \boxplus \lambda \subseteq \boxplus \mu.
    \]
\end{definition}

Containment order in fact induces a lattice, a sublattice of $\NN^2$'s powerset, once it's checked that $\Par$ is closed under union and intersection. 

\begin{remark}
    $\Par$, endowed with containment order, has a lattice structure called \textit{Young's lattice}.
\end{remark}

In particular, it can be characterized as follows:

\begin{theorem}
    Young's lattice is the lattice of order ideals of $\NN^2$, i.e, Young's lattice is $J(\NN^2)$.
\end{theorem}

\begin{proof}[Proof (sketch)]
    Every finite order ideal of $\NN^2$ is the Young diagram of a partition--- it's impossible, reading north to south, to have a row longer than the row above it, since that would violate the order ideal property.

    Every partition, as a Young diagram, is an order ideal of $\NN^2$.
    Explicitly, it is the order ideal generated by the ``outer corners'' of the partition.

    Since these two constructions specify the exact same subsets of $\NN^2$, they agree when taking unions and intersections, and so they both specify the same sublattice of $\mathscr{P}(\NN^2)$.
\end{proof}

\begin{theorem}
    Standard Young tableaux are in bijection with saturated chains in Young's lattice that begin at $\varnothing$.
\end{theorem}

\begin{proof}[Proof (sketch)]
    Construct the chain by adding one box at a time to $\varnothing$, specifically in the order the entries of a partition appear.
\end{proof}

Containment is sensitive to partition size--- for $\lambda \subseteq \mu$ it's necessary that $|\lambda| \leq |\mu|$.
Even more sharply, all partitions of a fixed size are incomparable in Young's lattice!

The next two orders are not so sensitive to a partition's size, because they are defined not from viewing partitions as subsets of $\NN^2$, but viewing them as integer sequences.

First, we have a \textit{partial order} on partitions.

\begin{definition}
    Let $\lambda$ and $\nu$ be two partitions. We say that $\lambda$ \defstyle{dominates} or \defstyle{majorizes} $\nu$ if
    \[
        \sum_{k=1}^i \lambda_k \geq \sum_{k=1}^i \nu_k \qquad \forall i\in\NN.
    \]
    We denote this relation $\lambda \leq \nu$, and we call $\leq$ the \defstyle{dominance order} on $\Par$.
\end{definition}

Next, we have a \textit{total order} on partitions.

\begin{definition}
    Let $\lambda, \mu \in \Par$, and suppose $\lambda \neq \mu$.
    Let $m$ be the first index for which $\lambda$ and $\mu$ differ.
    We say that $\lambda \succ \mu$ if $\lambda_m > \mu_m$, and $\lambda \prec \mu$ if $\lambda_m < \mu_m$.
    This is the \defstyle{lexicographic order} on $\Par$.
\end{definition}

Not only is it a total order, it is actually a linear extension of dominance order.

\begin{theorem}
    If $\lambda \geq \mu$, then $\lambda \succeq \mu$.
\end{theorem}

\begin{proof}
    Let $\lambda, \mu \in \Par$, such that $\lambda \neq \mu$.
    Let $m$ be the first index for which $\lambda$ and $\mu$ disagree.
    This means that
    \[
        \sum_{k=1}^{m-1} \lambda_k = \sum_{k=1}^{m-1} \mu_k.
    \]
    If $\lambda > \mu$, it \textit{must be} that $\lambda_m > \mu_m$, since if $\lambda_m < \mu_m$, we have that
    \[
        \sum_{k=1}^m \lambda_k = \sum_{k=1}^{m-1} \lambda_k + \lambda_m < \sum_{k=1}^{m-1} \mu_k + \mu_m = \sum_{k=1}^m \mu_k,
    \]
    contradicting the definition of dominance order.
    Now, $\lambda_m > \mu_m$ tells us that $\lambda \succ \mu$.
\end{proof}

\subsection{Partition transposition}

\begin{definition}
    Let $\lambda \in \Par$.
    We define its \defstyle{transpose} $\lambda^\top$ to be the partition
    \[
        \lambda^\top
        \coloneq
        \left(
            \bigcomma_{i \in \NN}
            \# \text{ parts of }\lambda\text{ with size }\leq i
        \right)
    \]
    This is also sometimes called $\lambda$'s \defstyle{conjugate}, and sometimes denoted $\lambda^\ast$, $\lambda'$, or $\lambda^T$.
\end{definition}

\begin{example}
    The transpose of the partition $\lambda = 54432111$ is
    \[
        \ydiagram{8,5,4,3,1},
    \]
    so $\lambda^\top = 85431$.
\end{example}

\begin{proposition}
    Fix $m \in \NN$.
    Partition transposition is involutory, and moreover a bijection between partitions of length $m$ and partitions whose largest part is $m$.
\end{proposition}

\begin{proof}
    \todo{Prove partition transposition}
\end{proof}

\section{Bases}

We cover some key bases of the ring of symmetric functions. These are \textit{all} indexed by partitions.

\subsection{Monomial symmetric functions}

This first basis has the property where it's \textit{immediately obvious the fact that it even is a basis}.

This is in total analogy to taking the \textit{monomial basis of a polynomial ring}. In this case, we group together monomials by the orbits of the variable permuting $S_n$ action.

\begin{definition}
    Let $\lambda \in \Par$. The \defstyle{monomial symmetric function} $m_\lambda$ is 
    \[
        m_\lambda \coloneq \sum_{\alpha \sim \lambda} \bfx^\alpha.
    \]
    Where $\alpha \sim \lambda$ means that $\alpha$ may be obtained by permuting the parts of $\lambda$.
\end{definition}

The above definition involves permuting around the exponent vector $\alpha$. 
I personally find it easier to think of the monomial symmetric functions as \textit{permuting the subscripts}. 

\begin{example}
    Let $\lambda = 5322$. Then
    \begin{align*}
        m_\lambda = m_{5322} = \sum_{i_1<i_2<i_3<i_4} \Big(&x_{i_1}^5x_{i_2}^3x_{i_3}^2x_{i_4}^2 + x_{i_1}^5x_{i_2}^2x_{i_3}^3x_{i_4}^2 + x_{i_1}^5x_{i_2}^2x_{i_3}^2x_{i_4}^3 \\ 
            + &x_{i_1}^2x_{i_2}^5x_{i_3}^3x_{i_4}^2 + x_{i_1}^2x_{i_2}^5x_{i_3}^2x_{i_4}^3 + x_{i_1}^2x_{i_2}^2x_{i_3}^5x_{i_4}^3  \\
            + &x_{i_1}^3x_{i_2}^5x_{i_3}^2x_{i_4}^2 + x_{i_1}^3x_{i_2}^2x_{i_3}^5x_{i_4}^2 + x_{i_1}^3x_{i_2}^2x_{i_3}^2x_{i_4}^5  \\
        + &x_{i_1}^2x_{i_2}^3x_{i_3}^5x_{i_4}^2 + x_{i_1}^2x_{i_2}^3x_{i_3}^2x_{i_4}^5 + x_{i_1}^2x_{i_2}^2x_{i_3}^3x_{i_4}^5 \Big).
    \end{align*}
    when you view the action of $S_n$ as permuting the exponents. When viewed as permuting the subscripts, we have that
    \begin{align*}
        m_\lambda = m_{5322} = \sum_{i_1<i_2<i_3<i_4} \Big(
        &x_{i_1}^5x_{i_2}^3x_{i_3}^2x_{i_4}^2
        + x_{i_1}^5x_{i_3}^3x_{i_2}^2x_{i_4}^2
        + x_{i_1}^5x_{i_4}^3x_{i_2}^2x_{i_3}^2 \\
        + &x_{i_2}^5x_{i_1}^3x_{i_3}^2x_{i_4}^2
        + x_{i_2}^5x_{i_3}^3x_{i_1}^2x_{i_4}^2
        + x_{i_2}^5x_{i_4}^3x_{i_1}^2x_{i_3}^2 \\
        + &x_{i_3}^5x_{i_1}^3x_{i_2}^2x_{i_4}^2
        + x_{i_3}^5x_{i_2}^3x_{i_1}^2x_{i_4}^2
        + x_{i_3}^5x_{i_4}^3x_{i_2}^2x_{i_4}^2 \\
        + &x_{i_4}^5x_{i_1}^3x_{i_2}^2x_{i_3}^2
        + x_{i_4}^5x_{i_2}^3x_{i_1}^2x_{i_3}^2
        + x_{i_4}^5x_{i_3}^3x_{i_1}^2x_{i_2}^2
    \Big).
    \end{align*}
\end{example}

The following theorem has to be stated for thoroughness's sake.

\begin{theorem}
    The monomial symmetric funtions form a basis for $\Lambda$.
\end{theorem}

\begin{proof}
    It's impossible to form a nontrivial linear combination of monomial symmetric functions that sum to zero.
\end{proof}

\subsection{Elementary symmetric functions}

Our next basis will be the \textit{elementary symmetric functions}, which we will refer to as the \defstyle{elementaries} or the $e$'s.

\begin{definition}
    Let $n\in\NN$. 
    The \defstyle{elementary symmetric function} $e_n$ is defined to be
    \[
        e_n 
        \coloneq 
        \sum_{i_1<i_2<\ldots<i_n} x_{i_1}x_{i_2}\cdots x_{i_n}.
    \]
    And if we let $\lambda \in \Par$, $e_\lambda$ is defined to be
    \[
        e_\lambda 
        \coloneq 
        e_{\lambda_1}e_{\lambda_2}\cdots.
    \]
\end{definition}

\begin{example}
    The elementary symmetric function $e_2$ is
    \begin{align*}
        e_2 = x_1x_2 + x_1x_3 + x_1x_4 + &\cdots \\
        + x_2x_3 + x_2x_4 + &\cdots \\
        + x_3x_4 + &\cdots
    \end{align*}
\end{example}

Now we examine the relationship between the elementaries and the monomials.

\begin{definition}
    Let $\lambda \vdash n$. We define $M_{\lambda\mu}$ to be the coefficient of $m_\mu$ in the expansion of $e_\lambda$ in the monomial basis. That is, the numbers such that
    \[
        e_\lambda = \sum_{\mu \in \Par} M_{\lambda\mu} m_\mu.
    \]
    More generally, for any weak composition $\alpha$, let $M_{\lambda\alpha}$ be the coefficient of $x^\alpha$ in $e_\lambda$.
    This means the numbers such that
    \[
        e_\lambda = \sum_{\alpha \in \Com} M_{\lambda\alpha} x^\alpha.
    \]
    We will also sometimes write $M_{\shape \lambda, \content \alpha}$, in light of Proposition \ref{prop:eTableauInterpretation}.
\end{definition}

We will give a combinatorial interpretation of these coefficients in terms of counting certain matrices.

\begin{definition}
    A \defstyle{zero-one matrix} is an infinite two-dimensional array $(a_{ij})_{i,j \geq 1}$ whose entries are either zero or one, and for which \textit{all but finitely many entries are zero}.
    Denote the set of all zero-one matrices by $\Mat_\infty(01)$.

    By this finiteness, the \textit{row sums} and \textit{column sums} of a zero-one matrix are well-defined.
    So for any zero-one matrix $A = (a_{ij})_{i,j \geq 1}$, we define
    \[
        \row A \coloneq \left(\bigcomma_{i \in \NN} \sum_{j \geq 1} a_{ij}\right)
    \]
    and $\col A$ is similarly defined.
\end{definition}

\begin{theorem} \label{thm:e2mCombInterpretation}
    The coefficient $M_{\lambda\mu}$ is counted by zero-one matrices whose row-sums are $\lambda$ and whose column sums are $\mu$.
\end{theorem}

\begin{proof}
    Consider what is going on when we compute the terms of $e_\lambda$,
    \[
        e_\lambda = e_{\lambda_1}e_{\lambda_2}\ldots.
    \]
    To name a term on the right hand side, we pick out $\lambda_1$ $x_i$'s from $e_{\lambda_1}$, $\lambda_2$ $x_i$'s from $e_{\lambda_2}$, and so on.
    These choices of distinct variables are $\lambda_i$-sized subsets of $(x_1, x_2, \ldots)$, and encoding these subsets with lists of $1$s and $0$s gives us the rows of our $0-1$ matrix, where the row $i$ corresponds to $e_{\lambda_i}$.     
    \[
        \begin{bmatrix}
            x_1 & x_2 & x_3 & \cdots \\
            x_1 & x_2 & x_3 & \cdots \\
            x_1 & x_2 & x_3 & \cdots \\
            \vdots & \vdots & \vdots & \ddots 
        \end{bmatrix}
    \]
    The fact that we picked up $\lambda_i$ variables in each row manifests as the $i$-th row sum being equal to $\lambda_i$.
The exponent of a given variable $x_j$ appearing in a monomial only depends on how many times we picked up an $x_j$ from each $e_{\lambda_i}$. 
This manifests as the $j$-th column sum being equal to $\mu_i$.
\end{proof}


\begin{theorem} \label{thm:e2msymmetric}
    Let $\lambda,\mu \in \Par$, then
    \[
        M_{\lambda\mu} = M_{\mu\lambda}.
    \]
\end{theorem}

\begin{proof}
    Matrix transposition is a bijection between the sets the two numbers count--- namely zero-one matrices with row sum $\lambda$ and column sum $\mu$, and zero-one matrices with row sum $\mu$ and column sum $\lambda$.
\end{proof}

We have a tableau interpretation for $e_\lambda$.

\begin{proposition}
    \label{prop:eTableauInterpretation}
    $M_{\lambda\mu}$ is the number of \textit{column strict} tableau of shape $\lambda^\top$ and content $\mu$.
    In particular, $M_{n\mu}$ is the number of column strict tableau of shape $1^n$ and content $\mu$.
\end{proposition}

\subsubsection{The fundamental theorem of symmetric functions}

We establish a key fact about the numbers $M_{\lambda\mu}$.
\begin{theorem}[Gale-Ryser] \label{thm:galeryser} Let $M = (a_{ij})_{i,j \geq 1}$ be a zero-one matrix, whose row sums are given by the composition $\alpha$ and whose column sums are given by composition $\beta$. Then it must be that $\alpha \leq \beta^\top$. Moreover, there is only \textit{one} zero-one matrix such that $\alpha = \beta^\top$.
\end{theorem}

\begin{proof}
    We demonstrate this algorithmically. 
    
    \todo{Gale-Ryser proof}
\end{proof}

\begin{theorem}
    [Fundamental theorem of symmetric functions] 
    The $e_\lambda$'s form a $\ZZ$-basis for the ring of symmetric functions, as $\lambda$ ranges over all partitions.
\end{theorem}

\begin{proof}
    By Theorem \ref{thm:galeryser}, the transition matrix for a fixed $n$,
    \[
        \{K_{\lambda\mu}\}_{1^n \preceq \lambda \preceq n, 1^n \preceq \mu \preceq n}
    \]
    is upper triangular and has $1$'s on the diagonal, hence it is invertible in $\ZZ$. 
\end{proof}

\subsection{Complete homogeneous symmetric functions}

The \textit{complete homogeneous symmetric functions}, or the \defstyle{completes}, or the $h$'s, have a very similar definition as the elementaries, but with distinctness relaxed.

\begin{definition}
    Let $n\in\NN$. The \defstyle{complete homogeneous symmetric function} $h_n$ is defined to be
    \[
        h_n \coloneq \sum_{i_1\leq i_2\leq\ldots\leq i_n} x_{i_1}x_{i_2}\cdots x_{i_n}.
    \]
    And if we let $\lambda \vdash n$, $h_\lambda$ is defined to be
    \[
        h_\lambda \coloneq h_{\lambda_1}h_{\lambda_2}\cdots.
    \]
\end{definition}

As with the elementaries and $M_{\lambda\mu}$, we define a set of monomial coefficients for the completes.

\begin{definition}
    Let $\lambda \in \Par$ and $\alpha \in \Com$. Define $N_{\lambda\alpha}$ be the coefficient of $x^\alpha$ in $h_\lambda$.
\end{definition}

These numbers satisfy analogous theorems.

\begin{theorem} \label{thm:h2mCombInterpretation}
    Let $\lambda, \mu$ be partitions. Then $N_{\lambda\mu}$ is counted by $\NN$-matrices with row sums $\lambda$ and column sums $\mu$.
\end{theorem}

\begin{theorem} \label{thm:h2mSymmetric}
    Let $\lambda, \mu \in \Par$, then
    \[
        N_{\lambda\mu} = N_{\mu\lambda}.
    \]
\end{theorem}

\begin{theorem}
    \label{thm:hAreBasis}
    The $h_\lambda$'s form a $\ZZ$-basis for the ring of symmetric functions, as $\lambda$ ranges over all partitions.
\end{theorem}

\begin{proof}[Proof of Theorems \ref{thm:h2mSymmetric} and \ref{thm:h2mCombInterpretation}]
    These are proved almost exactly the same way as Theorems \ref{thm:e2mCombInterpretation} and \ref{thm:e2msymmetric}.
\end{proof}

We do not have an analogous proof of Theorem \ref{thm:galeryser} for $N_{\lambda\mu}$, yet.
At the very least, we have another tableau mnemonic for $N_{\lambda\mu}$.

\begin{theorem}
    $N_{\lambda\mu}$ is the number of tableaux of shape $\lambda$ and content $\mu$ with nondecreasing rows. 

    In particular, $N_{n\mu}$ is the number of row-weak Young tableau of shape $n$ and content $\mu$.
\end{theorem}

\subsection{Power sum symmetric functions}

We have one more basis for $\Lambda$, which has roots closer to the representation theory of $S_n$ than the elementaries or completes.

\begin{definition}
    Let $n\in\NN$. The \defstyle{power sum symmetric function} $p_n$ is defined to be
    \[
        p_n 
        \coloneq 
        \sum_{i\in\PP} x_i^n
    \]
    And if we let $\lambda \in \Par$, $p_\lambda$ is defined to be
    \[
        p_\lambda 
        \coloneq 
        p_{\lambda_1}p_{\lambda_2}\cdots.
    \]
\end{definition}

\begin{definition}
    Let $\lambda \in \Par$ and $\alpha \in \Com$.
    We define $R_{\lambda\alpha}$ to be the coefficient of $x^\alpha$ in $p_\lambda$.
\end{definition}

\begin{theorem}
    Let $\lambda, \mu \in \Par$.
    $R_{\lambda\mu}$ counts the number of ordered partitions $\pi = (B_1, \ldots, B_k)$, where $k = \len \mu$, such that
    \[
        \mu_i = \sum_{j\in B_i} \lambda_j.
    \]
\end{theorem}

\begin{proof}
    Choosing a term in $p_\lambda$ means picking up an $x_{i_j}^{\lambda_j}$ term from each $p_{\lambda_j}$,
    \[
        p_{\lambda_j} = \Big(\cdots + x_{i_j}^{\lambda_j} + \cdots\Big).
    \]
    Evidently, given such a choice, we can partition the $\lambda_j$'s into subsets which pick out the same indeterminate $x_{i_j}$, and this subset determines the degree of $x_{i_j}$ in our monomial.
\end{proof}

\subsubsection{Cycle type}

\begin{definition}
    Let $w \in S_n$. The \textit{cycle type} $\rho(w)$ of $w$ is the partition of $n$ whose parts are the cycle lengths of $w$'s disjoint cycle decomposition.
\end{definition}

\begin{example}
    The cycle type of $w = 1657234$ is $\rho(w) = 421$, since $w$ factorizes as
    \[
        w = \cyc_{2635}\cyc_{47}\cyc_{1}.
    \]
\end{example}

\begin{definition}
    Define the number $z_\lambda$ to be
    \[
        z_\lambda \coloneq 1^{m_1}m_1!2^{m_2}m_2!\cdots.
    \]
\end{definition}

This quantity is important for enumeration with regards to cycle type.

\begin{theorem}
    The number of permutations $w \in S_n$ of cycle type $\rho = \langle 1^{m_1}2^{m_2}\cdots\rangle$ is
    \[
        \frac{n!}{1^{m_1}m_1!2^{m_2}m_2!\cdots} = n!z_\rho^{-1}.
    \]
\end{theorem}

\begin{proof}
    Organize the denominator as follows
    \[
        \frac{n!}{(1^{m_1}2^{m_2}\cdots)(m_1!m_2!\cdots)}.
    \]
    The left factor describes an ``internal'' symmetry, that of \textit{permuting the insides of each cycle}. 
    The right factor describes an ``external'' symmetry, that of \textit{permuting the cycles themselves}.
    Specifically, take a permutation $w = w_1w_2\cdots w_n$, viewing it only as a tuple of numbers, i.e a word.
    There are $n!$ many such $w$.
    We may construct a permutation $[w]$ out of $w$ with cycle type $\rho$ by considering the permutation
    \[
        (w_1\cdots w_{\rho_1})
        (w_{\rho_1+1}\cdots w_{\rho_1+\rho_2})\cdots(w_{\rho_1+\cdots+\rho_{k-1}+1}\cdots w_{\rho_1+\cdots+\rho_{k-1}+\rho_k}).
    \]
    The group action 
    \todo{finish cycle index proof}
\end{proof}

\section{Identities and algebraic gadgets}

We will introduce identities pertaining to the bases of $\Lambda$ we have covered so far.
These identities are the combinatorial backbone of certain algebraic mechanisms in $\Lambda$.

\subsection{
    The Newton-Girard formulas
    and $\omega$-involution
}

We have a theorem known as \defstyle{Newton's identities}, or, as we will call them, the \defstyle{Newton-Girard formulas}.

\begin{theorem}[Newton-Girard formulas]
    Let $n \in \PP$. Then
    \begin{align}
        \sum_{k=0}^n (-1)^k e_kh_{n-k} &= 0 \label{ng1} \\
        \sum_{k=0}^n (-1)^{k-1} e_{n-k}p_k &= ne_n  \label{ng2} \\
        \sum_{k=0}^n h_{n-k}p_k &= nh_n \label{eq:ng3}
    \end{align}
\end{theorem}

These can be used to define \defstyle{$\omega$-involution} on the ring of symmetric functions, giving it a \textit{antipode}, in the language of Hopf algebras.

To define it, we note that $\Lambda = \pring{\QQ}{e_1,e_2,\ldots}$, which we recall means that $\Lambda$ is freely generated by the $e_n$'s as a commutative $\QQ$-algebra, any homomorphism $\phi$ out of $\Lambda$ can be defined by specifying the images of the $e_n$'s under $\phi$, so we in fact get a well-defined map.

\begin{definition}
    We define $\omega$ to be the map 
    \begin{align*}
        \omega: \Lambda &\to \Lambda \\
        e_n &\mapsto h_n.
    \end{align*}
\end{definition}

And, this map is involutory.

\begin{theorem}
    \label{thm:OmegaIsInvolution}
    For all $n$,
    \[
        \omega(h_n) = e_n.
    \]
    Thus, $\omega$ is an involution.
\end{theorem}

Finally, we can give a proof of \ref{thm:hAreBasis}

\begin{proof}
    [Proof of Theorem \ref{thm:hAreBasis}]

\end{proof}

\subsubsection{Distinguished generating functions}

We define the following, natural, generating functions in the ring $\Lambda \lBrack t \rBrack$.

\begin{definition}
    \begin{align*}
        H(t) &\coloneq \sum_{n\geq0}h_n t^n \\
        E(t) &\coloneq \sum_{n\geq0}e_n t^n \\
        P(t) &\coloneq \sum_{n\geq1}p_n t^n
    \end{align*}
\end{definition}

Note that $P(t)$ has constant term zero, while $E(t)$ and $H(t)$ have constant term one.

\begin{theorem}\label{thm:HEPPowerSeries}
    We have that
    \begin{align*}
        H(t) &= \prod_{n\geq0}\frac{1}{1-x_nt}, \\
        E(t) &= \prod_{n\geq0}(1+x_nt), \\
        P(t) &= \sum_{n\geq0}\frac{x_nt}{1-x_nt}.
    \end{align*}
\end{theorem}

\begin{proof}
    For the first one, we compute that
    \begin{align*}
        \prod_{n\geq0}\frac{1}{1-x_nt} &= \prod_{n\geq0}\sum_{k\geq0}x_n^kt^k\\
                                        &= \prod_{n \geq 0}\Big(1 + x_nt + x_n^2t^2 + \cdots\Big) \\
                                        &= \sum_{k \geq 0}\sum_{\alpha \models k} \bfx^\alpha t^k \\
                                        &= \sum_{k \geq 0} h_kt^k.
    \end{align*}
    Similarly,
    \begin{align*}
        \prod_{n\geq0}(1+x_nt) 
        &=
        \sum_{k \geq 0}
        \sum_{i_1 < \cdots < i_k}
        x_k
        t
        \\
        &= 
        \sum_{k \geq 0}
        \left(
            \sum_{i_1 < \cdots < i_k}
            x_k
        \right)
        t^k
        \\
        &=
        \sum_{k \geq 0}
        e_kt^k.
    \end{align*}
\end{proof}

\subsubsection{Proof of the Newton-Girard formulas}

With the results of the previous subsection, we are ready to prove the Newton-Girard formulas.

\begin{proof}
    [Proof of the Newton-Girard formulas]
    These all follow from Theorem \ref{thm:HEPPowerSeries}. We have that
    \[
        H(t)E(-t) 
        = \left(\prod_{n\in\NN}\frac{1}{1-x_nt}\right)\left(\prod_{n\in\NN}1-x_nt\right) 
        = \prod_{n\in\NN}\frac{1-x_nt}{1-x_nt} = 1.
    \]
    Then $[t^n] H(t)E(-t) = 0$ for all $n \geq 1$, giving us
    \[
        \sum_{k=0}^n (-1)^k e_k h_{n-k} = 0 \qquad \forall n\geq 1.
    \]
    This proves the first Newton-Girard formula \eqref{ng1}.

    Then, we have that
    \begin{align*}
        E(-t)P(t) &= \left[\prod_{n\in\NN}(1-x_nt)\right]\left[t\sum_{m\in\NN}\frac{x_m}{1-x_mt}\right] \\
                  &= t\sum_{m\in\NN}\left[\frac{x_m}{1-x_mt}\prod_{n\in\NN}(1-x_nt)\right] \\
                  &= t\sum_{m\in\NN} \left[x_m\prod_{\substack{n\in\NN \\ n\neq m}}(1-x_nt)\right] \\
                  &= -t\sum_{m\in\NN} \left[-x_m\prod_{\substack{n\in\NN \\ n\neq m}}(1-x_nt)\right].
    \end{align*}
    The sum can be expressed as the derivative of an infinite product, and we can continue the simplification
    \begin{align*}
                  &= -t\frac{d}{dt}\left[\prod_{m\in\NN}(1-x_mt)\right] \\
                  &= -t\frac{d}{dt}E(-t) \\
                  &= -t\frac{d}{dt}\left[\sum_{n\in\NN}(-1)^n e_nt^n\right] \\
                  &= -t\left[\sum_{n\geq 1}n(-1)^n e_nt^{n-1}\right] \\
                  &= \sum_{n\geq 1}n(-1)^{n-1} e_nt^n.
    \end{align*}
    Then, the formula for $[t^n]E(-t)P(t)$ given $n \geq 1$ is
    \[
        \sum_{k=0}^n (-1)^k e_{n-k}p_k = n(-1)^{n-1}e_n \qquad \forall n \geq 1.
    \]
    And after moving the $-1$ factors,
    \[
        \sum_{k=0}^n (-1)^{k-1} e_{n-k}p_k = ne_n.
    \]
    This proves the second Newton-Girard formula, \eqref{ng2}.

    The proof of the third is very similar and actually even easier, since
    \begin{align*}
        \frac{d}{dt}H(t) &= \frac{d}{dt}\left[\prod_{n\in\NN}\frac{1}{1-x_nt}\right] \\
                         &= \sum_{m\in\NN}\left[\frac{x_m}{(1-x_mt)^2}\prod_{\substack{n\in\NN \\ n \neq m}}\frac{1}{1-x_nt}\right] \\ 
                         &= \sum_{m\in\NN}\left[\frac{x_m}{1-x_mt}\prod_{n\in\NN}\frac{1}{1-x_nt}\right]
    \end{align*}
    Then
    \begin{align*}
        H(t)P(t) &= \left[\prod_{n\in\NN}\frac{1}{1-x_nt}\right]\left[t\sum_{m\in\NN}\frac{x_m}{1-x_mt}\right] \\
                 &= t\sum_{m\in\NN}\left[\frac{x_m}{1-x_mt}\prod{n\in\NN}\frac{1}{1-x_nt}\right] \\
                 &= t\frac{d}{dt}H(t) \\
                 &= t\frac{d}{dt}\left[\sum_{n\in\NN}h_nt^n\right] \\
                 &= t\sum_{n\geq 1}nh_nt^{n-1} \\
                 &= \sum_{n\geq 1}nh_nt^n,
    \end{align*}
    which proves the third Newton-Girard formula, \eqref{eq:ng3}.
\end{proof}

\subsection{Cauchy identities and the Hall inner product}

Next, we have a \textit{sort} of identity known in algebraic combinatorics as a \defstyle{Cauchy identity}.

The classic Cauchy identity is as follows:

\begin{theorem}
    [Cauchy identity]
    The formal power series identity
    \[
        \prod_{i,j \geq 1}
        (1-x_iy_j)^{-1}
        =
        \sum_{\lambda\in\Par}
        s_\lambda(\bfx)s_\lambda(\bfy)
    \]
    holds, where $s_\lambda$ is the \defstyle{Schur function} indexed by the partition $\lambda$.
\end{theorem}

For now, we will begin with Cauchy identities for the bases we already know

\begin{theorem}
    [Cauchy identities]
    \begin{align}
        \prod_{i,j \geq 1}
        (1-x_iy_j)^{-1}
        &=
        \sum_{\lambda\in\Par}
        m_\lambda(\bfx)h_\lambda(\bfy)
        \\
        \prod_{i,j \geq 1}
        (1+x_iy_j)
        &=
        \sum_{\lambda\in\Par}
        m_\lambda(\bfx)e_\lambda(\bfy)
        \\
        \prod_{i,j \geq 1}
        (1-x_iy_j)^{-1}
        &=
        \sum_{\lambda\in\Par}
        z_\lambda^{-1}p_\lambda(\bfx)p_\lambda(\bfy)
        \\
        \prod_{i,j \geq 1}
        (1+x_iy_j)
        &=
        \sum_{\lambda\in\Par}
        z_\lambda^{-1}\varepsilon_{\lambda} p_\lambda(\bfx) p_\lambda(\bfy).
    \end{align}
\end{theorem}

Cauchy identities will be used to define the \defstyle{Hall inner product}, named after the algebraist Philip Hall, who described them in \textit{The algebra of partitions} (1959).

\begin{definition} Let $\langle -,- \rangle$ be the scalar product defined by the relationship
    \[
        \langle m_\lambda, h_\nu \rangle 
        \coloneq 
        \delta_{\lambda\nu}.
    \]
    Where $\delta_{\lambda\nu} \coloneq [\lambda = \nu]^?$.

    This scalar product is called the \defstyle{Hall inner product}.
    It is well defined since the $m$'s and $h$'s form a basis for $\Lambda$.
\end{definition}

\begin{theorem}
    $\langle\cdot,\cdot\rangle$ is symmetric.
\end{theorem}

\begin{proof}
    It suffices to prove that products of basis elements are symmetric for some basis of $\Lambda$.
    We'll use the basis $\{h_\lambda\}_{\lambda\in\Par}$.
    By their Cauchy identity (Theorem \ref{CauchyIdentityForMH}), we have that
    \[
        \langle h_\lambda, h\nu \rangle = \left\langle \sum_{\gamma\in\Par} N_{\lambda\gamma} m_\gamma, h\nu \right\rangle = N_{\lambda\nu}.
    \]
    Then $\langle h_\lambda, h_\nu \rangle = N_{\lambda\nu} = N_{\nu\lambda} = \langle h_\lambda, h_\nu \rangle$.
\end{proof}

\begin{theorem}\label{thm:CauchyIdentityImpliesOrthonormal}
    Any two bases $\{u_\lambda\}_{\lambda\in\Par}$ and $\{v_\lambda\}_{\lambda\in\Par}$ of $\Lambda$ that have a Cauchy identity
    \begin{equation}\label{eq:CauchyIdentityArbitrary}
        \prod_{i,j \geq 1}(1 - x_iy_j)^{-1}
        =
        \sum_{\lambda\in\Par}
        u_\lambda(\bfx) v_\lambda(\bfx)
    \end{equation}
    are orthonormal with respect to the Hall inner product.
\end{theorem}

\begin{proof}
    We'll make use of \href{https://en.wikipedia.org/wiki/Einstein_notation}{Einstein summation} to make calculations quick.

    Define the numbers $U_{\cdot,\cdot}$ and $V_{\cdot,\cdot}$ by
    \[
        m_\lambda = U_{\lambda\rho} u_\rho, \qquad h_\mu = V_{\mu\nu} v_\nu.
    \]
    Now, we will suppress the variables, so we will always take $u_\lambda$ to be $u_\lambda(\bfx)$, and similarly we will always take $v_\lambda$ to mean $v_\lambda(\bfy)$.
    Similarly, we will take $m_\lambda$ to be in $\bfx$ and $h_\lambda$ to be in $\bfy$.

    The right hand side of the Cauchy identity for the completes (Theorem \ref{CauchyIdentityForMH}) becomes
    \begin{align*}
        m_\lambda h_\lambda &= \Big(U_{\lambda\rho}u_\rho\Big)\Big(V_{\lambda\nu}v_\nu\Big) \\
                                                                                &= U_{\lambda\rho}V_{\lambda\nu} \Big(u_\rho v_\nu\Big)
    \end{align*}

    Since we're assuming the Cauchy identity for the bases $\{u_\lambda\}_{\lambda\in\Par}$ and $\{v_\lambda\}_{\lambda\in\Par}$ also, we have shown that
    \[
        U_{\lambda\rho}V_{\lambda\nu} \Big(u_\rho v_\nu\Big) = u_\mu v_\mu
    \]
    hence, it must be that
    \[
    \]
\end{proof}

\iffalse
\begin{proof}[Proof (detailed)]
    Define the numbers $U_{\cdot,\cdot}$ and $V_{\cdot,\cdot}$ by
    \[
        m_\lambda = \sum_\rho U_{\lambda\rho} u_\rho, \qquad h_\mu = \sum_\nu V_{\mu\nu} v_\nu.
    \]
    Then, by linearity of the Hall inner product,
    \[
        \delta_{\lambda\mu} = \langle m_\lambda, h_\mu \rangle = \sum_{\rho,\nu}U_{\lambda\rho} V_{\mu\nu} \langle u_\rho, u_\nu \rangle
    \]
    Now, if we are to have that $\langle u_\lambda, v_\mu \rangle = \delta_{\lambda\mu}$, the above should be equivalent to 
    \[
        \delta_{\lambda\mu} = \sum_{\rho,\nu} U_{\lambda\rho} V_{\mu\nu} \delta_{\rho\nu}.
    \]
    Since $\sum_\nu V_{\mu\nu}\delta_{\rho\nu} = V_{\mu\rho}$, we find that
    \begin{equation}\label{eq:CauchyIdentityImpliesDualBasis:1}
        \delta_{\lambda\mu} =  \sum_\rho U_{\lambda\rho} V_{\mu\rho}.
    \end{equation}
    The sequence of equalities given can be run backwards, so we have to just show (\ref{eq:CauchyIdentityImpliesDualBasis:1}) and the theorem is proven.
    By Theorem \ref{CauchyIdentityForMH}, we have that
    \[
        \prod_{i,j}(1-x_iy_j)^{-1} = \sum_\lambda m_\lambda(X) h_\lambda(Y).
    \]
    So 
    \[
        \prod_{i,j}(1-x_iy_j)^{-1} = \sum_\lambda \left(\sum_\rho \zeta_{\lambda\rho} u_\rho(X)\right)\left(\sum_\nu \eta_{\lambda\nu}v_\nu(Y)\right).
    \]
    Interchanging sums,
    \[
        \sum_\lambda \left(\sum_\rho \zeta_{\lambda\rho} u_\rho(X)\right)\left(\sum_\nu \eta_{\lambda\nu}v_\nu(Y)\right) = \sum_{\rho,\nu} \left(\sum_\lambda \zeta_{\lambda\rho} \eta_{\lambda\nu}\right) u_\rho(X) v_\nu(Y).
    \]
    By (\ref{eq:CauchyIdentityArbitrary})
    \[
        \sum_{\rho,\nu} \left(\sum_\lambda \zeta_{\lambda\rho} \eta_{\lambda\nu}\right) u_\rho(X) v_\nu(Y) = \sum_\mu u_\mu(X) v_\mu(Y).
    \]
    Since the $u_\lambda(X)v_\lambda(Y)$ are linearly independent as power series, we can compare coefficients, which gives us the desired equality.
\end{proof}
\fi

\begin{corollary}
    The power sums $\{p_\lambda\}$ form an orthogonal basis form $\Lambda$.
\end{corollary}

\begin{theorem}
    $\omega$ is an isometry of $\Lambda$.
\end{theorem}
\begin{proof}
\end{proof}

\subsubsection{Proofs of Cauchy identities}

\begin{proof}
    [Proof of the Cauchy identity for $m_\lambda$, $e_\lambda$]
    The coefficient of $\bfx^\alpha \bfy^\beta$ is obtained by taking a zero-one matrix $A$ whose row sum is $\alpha$ and whose column sum is $\beta$. Then
    \begin{align*}
        \prod_{i,j\geq1}(1+x_iy_j)
        &=
        \sum_{A \in \Mat_\infty(01)}
        \bfx^{\row A}\bfy^{\col A} \\
        &=
        \sum_{\alpha,\beta \in \Com}
        M_{\alpha\beta}\bfx^\alpha \bfy^\beta \\
        &=
        \sum_{\lambda,\mu\in\Par}
        \left[
            \sum_{\alpha \in \Com(\lambda), \beta \in \Com(\mu)}
            M_{\alpha\beta} \bfx^\alpha \bfy^\beta
        \right] \\
        &=
        \sum_{\lambda,\mu\in\Par}
        M_{\lambda\mu}
        \left[
            \sum_{\alpha \in \Com(\lambda), \beta \in \Com(\mu)}
            \bfx^\alpha \bfy^\beta
        \right] \\
        &=
        \sum_{\lambda,\mu \in \Par}
        M_{\lambda\mu}
        \Big(
            m_\lambda(\bfx) m_\mu(\bfy)
        \Big) \\
        &=
        \sum_{\lambda \in \Par}
        m_\lambda(\bfx)
        \left[
            \sum_{\mu\in\Par}
            M_{\lambda\mu} m_\mu(\bfy)
        \right] \\
        &=
        \sum_{\lambda \in \Par}
        m_\lambda(\bfx) e_\lambda(\bfy).
    \end{align*}
\end{proof}

\begin{proof}
    [Proof of the Cauchy identity for $m_\lambda$, $h_\lambda$]
    The exact same idea as the previous proof.
    \begin{align*}
        \prod_{i,j\geq1}(1-x_iy_j)^{-1} 
        &= 
        \sum_{A \in \Mat_\infty(\NN)}
        \bfx^{\row A}\bfy^{\col A} 
        \\
        &= 
        \sum_{\lambda,\mu\in\Par} N_{\lambda\mu} 
        \big(m_\lambda(\bfx) m_\mu(\bfy)\big) 
        \\
        &= 
        \sum_{\lambda\in\Par} 
        m_\lambda(\bfx) 
        \left[
            \sum_\mu N_{\lambda\mu} m_\mu(\bfy)
        \right]
        \\
        &= 
        \sum_{\lambda\in\Par} 
        m_\lambda(\bfx) h_\lambda(\bfy).
    \end{align*}
\end{proof}

The next proof is more involved.

\begin{proof}
    [Proof of the Cauchy identity for $p_\lambda$]
    We apply $\log$ to the left hand side, and grind to obtain
    \begin{align*}
        \log \prod_{i,j \geq 1}
        (1-x_iy_j)^{-1}
        &=
        \sum_{i,j\geq1}
        \log(1-x_iy_j)^{-1} \\
        &=
        -\sum_{i,j \geq 1}
        \log(1-x_iy_j) \\
        &=
        -\sum_{i,j \geq 1}
        \sum_{n\geq 1}
        \frac{(-1)^{n-1}}{n}(-x_iy_j)^n \\
        &=
        \sum_{i,j \geq 1}
        \sum_{n\geq 1}
        \frac{1}{n} x_i^ny_j^n \\
        &=
        \sum_{n \geq 1}
        \sum_{i,j\geq 1}
        \frac{1}{n} x_i^ny_j^n \\
        &=
        \sum_{n \geq 1}
        \frac{1}{n}
        \left(\sum_{i\geq1}x_i^n\right)
        \left(\sum_{j\geq1}y_j^n\right) \\
        &=
        \sum_{n \geq 1}
        \frac{1}{n} p_n(\bfx) p_n(\bfy).
    \end{align*}
    Hence we have that
    \[
        \prod_{i,j\geq1}(1-x_iy_j)^{-1} 
        = 
        \exp \log \prod_{i,j\geq1}(1-x_iy_j)^{-1} 
        = 
        \exp \sum_{n \geq 1} 
        \frac{1}{n} p_n(\bfx) p_n(\bfy).
    \]
    Then, we apply the \textit{permutation version of the exponential formula}, which tells us that, for any function $f: \PP \to \KK$ where $\KK$ is some commutative ring,
    \[
        \exp
        \left(
            \sum_{n=1}^\infty
            f(n)
            \frac{t^n}{n}
        \right)
        =
        \sum_{n=0}^\infty
        \left[
            \sum_{
                \pi \in S_n
            }
            f(\rho(\pi)_1) \cdots f(\rho(\pi)_{\ell(\rho(\pi))})
        \right]
        \frac{t^n}{n!},
    \]
    where $\rho(\pi)$ is $\pi$'s cycle type.
    Alternatively, we can express it in the form
    \begin{align*}
        \exp
        \left(
            \sum_{n=1}^\infty
            f(n)
            \frac{t^n}{n}
        \right)
        &=
        \sum_{n=1}^\infty
        \left[
            \sum_{\lambda \vdash n}
            \sum_{
                \substack{
                \pi \in S_n \\
                \rho(\pi) = \lambda
                }
            }
            f(\lambda_1) \cdots f(\lambda_k)
        \right]
        \frac{t^n}{n!} \\
        &=
        \sum_{n=1}^\infty
        \left[
            \sum_{\lambda \vdash n}
            f(\lambda_1) \cdots f(\lambda_k)
            \sum_{
                \substack{
                \pi \in S_n \\
                \rho(\pi) = \lambda
                }
            }
            1 
        \right]
        \frac{t^n}{n!} \\
        &=
        \sum_{n=1}^\infty
        \left[
            \sum_{\lambda \vdash n}
            f(\lambda_1) \cdots f(\lambda_k)
            n!
            z_\lambda^{-1}
        \right]
        \frac{t^n}{n!} \\
        &=
        \sum_{n=1}^\infty
        \left[
            \sum_{\lambda \vdash n}
            f(\lambda_1) \cdots f(\lambda_k)
            z_\lambda^{-1}
        \right]
        t^n \\
        &=
        \sum_{\lambda \in \Par}
        f(\lambda_1) \cdots f(\lambda_k) z_\lambda^{-1}
        t^{|\lambda|}
    \end{align*}
    Then, if we put $f(n) = p_n(\bfx)p_n(\bfy)$,
    \begin{align*}
        \exp
        \sum_{n=1}^\infty
        \frac{t^n}{n}
        p_n(\bfx) p_n(\bfy)
        &=
        \sum_{\lambda \in \Par}
        p_{\lambda_1}(\bfx) p_{\lambda_1}(\bfy)
        \cdots
        p_{\lambda_k}(\bfx) p_{\lambda_k}(\bfy)
        z_\lambda^{-1}
        t^{|\lambda|} \\
        &=
        \sum_{\lambda \in \Par}
        \underbrace{
            \Big(
                p_{\lambda_1}(\bfx) \cdots p_{\lambda_k}(\bfx)
            \Big)
        }_{=p_\lambda(\bfx)}
        \underbrace{
            \Big(
                p_{\lambda_1}(\bfy) \cdots p_{\lambda_k}(\bfy)
            \Big)
        }_{=p_\lambda(\bfy)}
        z_\lambda^{-1}
        t^{|\lambda|} \\
        &=
        \sum_{\lambda \in \Par}
        p_\lambda(\bfx) p_\lambda(\bfy) z_\lambda^{-1}
        t^{|\lambda|}.
    \end{align*}
    Finally, putting $t=1$, we have
    \[
        \exp
        \sum_{n \geq 1}
        \frac{1}{n}
        p_n(\bfx) p_n(\bfy)
        =
        \sum_{\lambda \in \Par}
        p_\lambda(\bfx) p_\lambda(\bfy) z_\lambda^{-1},
    \]
    which proves the first equality.

    The second equality is proven very similarly.

    \begin{align*}
        \log \prod_{i,j \geq 1}
        (1+x_iy_j)
        &=
        \sum_{i,j\geq1}
        \log(1+x_iy_j) \\
        &=
        \sum_{i,j \geq 1}
        \sum_{n\geq 1}
        \frac{(-1)^{n-1}}{n}(x_iy_j)^n \\
        &=
        \sum_{n \geq 1}
        \frac{(-1)^{n-1}}{n} p_n(\bfx) p_n(\bfy).
    \end{align*}
    Using the exponential formula again,
    \begin{align*}
        &\exp
        \sum_{n=1}^\infty
        \frac{t^n}{n}
        (-1)^{n-1}
        p_n(\bfx) p_n(\bfy)
        \\
        &=
        \sum_{\lambda \in \Par}
        \Big(
            (-1)^{\lambda_1-1}
            p_{\lambda_1}(\bfx) p_{\lambda_1}(\bfy)
        \Big)
        \cdots
        \Big(
            (-1)^{\lambda_k-1}
            p_{\lambda_k}(\bfx) p_{\lambda_k}(\bfy)
        \Big)
        z_\lambda^{-1}
        t^{|\lambda|} \\
        &=
        \sum_{\lambda \in \Par}
        (-1)^{(\lambda_1 - 1) + \cdots + (\lambda_{\ell(\lambda)} - 1)}
        p_\lambda(\bfx) p_\lambda(\bfy) z_\lambda^{-1}
        t^{|\lambda|} \\
        &=
        \sum_{\lambda \in \Par}
        (-1)^{n - \ell(\lambda)}
        p_\lambda(\bfx) p_\lambda(\bfy) z_\lambda^{-1}
        t^{|\lambda|} \\
        &=
        \sum_{\lambda \in \Par}
        \varepsilon_\lambda
        p_\lambda(\bfx) p_\lambda(\bfy) z_\lambda^{-1}
        t^{|\lambda|}.
    \end{align*}
    Now we prove the second equality by again putting $t=1$.
\end{proof}

\section{Schur functions}
\label{ch:schurs}

We define the Schur functions now, and these play a central role in the ring of symmetric functions.

To name a few of its algebraic avatars, Schur functions appear as
\begin{enumerate}[label=(\alph*)]
    \item 
        images under the Frobenius characteristic map of irreducible characters of $S_n$,
    \item 
        characters of irreducible polynomial representations of $\operatorname{GL}_n$, and as
    \item 
        representatives in the cohomology ring of the complex Grassmannian.
\end{enumerate}

And, they embody beautiful combinatorics--- that of semistandard Young tableaux.

\begin{definition}
    Let $\lambda$ be a partition. 
    The \defstyle{Schur function} $s_\lambda$ is defined to be
    \[
        s_\lambda(\bfx)
        \coloneq
        \sum_{T\in\SSYT_{\shape \lambda}} \bfx^{\content T}.
    \]
    Where $\SSYT$ is the set of all semistandard Young tableaux.
\end{definition}

\subsection{Tableaux}

We will recall the definition of a semistandard Young tableau, introduce the definition of a \defstyle{skew tableau}, and introduce the \defstyle{Kostka numbers}.

\subsubsection{Semistandard Young tableaux}

\begin{definition}
    A \defstyle{semistandard Young tableaux} is a tableau whose columns are strongly increasing and whose rows are weakly increasing.
\end{definition}

We use the following shorthand for doing computations involving SSYT:

\begin{definition}
    Let $T$ be a tableau. Then define $x^T$ to be the monomial
    \[
        \bfx^T
        \coloneq 
        \bfx^{\content T}.
    \]
\end{definition}

\begin{example}
    \[
        \ytabalign{bottom}
        \bfx^{\ytableaushort{11,23}} = x_1^2x_2x_3.
    \]
\end{example}

\begin{example}
    For the partition $22$, we compute $s_\lambda(x_1,x_2,x_3)$.
    The following tableaux make up $\SSYT_{\shape \lambda}$ with entries in $\{1,2,3\}$.
    \[
        \ytableausetup{boxsize=normal}
        \ytableaushort{11,22},
        \ytableaushort{11,23},
        \ytableaushort{12,23},
        \ytableaushort{13,13},
        \ytableaushort{13,23},
        \ytableaushort{23,23}.
    \]
    These give us the monomials 
    \[
        x_1^2x_2^2, \qquad 
        x_1^2x_2x_3, \qquad 
        x_1x_2^2x_3, \qquad 
        x_1^2x_3^2, \qquad 
        x_1x_2x_3^2, \qquad 
        x_2^2x_3^2
    \]
    respectively. Hence we have computed that
    \[
        s_{22}(x_1,x_2,x_3) = x_1^2x_2^2 + x_1^2x_3^2 + x_2^2x_3^2 + x_1^2x_2x_3 + x_1x_2^2x_3 + x_1x_2x_3^2.
    \]
\end{example}

\subsubsection{Skew tableaux}

\begin{definition}
    Let $\lambda, \nu$ be partitions such that $\nu \subseteq \lambda$. Then the pair $(\lambda, \nu)$ is referred to as a \textit{skew shape} and is denoted $\lambda \setminus \nu$.

    The \textit{skew diagram} of $\lambda \setminus \nu$ is the diagram obtained by taking $\lambda$'s Young diagram and removing all boxes that would be contained in $\nu$'s Young diagram--- $\boxplus \lambda \setminus \boxplus \nu$.

    Finally, a \textit{skew tableau of shape $\lambda \setminus \nu$} or a \textit{tableau of skew shape $\lambda \setminus \nu$} is a filling of the aforementioned skew diagram. 

    Such a tableau will still be called \textit{semistandard} if it weakly increases along rows and strongly increases along columns.
\end{definition}

\begin{example} 
    \ytabsmallbox
    Let $\lambda = \ydiagram{4,4,3,2}$ and $\nu = \ydiagram{3,2,2}$.
    Then 
    \ytabnormalbox
    \[\lambda \setminus \nu = \ydiagram{3+1,2+2,2+1,2}.\]
\end{example}

\subsubsection{Kostka numbers}

\begin{definition}
    Let $\lambda \in \Par$ and let $\alpha \in \Com$.
    Then we define the \defstyle{Kostka number} $K_{\lambda,\alpha}$ to be
    \[
        K_{\lambda,\alpha} \coloneq \#\SSYT_{\shape \lambda, \content \alpha}.
    \]
    Let $\nu \subseteq \lambda$. We define the skew Kostka number $K_{\lambda\setminus\nu,\alpha}$ similarly.
\end{definition}

\begin{example}
    Let $\lambda = 31$ and let $\mu = 211$.
    Then
    \[
        K_{\lambda\mu} = 2,
    \]
    since there are two semistandard Young tableaux of shape $31$ and content $211$, which are \ytabsmallbox\ytableaushort{112,3} and \ytableaushort{113,2}.
\end{example}

\begin{remark}
    We have that, similarly to the other numbers $M$, $N$, $R$,
    \[
        s_\lambda(\bfx)
        = 
        \sum_{\alpha\in\Com} K_{\lambda\alpha}\bfx^\alpha.
    \]
    In particular,
    \[
        s_\lambda
        = 
        \sum_{\mu\in\Par} 
        K_{\lambda\mu}m_\mu.
    \]
\end{remark}

\subsection{Symmetry of Schur functions}


\begin{definition}
    A \defstyle{Bender-Knuth} involution $\BK_i$ is a map $\SYT \to \SYT$ 
\end{definition}

Since these notes are about symmetric functions, we have to prove the following theorem.

\begin{theorem} The skew Schurs, and therefore also the Schurs, are symmetric functions.
\end{theorem}

\begin{proof}
    We will prove that $s_{\lambda/\nu}$ is invariant under the action of simple transpositions, meaning that
    \[
        \refl_i s_{\lambda/\nu} (\bfx)
        = 
        s_{\lambda/\nu} (\refl_i \bfx)
        =
        s_{\lambda/\nu} (\bfx)
    \]
    for any $\refl_i$, which we recall is the simple transposition which swaps $i$ and $i+1$.
    Since any permutation $w \in S_\infty$ can be written as a product of simple transpositions $\refl_{i_1}\cdots \refl_{i_k}$, proving the above tells us that 
    \[
        ws_{\lambda/\nu} 
        = 
        ws_{\lambda\nu}
    \]
    for all $w \in S_\infty$.

    Rephrased, we wish to now show that 
    \[
        K_{\lambda\setminus\nu,\alpha} 
        = 
        K_{\lambda\setminus\nu,\refl_i\alpha}
    \]
    for all $\refl_i$ and for all weak compositions $\alpha$.

    We will prove this by bijection.
    The key fact here is that due to the column-strictness of semistandard Young tableau, \textit{if $i$ and $i+1$ appear in the same column, they must be vertically adjacent}. 

    Let $T\in\SSYT_{\shape \lambda/\nu,\content \alpha}$. 
    Take all the columns of $T$ which contain $i$ or $i+1$ but not both. 
    We will have rows consisting of consecutive $i$'s followed by consecutive $i+1$'s. 
    In these rows, swap the number of consecutive $i$'s with the number of consecutive $i+1$'s. 
    This swap works because the $i$'s that become $i+1$'s will not break column strictness since there is no $i+1$ in the same column, similarly $i+1$'s that become $i$'s will not break column strictness since there is no $i$ in the same column.

    This correspondence is involutive and therefore bijective. This completes the proof.
\end{proof}

    For example, if $T$ looked like
    % I desperately need to refactor this
    \[
        \ytabbigbox
        \begin{ytableau}
            \none & \none & \none & \none & \none & \none & \none & \none & & & i & i+1 & i+1 \\
            \none & \none & \none & \none & \none & \none & \none &  & i & i &  \\
            \none & \none & \none & \none & \none & \none & \none & i & i+1 & i+1\\
            \none & \none & \none & i & i+1 & i+1 & i+1 & i+1 & & \\
            \none & \none & \none & i+1 &   \\
            i & i & i+1
        \end{ytableau},
    \]
    then we ignore all other entries with both $i$ and $i+1$.
    \[
        \begin{ytableau}
            \none & \none & \none & \none & \none & \none & \none & \none & & & i & i+1 & i+1 \\
            \none & \none & \none & \none & \none & \none & \none &  & *(red) i & *(red) i &  \\
            \none & \none & \none & \none & \none & \none & \none & *(red) i & *(red) i+1 & *(red) i+1\\
            \none & \none & \none & *(red)i & i+1 & i+1 & i+1 & *(red) i+1 & & \\
            \none & \none & \none & *(red)i+1 &   \\
            i & i & i+1
        \end{ytableau}
    \]
    and consider the remaining ones. Then, we keep track of the rows of consecutive $i$ and $i+1$'s.
    \[
        \begin{ytableau}
            \none & \none & \none & \none & \none & \none & \none & \none & & & *(green) i & *(yellow) i+1 & *(yellow) i+1 \\
            \none & \none & \none & \none & \none & \none & \none &  & *(red) i & *(red) i &  \\
            \none & \none & \none & \none & \none & \none & \none & *(red) i & *(red) i+1 & *(red) i+1\\
            \none & \none & \none & *(red) i & *(yellow) i+1 & *(yellow) i+1 & *(yellow) i+1 & *(red) i+1 & & \\
            \none & \none & \none & *(red)i+1 &   \\
            *(green) i & *(green) i & *(yellow) i+1
        \end{ytableau}
    \]
    Then, we flip the number of consecutive $i$ and $i+1$'s for each row.
    \[
        \begin{ytableau}
            \none & \none & \none & \none & \none & \none & \none & \none & & & *(green) i & *(green) i & *(yellow) i+1 \\
            \none & \none & \none & \none & \none & \none & \none &  & *(red) i & *(red) i &  \\
            \none & \none & \none & \none & \none & \none & \none & *(red) i & *(red) i+1 & *(red) i+1\\
            \none & \none & \none & *(red) i & *(green) i & *(green) i & *(green) i & *(red) i+1 & & \\
            \none & \none & \none & *(red)i+1 &   \\
            *(green) i & *(yellow) i+1 & *(yellow) i+1
        \end{ytableau}
    \]

As a consequence,
\begin{corollary}
    Let $\lambda\setminus\nu$ be a skew shape. Then
    \[
        s_{\lambda\setminus\nu} = \sum_{\mu\in\Par}K_{\lambda\setminus\nu,\mu}m_\mu.
    \]
\end{corollary}

Next, we will prove that the Schurs $s_\lambda$ form a $\ZZ$-basis for $\Lambda$.

\begin{theorem}
    Fix $n$.
    Let $\lambda, \mu \vdash n$. Then $K_{\lambda\mu} \neq 0$ implies $\lambda \geq \mu$. Also, $K_{\lambda\lambda} = 1$.
\end{theorem}

\begin{proof}
    Let $K_{\lambda\mu} \neq 0 $. Then we have the existence of $T \in \SSYT_{\shape \lambda, \content \mu}$

    We will show that the entries $1,\ldots,k$ can only appear in the first $k$ rows of $T$.
    Since there can only be $\lambda_1 + \cdots + \lambda_k$ entries in the first $k$ rows of $T$, this automatically tells us that
    \[
        \mu_1 + \cdots + \mu_k \leq \lambda_1 + \cdots + \lambda_k.
    \]
    Suppose that $k$ appeared in the row $i > k$, and let $T_{ij} = k$. 
    Then 
    \[
        1 \leq T_{1j} < T_{2j} < \cdots < T_{kj} < \cdots < T_{ij} = k.
    \]
    But that implies
    \[
        k \leq T_{kj} < \cdots < T_{ij} = k.
    \]
    which gives us $k < k$, a contradiction.

    If $\nu = \lambda$, the only possible SSYT has the $i$th row filled with $i$ for all $i$.
\end{proof}

Now, similarly as with the $M_{\lambda\mu}$'s, we have the following:

\begin{corollary}
    Fix $n$.
    $\{ s_\lambda \}_{\lambda \vdash n}$ forms a basis for $\Lambda^n$, as the transition matrix $(K_{\lambda\mu})_{n \preceq \lambda \preceq 1^n, n \preceq \mu \preceq 1^n}$ is lower unitriangular.

    Consequently, the set $\{s_\lambda\}_{\lambda \in \Par}$ forms a basis for $\Lambda$.
\end{corollary}


\subsection{The Jacobi-Trudi identity}

\subsubsection{Statement}

\begin{theorem} Let $\lambda$ be a partition of length $n$. Then
    \[
        s_\lambda 
        = 
        \det(h_{\lambda_i+j-i})_{1 \leq i \leq n, 1 \leq j \leq n}.
    \]
\end{theorem}

This is proven by cancelling out terms in the determinant.

\subsubsection{The Lindstr\"om-Gessel-Viennot lemma}

Let's get some graph theory out of the way.

\begin{definition} A \textit{digraph} $D$ is a pair consisting of a vertex set $V(D)$ and an arc set $A(D)$ which consists of ordered pairs of vertices. We will suppress the $D$ and refer to the vertex and arc sets as $V$ and $A$.
\end{definition}

\begin{definition}
    A \textit{path} $p$ in a digraph $D$ is an ordered list of arcs $(a_1, \ldots, a_k)$ which are connected end-to-end.
\end{definition}

\begin{definition}
    A \textit{cycle} is a path which starts and ends at the same vertex.
\end{definition}

\begin{definition}
    Let $D$ be a digraph.
    \begin{itemize}
        \item We say that $D$ is acyclic if it contains no cycles.
        \item We say that $D$ is path-finite whenever there exist only finitely many paths from $u$ to $v$ for all $u,v \in V$.
        \item Let $\KK$ be a ring. We say that $D$ is \textit{weighted} when we have a function $w: A \to \KK$ that assigns a \textit{weight} to each arc of $D$.
    \end{itemize}
\end{definition}

\begin{theorem}[Lindstr\"om-Gessel-Viennot] 
    Let $D$ be an weighted, acyclic path-finite, digraph.

    Let $U, V$ be two sets of $n$ vertices in $D$. Define the \textit{weight} of a path $p$ to be 
    \[
        w(p) = \prod_{a \in p} a
    \]
    For any two vertices $u, v$ of $D$, define the quantity $\phi(u, v)$ to be
    \[
        \phi(u,v) = \sum_{p:u \rightarrow v} w(p),
    \]
    where $p: u \rightarrow v$ means that $p$ is a path from $u$ to $v$.

    Consider now the determinant 

    \[
        \det \begin{bmatrix}
            \phi(u_1, v_1) & \cdots & \phi(u_n, v_1) \\
             \vdots & \ddots & \vdots \\
            \phi(u_1, v_n) & \cdots & \phi(u_n, v_n)
        \end{bmatrix}
    \]

    \todo{Finish proof of LGV}
\end{theorem}

\subsubsection{Proof of the Jacobi-Trudi identity}

\begin{proof}[Proof of the Jacobi-Trudi identity]
    Fix $N \in \NN$. Consider the digraph $D$ whose vertex set is $\NN \times \{1,\ldots,N\}$. 

    We assign weights to the edges so that all vertical arcs are weighted $1$, and all horizontal arcs $(i, j) \to (i+1, j)$ are weighted $x_{j+1}$.

    For example, consider the following path
    \begin{center}
        \begin{tikzpicture}[
            scale=2,
            font=\Large
            ]
            \draw[
                step=1cm,
                line width = 0.05cm,
                ] (0,0) grid (3,3);
            \draw[
                blue, 
                line width = 0.2cm,
                ->
                ] (0,0) -- node[above] {$x_1$} ++(1,0);
            \draw[
                red, 
                line width = 0.2cm,
                ->
                ] (1,0) -- ++(0,1);
            \draw[
                red, 
                line width = 0.2cm,
                ->
                ] (1,1) -- ++(0,1);
            \draw[
                blue, 
                line width = 0.2cm,
                ->
                ] (1,2) -- node[above] {$x_3$} ++(1,0);
            \draw[
                blue, 
                line width = 0.2cm,
                ->
                ] (2,2) -- node[above] {$x_3$} ++(1,0);
            \draw[
                red, 
                line width = 0.2cm,
                ->
                ] (3,2) --++ (0,0.75);
            \filldraw[
                fill=white,
                draw=black,
                line width=0.05cm,
                radius=0.25cm,
                ] (0,0) circle node {$u$};
            \filldraw[
                fill=white,
                draw=black,
                line width=0.05cm,
                radius=0.25cm
                ] (3,3) circle node {$v$};
        \end{tikzpicture}
    \end{center}

    This path's weight is $x_1x_3^2$.

    Paths $p: (i,1) \to (i+n, N)$ are in bijection with monomials in $\KK[X_N]$ of degree $n$.



    We lay out the parts of $\lambda$ in ascending order at $x=1$.

    \todo{Finish proof of Jacobi-Trudi}
\end{proof}

We can use the Jacobi-Trudi formula to 

\begin{definition}[Schurs indexed by any integer tuple]
    Let $\gamma = (\gamma_1,\ldots,\gamma_n) \in \ZZ^n$.
    Then, we define the Schur function $s_\gamma$ to be
    \[
        s_\gamma \coloneq \det (h_{\gamma+j-i})_{1 \leq (i,j) \leq n}.
    \]
\end{definition}

Evidently, we can always rearrange the rows of $(h_{\alpha_i+j-i})$ so that we \textit{do} get a ``proper'' Jacobi-Trudi matrix, i.e one of the form $\lambda + \delta$ where $\lambda$ is a partition.

We may pick up a sign, or the determinant might be zero entirely.

\begin{theorem}[Schur function straightening]
   Let $\gamma \in \ZZ^n$. 
   Then
   \[
       s_\gamma = \begin{cases}
           \sgn(\gamma + \delta)s_{\sort(\gamma+\delta)-\delta}, & \\
           0. & \text{otherwise}
       \end{cases}
   \]
\end{theorem}

\begin{example}
    \todo{Schur function straightening example}
\end{example}

\subsection{Cauchy's bialternant formula}

Schur \textit{polynomials} appear as a special case of what's called the Weyl character formula, and this goes by the name of \defstyle{Cauchy's bialternant formula}.

\subsubsection{The Vandermonde determinant}

\begin{definition}
    Let $\gamma = (\gamma_1,\ldots,\gamma_n)$.
    We define the \defstyle{alternant} $\alt_\gamma$ to be 
    \[
        \alt_\gamma 
        \coloneq 
        \det \left(x_j^{\gamma_i}\right)_{i,j=1}^n.
    \]
\end{definition}

\begin{definition}
    An \defstyle{alternating polynomial} is a polynomial $p$ such that
    \[
        \pi p
        = 
        (\sgn \pi) \cdot p.
    \]
\end{definition}

\begin{example}
    The polynomial $p(x,y) = x-y$ is alternating, as
    \[
        p(y,x) = y-x = -(x-y) = -p(y,x).
    \]
\end{example}

Since swapping columns in matrix changes the sign of its determinant, it's clear that $\alt_\gamma$ is an alternating polynomial in $x_1,\ldots,x_n$ for all $\gamma \in \NN^n$.

\begin{proposition} \label{prop:altpolyfactor}
    Any alternating polynomial $p(\bfx_n)$ is divisible by $(x_i - x_j)$ for any $1 \leq i < j \leq n$.
\end{proposition}

\begin{proof}
    We note that it suffices to prove this in the two variable case, as we have that $\KK[x_1,\ldots,x_n] \simeq (\KK[x_1,\ldots,\bcancel{x_i},\ldots,\bcancel{x_j},\ldots,x_n])[x_i,x_j]$.
\end{proof}

There are special cases in which a determinant, a \textit{signed sum}, can be replaced with a \textit{product}.

One well known case is $\alt_\delta$, where $\delta \coloneq (n-1,n-2,\ldots,0)$.
This is known as the \defstyle{Vandermonde determinant}.

\begin{theorem}
    We have the following formula for the Vandermonde determinant
    \[
        \alt_\delta(\bfx) 
        = 
        \prod_{1 \leq i < j \leq n} (x_i-x_j).
    \]
\end{theorem}

\begin{proof}
    The proof is a little funny.

    We know, a priori, the degree of $\alt_\delta$: $n(n+1)/2$. We read this off the definition of $\alt_\delta$, which is
    \[
        \alt_\delta = \det\left(x_j^{i}\right)_{i,j=1}^n = \sum_{\pi \in S_n} (\sgn \pi) \cdot x_1^nx_2^{n-1}\cdots x_n
    \]
    Knowing this, we conclude that $\alt_\delta$ factorizes into \textit{at most} $n(n+1)/2$ linear factors.

    However, by Proposition \ref{prop:altpolyfactor}, we know that $\alt_\delta$ should factorize into \textit{at least} $n(n+1)/2$ linear factors!
    This is because it is alternating in $n$ variables, hence any $(x_i - x_j)$ where $i \neq j$ is a factor of $\alt_\delta$.

    This must be \textit{all of the factors}, hence we obtain the above formula.
\end{proof}
    

\subsubsection{The bialternant formula}

\begin{theorem}
    Fix $n \in \NN$.
    We have that
    \[
        s_\lambda(\bfx_n) 
        = 
        \frac{\alt_{\lambda + \delta}(\bfx_n)}{\alt_\delta(\bfx_n)}.
    \]
\end{theorem}

\begin{proof}
    We have that
    \[
        e_\mu = \sum_{\lambda \in \Par} K_{\shape\lambda^\top,\content\mu} \cdot s_\lambda.
    \]
    So, by specializing $f \mapsto f(\bfx_n)$, we have that
    \[
        e_\mu(\bfx) 
        = 
        \sum_{\lambda \in \Par} 
        K_{\sh\lambda^\top,\ct\mu} \cdot s_\lambda(\bfx).
    \]
    And we want to show that
    \[
        e_\mu(\bfx) 
        = 
        \sum_{\lambda \in \Par} 
        K_{\sh\lambda^\top,\ct\mu} 
        \cdot 
        \frac{\alt_{\lambda+\delta}(\bfx)}{\alt_\delta(\bfx)},
    \]
    which is equivalent to showing that
    \[
        \alt_\delta(\bfx) \cdot e_\mu(\bfx) 
        = 
        \sum_{\lambda \in \Par} K_{\sh\lambda^\top,\ct\mu} \cdot \alt_{\lambda+\delta}(\bfx).
    \]
    Both sides are antisymmetric polynomials.
    Hence, we may group together monomials based on their orbit under variable permutation--- these will have the same coefficients up to a sign.
    Each orbit is \textit{precisely} the monomials that appear in $\alt_{\lambda+\delta}(\bfx)$--- generated by permuting the exponent of $\bfx^{\lambda+\delta}$'s.

    Hence, to prove the above formula, we want to show that 
    \[
    \left[\bfx^{\lambda+\delta}\right] 
    \Big(
        \alt_\delta (\bfx) e_\mu(\bfx)
    \Big) 
    = 
    K_{\shape \lambda^\top, \content \mu}
    \]
    \todo{proof of bialternant formula}
\end{proof}

\begin{corollary}
    \[
        s_\nu e_\mu = \sum_{\lambda\in\Par}K_{\lambda^\top/\nu^\top,\mu} s_\lambda.
    \]
\end{corollary}


\begin{theorem}
    Multiplying Schurs and skewing them are adjoint operations, which means we have that
    \[
        \langle s_\mu s_\nu, s_\lambda \rangle
        =
        \langle s_\mu, s_{\lambda/\nu} \rangle.
    \]
\end{theorem}

\begin{proof}
    \todo{prove skewing adjointness}
\end{proof}

The next theorem generalizes the statement that $\omega$ sends elementaries to completes and vice versa.

\begin{theorem}
    For all $\lambda,\nu \in \Par$,
    \[
        \omega(s_{\lambda/\nu}) = s_{\lambda^\top/\nu^\top}.
    \]
\end{theorem}

\section{The Robinson-Schensted-Knuth correspondence}

\subsection{Row insertion}

\todo{Rewrite row insertion}

The basic operation will be that of \textit{row insertion}.

\begin{definition}[Row insertion]
    \ytabsmallbox
    We define \defstyle{row insertion} to be the function 

    \begin{algorithm}[H]
        \SetAlgoLined
        \SetKw{Kwin}{in}
        \SetKw{Kwif}{if}
        \SetKw{Kwelse}{else}
        \KwIn{$w_1 \leq \cdots \leq w_k$, $x$}
        \KwOut{$w'_1 \leq \ldots \leq w'_{k+1}$}
        \Begin{
            $j \mapsfrom \operatorname{argmin}_i{x < w_i}$ \Kwif $x < w_k$ \Kwelse $k+1$
        }
    \end{algorithm}
\end{definition}

Now we define the tableau that results from row insertion, $(P \leftarrow t)$.

We state a property of insertion paths that will allow us to prove that row insertion gives us SSYT.

\begin{theorem}
    Let $P$ be a SSYT of shape $\lambda$, let $t \in \PP$, and let $I(P\leftarrow t) = [j_1,j_2,\ldots,j_M]$. Then $I(P\leftarrow t)$ is weakly decreasing.
\end{theorem}

\begin{proof}
\end{proof}

\begin{corollary}\label{InsertionPreservesSSYT}
    If $P$ is a SSYT and $t \in \PP$, then $(P \leftarrow t)$ is a SSYT.
\end{corollary}

\begin{proof}
    By the definition of row insertion, the rows are weakly increasing. Consider inserting a bumped number. By the previous theorem, it can only be moved down or down left, which means that it will always be inserted below a smaller number. This continues for the whole insertion path.
\end{proof}

\subsection{Biwords}

\begin{definition}
    A \textit{biword} is a pair of strings of the same length.
\end{definition}

\begin{definition}
    Let $A \in \Mat_\infty(\NN)$.
    We define $A$'s \textit{biword}, which we denote $\biword A$, to be.

    This is also often denoted $w_A$.
\end{definition}

\begin{example}
    Let $A$ be the matrix
    \[
        \begin{bmatrix}
            1 & 0 & 3 & 2 \\
            0 & 1 & 0 & 0 \\
            2 & 4 & 1 & 0 \\
            0 & 0 & 1 & 0
        \end{bmatrix}.
    \]
    Then 
    \[
        \biword A = \begin{pmatrix}
            1 & 1 & 1 & 1 & 1 & 1 \
              & 2 \
              & 3 & 3 & 3 & 3 & 3 & 3 & 3\
              & 4 \\
            1 \
              & 2 & 2 & 2 \
              & 4 & 4 \
              & 2 \
              & 1 & 1 \
              & 2 & 2 & 2 & 2 \
              & 3 \
              & 3
        \end{pmatrix}
    \]
\end{example}

\subsection{The RSK algorithm}

\begin{definition}
    We call a pair of tableaux $(P,Q)$ a \textit{bitableau} if $\shape P = \shape Q$.
    The set of semistandard bitableaux will be denoted $\biSSYT$.
    That is,
    \[
        \biSSYT \coloneq \{(P,Q) \in \SSYT^2 : \shape P = \shape Q\}.
    \]
\end{definition}

\begin{definition}
    We define the \textit{Robinson-Schensted-Knuth correspondence} to be a rule that assigns to each finitely supported $\NN$ matrix $A \in \Mat_\infty(\NN)$ a pair of semistandard Young tableaux $(P, Q) \in \biSSYT$, which we notate as
    \[
        A \rskarrow (P, Q).
    \]
    The algorithm itself is as follows:

    \begin{algorithm}[H]
        \SetAlgoLined
        \SetKw{Kwin}{in}
        \KwIn{$A:\Mat_\infty(\NN)$}
        \KwOut{$(P,Q):\biSSYT$}
        \Begin{
            $(P,Q) \mapsfrom (\varnothing,\varnothing)$\;
            \For{$ij$ \Kwin $\biword A$}{
                Row insert $j$ into $P$, and add to $Q$ a box containing $i$ in the location $j$ landed in $P$.
            }
            \KwRet $(P,Q)$\;
        }
    \end{algorithm}
    $P$ is referred to as the \textit{insertion tableau} and $Q$ is referred to as the \textit{recording tableau}.
\end{definition}

The fact that $P$ is called the insertion tableaux has to do with the fact that it's a direct result of iterated row insertion, whereas $Q$ is called the recording tableaux because the entries inserted in $Q$ happen to just ``come along for the ride'' following the inserted entries.

\begin{example}
    Let 
    \[
        A \coloneq \begin{bmatrix}
            1 & 0 & 2 \\
            0 & 2 & 0 \\
            1 & 1 & 0
        \end{bmatrix}.
    \]
    We have that
    \[
        w_A = \begin{pmatrix}
            1 & 1 & 1 \
              & 2 & 2 \
              & 3 & 3 \\
            1 \
              & 3 & 3 \
              & 2 & 2 \
              & 1 & 2
        \end{pmatrix}.
    \]
    So, going through the insertion process,

    \ytableausetup{boxsize=10pt}
    \begin{center}
        \ytabmathmode
        \begin{tabular}{r | l | l}
            $t$ &  $P_t$ & $Q_t$ \\
            \hline 
            $1$ \
              & \begin{ytableau} 
                  \none
                \end{ytableau} \
              & \begin{ytableau} 
                  \none
              \end{ytableau} \\
            $2$ \
              & \begin{ytableau} 
                    *(green) 1
                \end{ytableau} \
              & \begin{ytableau} 
                  *(green) 1
              \end{ytableau} \\
            $3$ \
              & \begin{ytableau} 
                  1 & *(green) 3
                \end{ytableau} \
              & \begin{ytableau} 
                  1 & *(green) 1
              \end{ytableau} \\
            $4$ \
              & \begin{ytableau} 
                  1 & 3 & *(green) 3
                \end{ytableau} \
              & \begin{ytableau} 
                  1 & 1 & *(green) 1
              \end{ytableau} \\
            $5$ \
              & \begin{ytableau} 
                  1 & *(yellow) 2 & 3 \\
                  *(green) 3
                \end{ytableau} \
              & \begin{ytableau} 
                  1 & 1 & 1 \\
                  *(green) 2
              \end{ytableau} \\
            $6$ \
              & \begin{ytableau} 
                  1 & 2 & *(yellow) 2 \\
                  3 & *(green) 3
                \end{ytableau} \
              & \begin{ytableau} 
                  1 & 1 & 1  \\
                  2 & *(green) 2
              \end{ytableau} \\
            $7$ \
              & \begin{ytableau} 
                  1 & *(yellow) 1 & 2 \\
                  *(yellow) 2 & 3 \\
                  *(green) 3
                \end{ytableau} \
              & \begin{ytableau} 
                  1 & 1 & 1  \\
                  2 & 2 \\
                  *(green) 3
              \end{ytableau} \\
            $8$ \
              & \begin{ytableau} 
                  1 & 1 & 2 & *(green) 2 \\
                  2 & 3 \\
                  3
                \end{ytableau} \
              & \begin{ytableau} 
                  1 & 1 & 1 & *(green) 3 \\
                  2 & 2 \\
                  3
              \end{ytableau} \\
        \end{tabular} 
    \end{center}  

    In the above table, green squares are inserted, whereas yellow squares are bumped.
    We find that
    \[
        P = \ytableaushort{1122,23,3} \qquad Q = \ytableaushort{1113,22,3}.
    \]
\end{example}

\begin{theorem}
    RSK is a bijection $\Mat_\infty(\NN) \leftrightarrow \biSSYT$ such that for all $A \rskarrow (P,Q)$,
    \begin{align*}
        \content P &= \col A, \\
        \content Q &= \row A.
    \end{align*}
\end{theorem}

\begin{proof}
    That the contents of $P$ and $Q$ correspond to row and column sums of $A$ is obvious-- $j$ coordinates with multiplicities get inserted into $P$, while $i$ coordinates with multiplicities get inserted into $Q$.
    That $P$ is a SSYT follows from (\ref{InsertionPreservesSSYT}).
    That $Q$ is a SSYT follows from properties of the insertion path.

    Now, it remains to prove that the RSK correspondence is a bijection. 
    RSK can actually be run backwards. 
    First, we use this to prove injectivity, by showing that running RSK backwards is actually the inverse of RSK. 
    Second, we use this to prove surjectivity, by showing that backwards RSK works for arbitrary pairs of SSYT.

\end{proof}

\subsection{Standardization}

\todo{Rewrite standardization}

\begin{definition}
    Let $w_A = \binom{i_1\cdots i_n}{j_1\cdots j_n}$ be a two-line array arising from the $\NN$-matrix $A$.

    We define the \textit{standardized two-line array} $\widetilde{w}_A$ to be $w_A$ with both of its rows standardized. 
\end{definition}

In effect, we will always get 
\[
    \begin{pmatrix}
        1 & \cdots & n \\
        \widetilde{j_1} & \cdots & \widetilde{j_n}
    \end{pmatrix}.
\]
where $\widetilde{j_i}$ is $j_i$ standardized.

\begin{example}
    Combining the first two examples of standardized rows, if
    \[
        w_A = \begin{pmatrix}
            1 & 1 & 1 & 2 & 2 & 3 & 3 & 3 & 3 \\
            1 & 1 & 3 & 2 & 3 & 1 & 2 & 2 & 2
        \end{pmatrix},
    \]
    then
    \[
        \widetilde{w}_A = \begin{pmatrix}
            1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 \\
1 & 2 & 8 & 4 & 9 & 3 & 5 & 6 & 7
        \end{pmatrix}.
    \]
\end{example}

\begin{lemma}
    Standardization commutes with RSK.
    Meaning, if you keep track of the map of entries going from $w_A$ to $\widetilde{w}_A$, then reversing this map on $\widetilde{P},\widetilde{Q}$ gives you $P,Q$ respectively.
\end{lemma}

\subsection{Symmetry}

\begin{theorem}\label{thm:rsksymmetry}
    If $A \rskarrow (P,Q),$ then $A^\top \rskarrow (Q,P).$
\end{theorem}

The first proof given in \cite{Stanley} makes use of the \textit{inversion poset} of a permutation.

\begin{definition}
    Consider the two-line array
    \[
        w = \begin{pmatrix}
            i_1 & \cdots & i_k \\
            j_1 & \cdots & j_k
        \end{pmatrix}
    \]
    and define the \textit{inversion poset} $I$ to be the poset whose elements are the pairs $(i_t, j_t)$ for $1 \leq t \leq k$, and whose partial order is given by putting $(a,b) \leq (c,d)$ whenever $a \leq b$ and $c \leq d$.
\end{definition}

For convience, we will use the compact notation $i_tj_t$.
For example, the pair $(3,5)$ will just be written $35$.

\begin{example}\label{ex:invposet}
    Let
    \[
        w = \begin{pmatrix}
            1 & 2 & 3 & 4 & 5 & 6 & 7 \\
            5 & 1 & 6 & 7 & 4 & 3 & 2
        \end{pmatrix}.
    \]
    Then the inversion poset of $w$ is 
    \begin{center}
        \begin{tikzpicture}[scale=1.3]
            \node (15) at (0,0) {$15$};
            \node (21) at (1,0) {$21$};
            \node (36) at (0,1) {$36$};
            \node (47) at (0,2) {$47$};
            \node (54) at (1,1) {$54$};
            \node (63) at (2,1) {$63$};
            \node (72) at (3,1) {$72$};
            \draw (15) -- (36);
            \draw (21) -- (36);
            \draw (36) -- (47);
            \draw (21) -- (54);
            \draw (21) -- (63);
            \draw (21) -- (72);
        \end{tikzpicture}.
    \end{center}
\end{example}

Given an inversion poset $I$, we may partition it into antichains $I_1, \ldots, I_k$ as follows:
\begin{enumerate}
    \item Let $I_1$ be the set of minimal elements of $I$.
    \item Suppose that $I_1, \ldots, I_j$ are defined. 
        Then define $I_{j+1}$ to be the minimal elements of $I \setminus (I_1 \cup \cdots \cup I_j)$.
\end{enumerate}

\begin{example}\label{ex:invposetantichains}
    For the same $w$ defined in Example \ref{ex:invposet}, the antichains are
    \begin{center}
        \begin{tikzpicture}[scale=1.3]
            \node (15) at (0,0) {$15$};
            \node (21) at (1,0) {$21$};
            \node (36) at (0,1) {$36$};
            \node (47) at (0,2) {$47$};
            \node (54) at (1,1) {$54$};
            \node (63) at (2,1) {$63$};
            \node (72) at (3,1) {$72$};
            \draw (15) -- (36);
            \draw (21) -- (36);
            \draw (36) -- (47);
            \draw (21) -- (54);
            \draw (21) -- (63);
            \draw (21) -- (72);
            \node[red] (i1) at (-0.5,0) {$I_1$};
            \node[red] (i2) at (-0.5,1) {$I_2$};
            \node[red] (i3) at (-0.5,2) {$I_3$};
            \draw[thick,red,rounded corners] (-0.3, -0.3) rectangle (1.3, 0.3) {};
            \draw[thick,red,rounded corners] (-0.3, 0.7) rectangle (3.3, 1.3) {};
            \draw[thick,red,rounded corners] (-0.3, 2.3) rectangle (0.3, 1.7) {};
        \end{tikzpicture}.
    \end{center}
\end{example}

Moreover, considering the definition of the inversion poset's order, we may define an order on its antichains, given by $(a,b) < (c,d)$ whenever $a < c$ and $b > d$.

This defines a total order on any antichain of $I$ (check it!).

\begin{example}
    Continuing Example \ref{ex:invposetantichains}, the order internal to each antichain (going east to west from least to greatest) is
    \begin{center}
        \begin{tikzpicture}[scale=1.3]
            \node (15) at (0,0) {$15$};
            \node (21) at (1,0) {$21$};
            \node (36) at (0,1) {$36$};
            \node (47) at (0,2) {$47$};
            \node (54) at (1,1) {$54$};
            \node (63) at (2,1) {$63$};
            \node (72) at (3,1) {$72$};
            \draw (15) -- (36);
            \draw (21) -- (36);
            \draw (36) -- (47);
            \draw (21) -- (54);
            \draw (21) -- (63);
            \draw (21) -- (72);
            \node[red] (i1) at (-0.5,0) {$I_1$};
            \node[red] (i2) at (-0.5,1) {$I_2$};
            \node[red] (i3) at (-0.5,2) {$I_3$};
            \draw[thick,red,rounded corners] (-0.3, -0.3) rectangle (1.3, 0.3) {};
            \draw[thick,red,rounded corners] (-0.3, 0.7) rectangle (3.3, 1.3) {};
            \draw[thick,red,rounded corners] (-0.3, 2.3) rectangle (0.3, 1.7) {};
            \draw[red] (15) -- (21);
            \draw[red] (36) -- (54) -- (63) -- (72);
        \end{tikzpicture},
    \end{center}
    where the covering relation is notated with red edges.
\end{example}

We can now state an important lemma that makes use of all the machinery we've just built up.

\begin{lemma}\label{lem:invposetfirstrowrsk}
    Consider a biword $w$, and let $w \rskarrow (P, Q)$.
    Suppose we've constructed $I$ and $I_1, \ldots I_k$ given $w$.
    Then, the first row of $P$ has $k$ entries, and is given by
    \[
        P_{1,r} = \text{the top number of }\min I_r,
    \]
    and the first row of $Q$ also has $k$ entries and is given by
    \[
        Q_{1,r} = \text{the bottom number of }\max I_r.
    \]
\end{lemma}

\begin{example}
    Completing the example, we compute the top and bottom numbers of the least and greatest elements of each antichain.
    \begin{center}
        \begin{tikzpicture}[scale=1.3]
            \node (15) at (0,0) {$15$};
            \node (21) at (1,0) {$21$};
            \node (36) at (0,1) {$36$};
            \node (47) at (0,2) {$47$};
            \node (54) at (1,1) {$54$};
            \node (63) at (2,1) {$63$};
            \node (72) at (3,1) {$72$};
            \draw (15) -- (36);
            \draw (21) -- (36);
            \draw (36) -- (47);
            \draw (21) -- (54);
            \draw (21) -- (63);
            \draw (21) -- (72);
            \begin{scope}[thick,red,rounded corners]
                \draw (-0.3, -0.3) rectangle (1.3, 0.3) {};
                \draw (-0.3, 0.7) rectangle (3.3, 1.3) {};
                \draw (-0.3, 2.3) rectangle (0.3, 1.7) {};
            \end{scope}
            \begin{scope}[color=red]
                \draw (15) -- (21);
                \draw (36) -- (54) -- (63) -- (72);
            \end{scope}
            \begin{scope}[color=blue,-{Latex[length=3mm]}]
                \draw (-0.5,2) -- (-1.5,2);
                \draw (-0.5,1) -- (-1.5,1);
                \draw (-0.5,0) -- (-1.5,0);
            \end{scope}
            \begin{scope}[color=green,-{Latex[length=3mm]}]
                \draw (0.5,2) -- (4.5,2);
                \draw (3.5,1) -- (4.5,1);
                \draw (1.5,0) -- (4.5,0);
            \end{scope}
            \begin{scope}
                \node at (-2, 2) {$4$};
                \node at (-2, 1) {$3$};
                \node at (-2, 0) {$1$};
            \end{scope}
            \begin{scope}
                \node at (5, 2) {$7$};
                \node at (5, 1) {$2$};
                \node at (5, 0) {$1$};
            \end{scope}
        \end{tikzpicture}
    \end{center}
    So the first row of $P$ is $\ytableaushort{134}$ and the first row of $Q$ is $\ytableaushort{127}$.
\end{example}

\todo{Finish proofs of RSK symmetry}
\begin{proof}[Proof of Theorem \ref{thm:rsksymmetry} using the inversion poset]

    Lemma \ref{lem:invposetfirstrowrsk} already shows us the first row becomes the first column.
    Then, we need to turn this into a row-by-row argument, and so we have to inspect closely all the bumped elements.
    

\end{proof}

\begin{proof}[Proof of Theorem \ref{thm:rsksymmetry} using Fomin growth diagrams]
\end{proof}

\subsection{Dual RSK}

\begin{definition}[Column insertion]
    We define \textit{column insertion}
\end{definition}

\section{Schur functions, continued}

\subsection{Cauchy identities}
We finally return to the 

\begin{theorem}
    [Cauchy identity]    
    We have
    \begin{align}
        \label{eq:CauchyIdentityForSchurs}
        \prod_{i,j \geq 1}(1-x_iy_j)^{-1}
        &=
        \sum_{\lambda\in\Par}
        s_\lambda(\bfx)s_\lambda(\bfy).
        \\
        \prod_{i,j \geq 1}(1+x_iy_j)
        &=
        \sum_{\lambda\in\Par}
        s_\lambda(\bfx)s_{\lambda^\top}(\bfy).
    \end{align}
\end{theorem}

\begin{proof}
    We have that
    \begin{align*}
        \left[\bfx^\alpha \bfy^\beta\right]
        \left(
            \prod_{i,j\geq1}(1-x_iy_j)^{-1}
        \right)
        &=
        \left(
            \#A \in \Mat_\infty(\NN) \text{ such that } \row A = \alpha, \col A = \beta
        \right), \\
        \left[\bfx^\alpha \bfy^\beta\right]
        \left(
            \sum_{\lambda\in\Par} 
            s_\lambda(\bfx) s_\lambda(\bfy)
        \right) 
        &= 
        \left(
            \#(P,Q) \in \biSSYT \text{ such that } \ct P = \alpha, \ct Q = \beta
        \right).
    \end{align*}
    RSK tells us that both counts are the same, hence both sides of the identity are equal as power series, since their coefficients agree for all monomials.
\end{proof}

\begin{corollary}
    The Schurs are an orthonormal basis of $\Lambda$.
\end{corollary}

\begin{proof}
    This follows from Theorem \ref{thm:CauchyIdentityImpliesOrthonormal}, which we recall says that \textit{any} two families of symmetric functions which satisfy a Cauchy identity are orthonormal bases of the ring of symmetric functions with respect to the Hall inner product.
\end{proof}

\begin{corollary}
    \[
        \sum_{\lambda \in \Par} K_{\lambda\mu}K_{\lambda\nu} = \langle h_\mu, h_\nu \rangle.
    \]
\end{corollary}

\begin{proof}
    Take the coefficient of $\bfx^\mu \bfy^\nu$ in (\ref{eq:CauchyIdentityForSchurs}).
\end{proof}

\begin{corollary}\label{thm:HToSIsKostka}
    We have that
    \[
        h_\mu = \sum_{\lambda \in \Par} K_{\lambda\mu}s_\lambda.
    \]
    Equivalently,
    \[
        \langle s_\lambda, h_\mu \rangle = K_{\lambda\mu}.
    \]
\end{corollary}

\cite{Stanley} gives three proofs of this corollary. The first one is the slickest, since it compactifies a lot of the underlying mechanics using the Hall inner product.

\begin{proof}[Proof via the Hall inner product]
    We know that
    \[
        s_\lambda = \sum_{\nu\in\Par} K_{\lambda\nu} m_\nu,
    \]
    so 
    \begin{align*}
        \langle h_\mu, s_\lambda \rangle &= \left\langle h_\mu, \sum_\nu K_{\lambda\nu} m_\nu \right\rangle \\
                                         &=\sum_{\nu} K_{\lambda\nu} \langle h_\mu, m_\nu \rangle \\
                                         &=\sum_{\nu} K_{\lambda\nu} \delta_{\mu\nu} \\
                                         &= K_{\lambda\mu}.
    \end{align*}
\end{proof}

This next one is really the same thing, but packaged differently.

\begin{proof}[Proof via Cauchy Identities]
    We have that
    \[
        \sum_\lambda 
        m_\lambda(\bfx) h_\lambda(\bfy) 
        = 
        \sum_\lambda 
        s_\lambda(\bfx) s_\lambda(\bfy),
    \]
    since both sides equal $\prod_{i,j}(1-x_iy_j)^{-1}$.
    So we have that
    \begin{align*}
        \sum_\mu 
        m_\mu(\bfx)h_\mu(\bfy) 
        &= 
        \sum_\lambda 
        \left( 
            \sum_\mu K_{\lambda\mu} m_\mu(\bfx) 
        \right) 
        s_\lambda(\bfy) 
        \\
        &= 
        \sum_\lambda \sum_\mu 
        m_\mu(\bfx) K_{\lambda\mu} s_\lambda(\bfy) \\
        &= 
        \sum_\mu 
        m_\mu(\bfx) 
        \left(\sum_\lambda K_{\lambda\mu} s_\lambda(\bfy)\right).
    \end{align*}
    We already know that the $m_\mu(\bfx)$'s are linearly independent; we finish the proof by equating their coefficients.
\end{proof}

This last proof is purely combinatorial, but is also again really the same thing. 

\begin{proof}[Proof via RSK]
    \begin{align*}
        h_\mu 
        &= 
        \sum_{\substack{A\in\Mat_\infty(\NN)\\\row A=\mu}} 
        \bfx^{\col A}
        \\
        &=
        \sum_{\substack{(P,Q)\in\biSSYT\\\content P=\mu}} 
        \bfx^{\content Q}
        \\
        &=
        \sum_{\lambda\in\Par}
        \left(
            \sum_{\substack{P,Q\in\SSYT_{\shape \lambda}\\\ct P=\mu}} 
            \bfx^{\ct Q}
        \right)
        \\
        &=
        \sum_{\lambda\in\Par}
        \left(
            \sum_{P\in\SSYT_{\sh \lambda, \ct \mu}} 
            \left(
                \sum_{Q\in\SSYT_{\sh \lambda}} \bfx^{\ct Q}
            \right)
        \right) 
        \\
        &=
        \sum_{\lambda\in\Par}
        \left(
            \substack{\#\text{of SSYT with}\\\text{shape $\lambda$ and content $\mu$}}
        \right)
        \sum_{Q\in\SSYT_{\sh \lambda}} 
        \bfx^{\ct Q} 
        \\
        &= 
        \sum_{\lambda\in\Par} 
        K_{\sh\lambda,\ct\mu} 
        \left(\sum_{Q\in\SSYT_{\sh \lambda}} \bfx^{\ct Q}\right) 
        \\
        &= \sum_{\lambda\in\Par} K_{\lambda\mu} s_\lambda.
    \end{align*}
\end{proof}

\begin{corollary}
    We have that
    \[
        h_{1^n} = \sum_{\lambda \vdash n} f^\lambda s_\lambda.
    \]
\end{corollary}

\begin{proof}
    Combine Corollary \ref{thm:HToSIsKostka} and the fact that $f^\lambda = K_{\lambda,1^n}$.
\end{proof}

\subsection{The Pieri rule}

We have a special case of the forthcoming \defstyle{Littlewood-Richardson rule}, called the \defstyle{Pieri rule}.

\begin{theorem}[Pieri rule]
    Let $n \geq 0$, and let $\lambda \in \Par$.
    Then
    \[
        h_ns_\lambda = \sum_{\substack{\mu}} s_\mu
    \]
    where $\mu$ ranges over all partitions obtained by adding a horizontal strip to $\lambda$, where a \defstyle{horizontal strip} is a skew diagram such that each column contains at most one box.
\end{theorem}

We have a slick algebraic proof:

\begin{proof}
    [Proof by adjointness of skewing and right-multiplying.]
    In other words, we wish to show that
    \[
        \InnerProduct{h_ns_\lambda,s_\mu}
        =
        \begin{cases}
            1 & \text{if $\mu$ is $\lambda$ plus a horizontal strip} \\
            0 & \text{otherwise}.
        \end{cases}
    \]
    We use the fact that skewing and multiplying by a Schur are adjoint, so
    \[
        \InnerProduct{h_ns_\lambda,s_\mu}
        =
        \InnerProduct{h_n,s_{\mu/\lambda}}.
    \]
    But we know this quantity--- it is a Kostka number
    \[
        \InnerProduct{h_n,s_{\mu/\lambda}}
        =
        K_{\shape \mu/\lambda, \content n}.
    \]
    Now, $\mu / \lambda$ must be a skew tableaux filled with $n$ $1$'s.
    This can only be possible if $\mu$ is $\lambda$ piled with a horizontal strip of size $n$.
\end{proof}

However, this is really a manifestation of the \textit{bumping lemma} we just proved.

\begin{proof}
    [Proof using Schensted insertion]
\end{proof}




\subsection{The Murnaghan-Nakayama rule}

The Pieri rule expresses a relationship between the Schurs and the $e_\lambda$'s and $h_\lambda$'s.

The \defstyle{Murnaghan-Nakayama rule} is an analogous rule with the $p_\lambda$'s.


\begin{theorem}[Murnaghan-Nakayama rule] 
    Let $\mu \in \Par$ and $r \in \NN$.
    Then
    \[
        s_\mu p_r = \sum_{\substack{\lambda\supseteq\mu \\ \lambda / \mu \text{ is a BST of size }r}}(-1)^{\opheight \lambda / \mu} s_\lambda.
    \]
\end{theorem}

\begin{proof}
    \todo{Proof of Murnaghan-Nakayama}
\end{proof}

\section{The tableau ring}

\subsection{Multiplication on tableaux}

\begin{definition}
    The \defstyle{plactic monoid} consists of the set of all semistandard Young tableaux, with the multiplication of tableau given by
    \[
        S \cdot T
        \coloneq
        S \leftarrow \rowword T.
    \]
\end{definition}


\subsection{Knuth equivalence}

\begin{definition}
    Given a word, an \textit{elementary Knuth transformation} is a replacement of a substring of three consecutive letters given by the following rules
    \[
        \begin{cases}
            abc \equiv bac, \\
            cab \equiv cba, \\
            baa \equiv aba, \\
            bba \equiv bab,
        \end{cases}
    \]
    where $a < b < c$.
\end{definition}

These can be pictorially remembered as the two different words which insert into the same tableaux

\begin{definition}
    We say that two words $u$ and $v$ are \textit{Knuth-equivalent} whenever there is a sequence of \textit{elementary Knuth transformations} which turns one into another.
\end{definition}

We state an important property of Knuth-equivalence

\begin{theorem}
    Two words are Knuth equivalent if and only if their insertion tableaux coincide.
\end{theorem}

\begin{corollary}
    The plactic monoid is the free monoid modulo Knuth equivalence.
\end{corollary}


\subsection{Jeu de taquin}

\begin{definition}
    A \defstyle{jeu-de-taquin slide} is
\end{definition}

\subsection{The Littlewood-Richardson rule}

\section{Representation theory of the symmetric group}

\subsection{The symmetric group action on tableaux}

\begin{definition}
    An \defstyle{$n$-tableau} is a tableau whose multiset of entries are precisely the numbers $\{1,\ldots,n\}$.
    We will denote the set of all $n$-tableaux by $\YT(n)$.
    Specifically, we have defined
    \[
        \YT(n)
        \coloneq
        \{\text{$T$ tableau} : \content T = 1^n\}.
    \]
\end{definition}

Let $T \in \YT(n)$.
Recall that $T$ can viewed as a bijective function $\sh T \to [n]$.
Then, if $\pi \in S_n$, we can define the left action of $\pi$ by (literal) composition $\pi \circ T$--- we obtain another another tableau of the same shape and content.
What this all means is that, given a box of $T$ with the letter $i$ in it, we replace it with the letter $\pi(i)$, which defines the following:

\begin{definition}
    The symmetric group $S_n$ has a left action on the set of $n$--tableaux $\YT(n)$ defined by putting $(\pi \cdot T)_{ij} = \pi (T_{ij})$.
    This means that the entry in box $i,j$ of $\pi \circ T$ is the entry in box $i,j$ of $T$ under the permutation $\pi$.
\end{definition}

\subsection{Young modules}

\begin{definition}
    Let $T \in \YT(n)$.
    \begin{enumerate}[label=(\alph*)]
        \item 
    The \defstyle{row subgroup} $R_T$ of $S_n$ is the subgroup consisting of permutations $\sigma$ which leave the content of each row of $T$ invariant.
        \item 
            Similarly, the \defstyle{column subgroup} $C_T$ of $S_n$ is the subgroup consisting of permutations $\sigma$ which leave the content of each column of $T$ invariant.
    \end{enumerate}
\end{definition}

\begin{definition}
    We say that two tableaux $S,T \in \YT(n)$ of the same shape are \defstyle{row equivalent} if $S = \sigma T$ for some $\sigma \in R(T)$.
    We define \defstyle{column equivalence} the same way.
\end{definition}


\begin{definition}
    A \defstyle{tabloid} is an \textit{equivalence class} of tableau up to row equivalence.
    The action of $S_n$ on tabloids is induced by the action of $S_n$ on tableaux.

    If $T$ is a tableau, its tabloid equivalence class will be denoted $\overline{T}$.
\end{definition}

This action is well-defined--- if $S,T$ are row-equivalent, then so are $\sigma S, \sigma T$.

\begin{definition}
    For any $\lambda \vdash n$, we define the \defstyle{Young module} $M^\lambda$ to be the $\CC S_n$-module with basis are tabloids of shape $\lambda$.
\end{definition}

\subsection{Specht modules}

\begin{definition}
    Let $T \in \YT(n)$.
    \begin{enumerate}[label=(\alph*)]
        \item 
            The \defstyle{row symmetrizer} $\nabla_{\row T}$ is defined by
            \[
                \nabla_{\row T}
                \coloneq
                \sum_{\sigma \in R(T)}
                \sigma.
            \]
        \item 
            The \defstyle{column anti-symmetrizer} $\nabla_{\col T}$ is defined by
            \[
                \nabla_{\col T}^-
                \coloneq
                \sum_{\sigma \in C(T)}
                \sigma.
            \]
        \item 
            The \defstyle{Young symmetrizer} $\Delta_T$ is the product $\nabla_{\col T}^- \nabla_{\row T}$.
    \end{enumerate}
\end{definition}

\begin{definition}
    Let $\lambda$ be a partition.
    The \defstyle{Specht module} $S^\lambda$ is defined to be the subspace of $M^\lambda$ given by
    \[
        S^\lambda
        \coloneq
        \operatorname{span}\
        \left\{
            \nabla_{\col T}^- \cdot \overline{T} :
            \overline{T} \in M^\lambda
        \right\}.
    \]
\end{definition}

\subsection{The Frobenius characteristic isometry}

\section{Representation theory of the general linear group}

\subsection{Schur functors}

\section{Schubert calculus of the Grassmannian}

\subsection{Basic definitions}

\begin{definition}
    A \defstyle{flag}
\end{definition}

\section{Appendix}

\subsection{Detailed proofs}

\begin{proof}
    [Detailed proof of Theorem \ref{thm:e2mCombInterpretation}]
    $e_\lambda = e_{\lambda_1}e_{\lambda_2}\cdots$ is \footnote{The definition which quantifies over $i_1 < \cdots < i_{\lambda_j}$ is actually just a slick way to pick $\lambda_j$ distinct integers--- what the definition ``morally'' is.
    In this proof we decode this once, but it will not be done again in subsequent proofs, and it'll be implicitly understood}
    \[
        \left(\sum_{i_1<\ldots<i_{\lambda_1}} x_{i_1}\cdots x_{i_{\lambda_1}}\right)
        \left(\sum_{j_1<\ldots<j_{\lambda_2}} x_{j_1}\cdots x_{j_{\lambda_2}}\right)\cdots
    \]
    Consider the first sum.
    It must be that each $i_k$ is distinct for $1 \leq k \leq \lambda_1$, so in fact we are picking $\lambda_1$ distinct positive integers--- this is a subset of $\NN$ of size $\lambda_1$.
    Using this to continue the calculation, we have that
    \[
        e_\lambda 
        =
        \left(\sum_{\substack{S:\lambda_1\text{ sized}\\\text{subset of }\NN}}x_1^{[1\in S]^?}x_2^{[2\in S]^?}\cdots\right)
        \left(\sum_{\substack{T:\lambda_2\text{ sized}\\\text{subset of }\NN}}x_1^{[1\in T]^?}x_2^{[2\in T]^?}\cdots\right)
        \cdots
    \]
    Where we have used the Iverson bracket to pick out the $x$'s we need.

    Recall that subsets of a set $X$ are in bijection with \textit{indicator functions}, functions $X \to \{0,1\}$ which encode membership.
    In this case, such an indicator function for a subset of $\NN$ is just a zero-one sequence: a function $\NN \to \{0,1\}$.

    Then, we can further recast the product to be
    \[
        e_\lambda 
        =
        \left(\sum_{\substack{(a_i)_{i\geq 1}\text{ is a}\\\text{zero-one sequence} \\ \sum_i a_i = \lambda_1}}x_1^{a_1}x_2^{a_2}\cdots\right)
        \left(\sum_{\substack{(b_i)_{i\geq 1}\text{ is a}\\\text{zero-one sequence} \\ \sum_i b_i = \lambda_2}}x_1^{b_1}x_2^{b_2}\cdots\right)
        \cdots
    \]
    We're almost there.
    Next, we expand this product, and we will use suggestive notation: to name a term in this product, we pick out a zero-one sequence $(a_{i,j})$ for each factor $e_{\lambda_j}$, of which there are $\len(\lambda)$ many, so if we let $m=\len(\lambda)$, we can pick out sequences
    \[
        \Big((a_{i,1})_{i\geq1},\ldots,(a_{i,m})_{i\geq1}\Big)
    \]
    where $\sum_i (a_{i,j}) = \lambda_j$.
    And, if we stack these together, treating each sequence as a row, we have a two-dimensional array
    \[
        (a_{ij})_{i,j \geq 1}
    \]
    where we have padded the columns past $m$ with zero sequences.
    This is a zero-one matrix!
    Then, the condition that $\sum_i (a_{i,j}) = \lambda_j$ becomes $\row(a_{ij}) = \lambda$.

    Our product is now expressed as
    \begin{align*}
        e_\lambda 
        &=
        \sum_{\substack{(a_{ij})\in\Mat_\infty(01)\\\row(a_{ij}) = \lambda}} (x_1^{a_{11}}x_2^{a_{21}}\cdots)(x_1^{a_{21}}x_2^{a_{22}}\cdots)\cdots \\
        &= 
        \sum_{\substack{(a_{ij})\in\Mat_\infty(01)\\\row(a_{ij}) = \lambda}} x_1^{\left(\sum_i a_{i1}\right)}x_2^{\left(\sum_i a_{i2}\right)} \cdots \\
        &= 
        \sum_{\substack{(a_{ij})\in\Mat_\infty(01)\\\row(a_{ij}) = \lambda}} \bfx^{\col(a_{ij})}
    \end{align*}
    And now we're almost there.
    It's clear that if we pick out the coefficient of $\mu$ in the above sum, it must have come from a zero-one matrix whose column sum is $\mu$.
    This completes the proof.
\end{proof}


\begin{thebibliography}{999999}
    \raggedright\footnotesize

    \bibitem[Stanley]{Stanley}
    Richard P. Stanley, 
    \textit{Enumerative Combinatorics. Volume 2}, 
    Cambridge University Press 2023

    \bibitem[Fulton]{Fulton}
    William Fulton, 
    \textit{Young Tableaux}, 
    Cambridge University Press 1996

    \bibitem[Macdonald]{Macdonald}
    Ian G. Macdonald,
    \textit{Symmetric functions and Hall polynomials}, 
    Oxford University Press 1995

    \bibitem[GrinbergAC]{DarijAC}
    Darij Grinberg, 
    \textit{An Introduction to Algebraic Combinatorics}, 
    \url{http://www.cip.ifi.lmu.de/~grinberg/t/21s/lecs.pdf}

    \bibitem[GrinbergSGA]{DarijSGA}
    Darij Grinberg, 
    \textit{An introduction to the symmetric group algebra}, 
    \url{http://www.cip.ifi.lmu.de/~grinberg/t/24s/sga.pdf}

    \bibitem[Wildon]{Wildon}
    Mark Wildon, 
    \textit{An Involutive Introduction to Symmetric Functions},
    \url{http://www.ma.rhul.ac.uk/~uvah099/Maths/Sym/SymFuncs2020.pdf}
\end{thebibliography}

\end{document}
