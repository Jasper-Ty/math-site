% !TEX TS-program = xelatex

\documentclass{article}

% See https://github.com/Jasper-Ty/dotfiles
\usepackage[garamond]{jaspercommon}
\usepackage{tikz-cd}

\newcommand*\wc{{\mkern 1mu-\mkern 1mu}}
\newcommand{\innerproduct}[2]{\ensuremath{\left\langle #1 , #2 \right\rangle}}
\newcommand{\ip}[1]{\ensuremath{\left\langle{#1}\right\rangle}}
\newcommand{\lb}[1]{\ensuremath{\left[{#1}\right]}}

\newcommand{\iverson}[1]{\ensuremath{\left[{#1}\right]^?}}

\DeclareMathOperator{\s}{s}
\DeclareMathOperator{\ogroup}{O}
\DeclareMathOperator{\Dih}{Dih}
\DeclareMathOperator{\End}{End}
\DeclareMathOperator{\Hom}{Hom}
\DeclareMathOperator{\Sym}{Sym}
\DeclareMathOperator{\Mat}{Mat}
\DeclareMathOperator{\Lie}{Lie}
\DeclareMathOperator{\lie}{lie}
\DeclareMathOperator{\tr}{tr}

\DeclareMathOperator{\im}{im}
\DeclareMathOperator{\rank}{rank}
\DeclareMathOperator{\nullity}{nullity}
\DeclareMathOperator{\id}{id}
\DeclareMathOperator{\opspan}{span}

% The algebra of derivations
\DeclareMathOperator{\Der}{Der}

% The adjoint representation of a Lie algebra
\DeclareMathOperator{\ad}{ad}

% An anonymous differential
\newcommand{\dd}{\ensuremath{\text{d}}}

% Zoo of fraktur letter shorthands
\newcommand{\frka}{{\ensuremath{\mathfrak{a}}}}
\newcommand{\frkb}{{\ensuremath{\mathfrak{b}}}}
\newcommand{\frkg}{{\ensuremath{\mathfrak{g}}}}
\newcommand{\frkh}{{\ensuremath{\mathfrak{h}}}}
\newcommand{\frku}{{\ensuremath{\mathfrak{u}}}}
\newcommand{\frki}{{\ensuremath{\mathfrak{i}}}}
\newcommand{\frkj}{{\ensuremath{\mathfrak{j}}}}
\newcommand{\frkU}{{\ensuremath{\mathfrak{U}}}}

\newcommand{\GL}{\ensuremath{\text{GL}}}
\newcommand{\SL}{\ensuremath{\text{SL}}}

\newcommand{\glalg}{\ensuremath{\mathfrak{gl}}}
\newcommand{\slalg}{\ensuremath{\mathfrak{sl}}}
\newcommand{\spalg}{\ensuremath{\mathfrak{sp}}}
\newcommand{\talg}{\ensuremath{\mathfrak{t}}}
\newcommand{\dalg}{\ensuremath{\mathfrak{d}}}
\newcommand{\nalg}{\ensuremath{\mathfrak{n}}}
\newcommand{\oalg}{\ensuremath{\mathfrak{o}}}

\title{Lie algebras}
\author{Jasper Ty}
\date{}

\titleauthorhead

\begin{document}

\maketitle

\section*{What is this?}

These are notes based on my reading of Humphreys's ``Introduction to Lie Algebras and Representation Theory''.

\tableofcontents

\newpage

\section{Basic definitions and examples}

\begin{convention}
    All vector spaces considered are finite dimensional and no assumptions are made yet about underlying fields.
    We use $V$ and $\KK$ to denote generic vector spaces and fields respectively.

    We will often use $\rightharpoonup$ to denote action in general, so if $v \in V$ and $x \in \End V$, we will define
    \[
        x \rightharpoonup v
        \coloneq
        x(v).
    \]
\end{convention}

\subsection{Lie algebras}

\begin{definition}
    A \defstyle{Lie algebra} $\frkg$ is a vector space equipped with a product
    \begin{align*}
        \lb{\wc,\wc}: \frkg \times \frkg
        &\to
        \frkg,
        \\
        (x,y)
        &\mapsto
        \lb{xy},
    \end{align*}
    such that
    \begin{enumerate}[label=(L\arabic*)]
        \item 
            \label{ax:LBIsBilinear}
            $\lb{\wc,\wc}$ is bilinear,
        \item 
            \label{ax:LBNilpotent}
            $\lb{xx} = 0$ for all $x \in \frkg$, and
        \item 
            \label{ax:LBJacobiIdentity}
            $\lb{x\lb{yz}} + \lb{y\lb{zx}} + \lb{z\lb{xy}} = 0$.
    \end{enumerate}
    We refer to $\lb{xy}$ as the \defstyle{bracket} or the \defstyle{commutator} of $x$ and $y$.
\end{definition}

\ref{ax:LBJacobiIdentity} is referred to as the \textit{Jacobi identity}.

As an exercise in using this definition, we show the following:

\begin{proposition}
    Brackets are anticommutative, i.e
    \begin{equation}
        [xy]
        =
        -[yx].
        \tag{L2'}
    \end{equation}
    is a relation in any Lie algebra.
\end{proposition}
\begin{proof}
    By \ref{ax:LBNilpotent}, we have that
    \[
        \lb{x+y,x+y}
        =
        0,
    \]
    and by \ref{ax:LBIsBilinear},
    \[
        \lb{xx} + \lb{xy} + \lb{yx} + \lb{yy}
        =
        0.
    \]
    By \ref{ax:LBNilpotent} again,
    \[
        \lb{xy} + \lb{yx}
        =
        0,
    \]
    which completes the proof.
\end{proof}

We will look at our first example of a Lie algebra, closely associated with the \defstyle{general linear group} $\GL(V)$ of invertible endomorphisms of a vector space $V$.

\begin{definition}[$\glalg$, abstractly]
    Let $V$ be a vector space.
    The \defstyle{general linear algebra} $\glalg(V)$ is defined to be the Lie algebra with underlying vector space $\End V$ and bracket given by
    \[
        \lb{xy}
        =
        xy - yx
    \]
    defined with $\End V$'s natural ring structure.
\end{definition}

    $\End V$'s aforementioned ring structure is exactly that of $n \times n$ matrices, where $n = \dim V$.
    Then, the following definition gives us a more concrete avatar of $\glalg$, and is in a sense ``the only'' finite dimensional $\glalg$.

\begin{definition}[$\glalg$, concretely]
    Let $\KK$ be some field and let $n$ be a positive integer.
    The \defstyle{general linear algebra} $\glalg_n(\KK)$ is defined
    \[
        \glalg_n(\KK)
        \coloneq
        \glalg\big(\Mat_n(\KK)\big).
    \]
\end{definition}

In this setting, we can easily compute the bracket of $\glalg$ relative to its standard basis:

\begin{proposition}
    Let $\{e_{pq}\}_{p,q = 0}^n$ be the standard basis of $\glalg_n(\KK)$.
    Then
    \[
        \lb{e_{pq}e_{rs}}
        =
        \delta_{qr}e_{ps}
        -
        \delta_{sp}e_{rq},
    \]
    where $\delta$ is the Kronecker delta.
\end{proposition}

\begin{proof}
    Using the Iverson bracket (see Definition \ref{def:Iverson}),
    \[
        (e_{pq})_{ij}
        =
        \iverson{p=i \wedge q=j}
        =
        \iverson{p=i}\iverson{q=j}
    \]
    and so
    \begin{align*}
        (e_{pq}e_{rs})_{ij}
        &=
        \sum_{k=1}^n
        (e_{pq})_{ik}
        (e_{rs})_{kj}
        \\
        &=
        \sum_{k=1}^n
        \iverson{p=i \wedge q=k}
        \iverson{r=k \wedge s=j}
        \\
        &=
        \sum_{k=1}^n
        \Big(
            \iverson{q=k}\iverson{r=k}
        \Big)
        \iverson{p=i}
        \iverson{s=j}
        \\
        &=
        \left(
            \sum_{k=1}^n
            \iverson{q=r=k}
        \right)
        \iverson{p=i \wedge s=j}
        \\
        &=
        \delta_{qr}
        (e_{ps})_{ij}.
    \end{align*}
    So $e_{pq}e_{rs} = \delta_{qr}e_{ps}$. 
    Similarly,
    $e_{rs}e_{pq} = \delta_{sp}e_{rq}$.
\end{proof}

Importantly, many Lie algebras, and in fact all the Lie algebras we are concerned with, occur as subalgebras of the general linear algebra--- a \defstyle{subalgebra} of a Lie algebra $\frkg$ is a subspace of $\frkg$ that is closed under $\frkg$'s bracket.

\begin{definition}
    A \defstyle{linear Lie algebra} is a subalgebra of $\glalg_n(\KK)$ for some $n$.
\end{definition}

All finite dimensional Lie algebras are linear, in the sense that they are isomorphic to some linear Lie algebra.

\subsection{Examples}

We have four distinguished families of Lie algebras:
\[
    \sfA_\ell,\qquad
    \sfB_\ell,\qquad
    \sfC_\ell,\qquad
    \sfD_\ell.
\]
These are parameterized by a positive integer $\ell$, and they classify all but five of the so-called \defstyle{semisimple Lie algebras}.

\subsubsection{Type \sfA: the special linear algebra}

\begin{definition}
    Let $V$ be a vector space with basis $\bfv = (v_1, \ldots, v_n)$ and dual basis $\bfv^\ast = (v^1, \ldots, v^n)$
    The \defstyle{trace} $\tr x$ of an endomorphism $x \in \End V$ of $V$ is defined to be the sum
    \[
        \sum_{i=1}^n 
        v^i\Big(x(v_i)\Big).
    \]
\end{definition}

In other words, it is the sum of the diagonal entries of the matrix representation of $x$.
The trace is independent of the basis used to compute it (see Theorem \ref{thm:TraceIsBasisIndependent} in the Appendix), hence it is a well defined quantity.

\begin{definition}[The type $\sfA_\ell$ Lie algebra]
    Let $V$ have dimension $n = \ell + 1$.
    We define $\sfA_\ell$ to be the \defstyle{special linear algebra} $\slalg(V)$, the set of all \defstyle{traceless} endomorphisms of $V$, which means
    \[
        A_\ell
        \coloneq
        \slalg(V)
        \coloneq
        \Big\{
            x \in \glalg(V) : \tr x = 0
        \Big\}.
    \]
    As is the case with $\glalg(V)$ and $\glalg_n(\KK)$, we also define 
    \[
        A_\ell
        \coloneq
        \slalg_{\ell+1}(\KK)
        \coloneq
        \Big\{
            x \in \glalg_{\ell+1}(\KK) : \tr x = 0
        \Big\}
    \]
    and will refer to them interchangeably.
\end{definition}

This algebra is so named because of its connection with the \defstyle{special linear group} $\SL(V)$, a distinguished subgroup of $\GL(V)$.
Unsurprisingly, $\slalg(V)$ shares a similar relationship to $\glalg(V)$.

\begin{proposition}
    $\slalg(V)$ is a subalgebra of $\glalg(V)$.
\end{proposition}
\begin{proof}
    The trace is a linear operator $\tr: \glalg_n(\KK) \to \KK$.
    Since the kernel of a linear operator is a subspace of its domain, we conclude that $\slalg_n(\KK) = \ker \tr$ is a subspace of $\glalg$.

    Finally, the fact that $\tr(xy - yx) = \tr(xy) - \tr(yx) = 0$ for \textit{all} $x,y \in \glalg_n(\KK)$ means that $\glalg_n(\KK)$'s Lie bracket is closed in $\slalg_n(\KK)$.
\end{proof}

Lastly, we will compute the dimension of $\slalg(V)$.
Firstly, it has to be strictly less than that of $\glalg(V)$'s, as it is a proper subalgebra of $\glalg(V)$.
Hence
\[
    \dim \slalg(V) < \dim \glalg(V) = (\ell+1)^2.
\]
So
\[
    \dim \slalg(V) \leq (\ell+1)^2 - 1 = \ell(\ell+2)
\]
However, we can explicitly name $\ell(\ell+2)$ linearly independent elements of $\slalg_n(\KK)$:
\begin{enumerate}
    \item 
        All the off-diagonal entries $e_{ij}$ where $i \neq j$--- there are $(\ell+1)^2 - (\ell + 1) = \ell^2 + \ell$ of these.
    \item 
        All of the elements $e_{ii} - e_{i+1,i+1}$, of which there are $(\ell + 1) -1 = \ell$. 
\end{enumerate}
So,
\[
    \dim \slalg(V) \geq \ell+2 + \ell + \ell = \ell(\ell + 2).
\]
And, putting it together, we have proven:
\begin{proposition}
    \[
        \dim A_\ell
        =
        \dim \slalg(V)
        =
        \dim \slalg_n(\KK)
        =
        \ell(\ell+2).
    \]
\end{proposition}

\subsubsection{Type \sfB: the odd-dimensional orthogonal algebra}

\begin{definition}
    The \defstyle{orthogonal algebra} $\oalg_{2\ell+1}(\KK)$ is defined to be
\end{definition}

\subsubsection{Type \sfC: the symplectic algebra}

\begin{definition}
    A \defstyle{symplectic form} on a vector space $V$ is a bilinear form $\omega$ such that
    \begin{enumerate}[label=(\alph*)]
        \item
            $\omega$ is bilinear,
        \item 
            $\omega(v,u) = -\omega(u,v)$, and
        \item 
            $\omega(v,u) = 0$ for all $v \in V$ implies that $u=0$.
    \end{enumerate}
\end{definition}

\begin{definition}[The type $\sfC_\ell$ Lie algebra]
    Let $\dim V = 2\ell$, and let $V$ be endowed with a symplectic form $\omega$.

    We define $\sfC_\ell$ to be the \defstyle{symplectic algebra} $\spalg(V)$, the set of all $x \in \End V$ such that
    \[
        \sfC_\ell
        \coloneq
        \spalg(V)
        \coloneq
        \Big\{
            x \in \glalg(V)
            :
            \omega\Big(x(\wc),\wc\Big)
            + \omega\Big(\wc,x(\wc)\Big)
            =
            0
        \Big\}
    \]
    In matrix form, we define
    \[
        \sfC_\ell
        \coloneq
        \spalg_{2\ell}(\KK)
        \coloneq
        \Big\{
            x \in \glalg_{2\ell}(\KK)
            :
            Jx + x^\top J = 0
        \Big\}
    \]
    where
    \[
        J
        =
        \begin{pmatrix}
            0 & I_\ell \\
            -I_\ell & 0
        \end{pmatrix}
    \]
    is the standard symplectic form on $\KK^{2\ell}$.
\end{definition}

\subsubsection{Type \sfD: the even-dimensional orthogonal algebra}

\begin{definition}[Type $\sfD$ Lie algebra]
    Let $\dim V = 2\ell$.
    We define $\sfD$ to be the \defstyle{orthogonal algebra} $\oalg(V)$, the set of all compatible bilinear transformations.
    \[
        \sfD_\ell
        \coloneq
        \oalg(V)
        \coloneq
        \Big\{
            x \in \glalg(V)
            :
            x + 
        \Big\}
    \]
\end{definition}

\subsection{Lie algebras from algebras}

\begin{definition}[Algebras over a field]
    Let $\KK$ be a field.
    An \defstyle{algebra over $\KK$}, or a \defstyle{$\KK$-algebra} is a $\KK$-vector space equipped with a bilinear product.

    We will use qualifiers like \textit{associative} and \textit{unital} to indicate that this product is associative and has unit respectively.
\end{definition}

Put another way, a unital associative algebra over a field is 
\begin{itemize}
    \item 
        a vector space with a compatible ring structure, (vector space + bilinear product)
    \item 
        or a ring with a compatible vector space structure.
        (ring + bilinear scaling map)
\end{itemize}
For example, $\Mat_n(\KK)$ is a unital associative algebra over $\KK$.

However, we don't in general expect algebras to have unit or to be associative--- $\RR^3$ with the cross product is neither unital nor associative.
Hence, the following is clear:

\begin{proposition}
    Lie algebras are algebras, with the product given by the Lie bracket.
\end{proposition}

To go along with this definition, we have notion of a homomorphism of algebras.

\begin{definition}
    An \defstyle{algebra homomorphism} $f: \scrA \to \scrB$ between two algebras $\scrA$ and $\scrB$ is a vector space homomorphism that respects the product, i.e
    \[
        f(xy) 
        = 
        f(x)f(y)
    \]
    for all $x, y \in \scrA$.

    We say that an algebra homomorphism is an \defstyle{algebra isomorphism} if it is also a vector space isomorphism.
\end{definition}

For example, the determinant is an algebra homomorphism from $\Mat_n(\KK)$ to $\KK$.

$\KK$-algebras can be turned into Lie algebras by defining the bracket $\lb{xy} \coloneq xy - yx$.

\begin{definition}
    Let $\scrA$ be a $\KK$-algebra.
    Then $\Lie[\scrA]$ is defined to be the Lie algebra whose underlying vector space is $\scrA$ and whose bracket is given by
    \[
        \lb{xy}
        \coloneq
        xy - yx
    \]
    for all $x, y \in \scrA$.
\end{definition}

We can check the following nice fact: 

\begin{proposition}
    Let $\scrA$ and $\scrB$ be two $\KK$-algebras, and let $\phi: \scrA \to \scrB$ be an algebra homomorphism.

    Then $\phi$ is also a \textit{Lie algebra homomorphism} (see Definition \ref{def:LieAlgHom}) between $\Lie[\scrA]$ and $\Lie[\scrB]$.
\end{proposition}
\begin{proof}
    \begin{align*}
        \phi\Big(\lb{xy}\Big)
        &=
        \phi(xy - yx)
        \\
        &=
        \phi(xy) - \phi(yx)
        \\
        &=
        \phi(x)\phi(y) - \phi(y)\phi(x)
        \\
        &=
        \lb{\phi(x)\phi(y)}.
    \end{align*}
\end{proof}

Hence $\Lie[\wc]$ is actually \textit{functorial}, with mapping of arrows given by the identity map.

What happens when we consider $\Lie[\frkg]$, where $\frkg$ is \textit{already} a Lie algebra?

Let the new bracket of $\Lie[\frkg]$ be denoted by $\lBrack \wc, \wc \rBrack$.
Then
\[
    \left\lBrack xy \right\rBrack
    =
    \lb{xy} - \lb{yx}
    =
    \lb{xy} + \lb{xy}
    =
    2\lb{xy}
\]
for all $x, y \in \frkg$.

Then $\lBrack \wc, \wc \rBrack = 2[\wc, \wc]$.
This fact actually characterizes Lie algebras.

\begin{proposition}
    Let $\scrA$ be a $\KK$-algebra with product $\ast$.
    If $\Lie[\scrA]$ has product $2\ast$, then $\scrA$ is a Lie algebra with bracket given by $\lb{xy} = x \ast y$.
\end{proposition}

\begin{proof}
    The product is bilinear by definition, so we have \ref{ax:LBIsBilinear}.

    Next, we check \ref{ax:LBNilpotent}:
    \[
        x \ast x
        =
        \frac{2(x \ast x)}{2}
        =
        \frac{\lb{xx}}{2}
        =
        0.
    \]
    And finally, in the exact same way, we check the Jacobi identity, \ref{ax:LBJacobiIdentity}:
    \[
        x \ast (y \ast z)
        + y \ast (z \ast x)
        + z \ast (x \ast y)
        =
        \frac{\lb{x\lb{yz}} + \lb{y\lb{xz}} + \lb{z\lb{xy}}}{4}
        =
        0.
    \]
\end{proof}

The number $2$ is special here--- this appears when we consider actions on the \textit{adjoint representation} of a Lie algebra $\frkg$.

\subsection{Derivations, the adjoint representation}

\begin{definition}
    Let $\scrA$ be a $\KK$-algebra.
    A \defstyle{derivation} of $\scrA$ is a linear map $\dd: \scrA \to \scrA$ which satisfies the \textit{Leibniz rule}:
    \[
        \dd(xy)
        =
        x(\dd y) + (\dd x) y.
    \]
    The collection of all derivations of $\scrA$ is denoted $\Der \scrA$.
\end{definition}

Derivations play nicely with the vector space structure of $\End \scrA$ as well as with the bracket inherited from $\glalg(\scrA)$.

\begin{proposition}
    Let $\scrA$ be a $\KK$-algebra.
    Then $\Der \scrA$ is a subspace of $\End \scrA$.
    Moreover, it is a subalgebra of $\glalg(\scrA)$
\end{proposition}

\begin{proof}
    If $\dd$ and $\dd'$ are two derivations, then
    \begin{align*}
        (a\dd + b\dd')(xy)
        &=
        (a\dd)(xy) + (b\dd')(xy)
        \\
        &=
        x(a\dd y) + (a\dd x)y
        +
        x(b\dd'y) + (b\dd'x)y
        \\
        &=
        x
        \Big(
            a\dd y + b\dd'y
        \Big)
        +
        \Big(
            a\dd x + b\dd'x
        \Big)
        y
        \\
        &=
        x(a\dd + b\dd')(y)
        +
        (a\dd + b\dd')(x)y.
    \end{align*}
    Hence $a\dd + b\dd' \in \Der \scrA$, so $\Der \scrA$ is a subspace of $\End \scrA$.

    Moreover, 
    \begin{align*}
        &\lb{\dd\dd'}(xy)
        \\
        &=
        (\dd\dd' - \dd'\dd)(xy)
        \\
        &=
        (\dd\dd')(xy) - (\dd'\dd)(xy)
        \\
        &=
        \dd
        \Big(
            x(\dd'y) + (\dd'x)y
        \Big)
        -
        \dd'
        \Big(
            x(\dd y) + (\dd x)y
        \Big)
        \\
        &=
        \dd\Big(x(\dd'y)\Big) 
        +
        \dd\Big((\dd'x)y\Big) 
        -
        \dd'\Big(x(\dd y)\Big) 
        -
        \dd'\Big((\dd x)y\Big) 
        \\
        &=
        x\dd\dd'y
        +
        \dd x \dd' y
        +
        \dd'x \dd y
        +
        \dd\dd'x y
        -
        x\dd'\dd y
        -
        \dd' x \dd y
        -
        \dd x \dd' y
        -
        \dd' \dd x y
        \\
        &=
        x\dd\dd'y
        +
        \dd\dd'x y
        -
        x\dd'\dd y
        -
        \dd' \dd x y
        \\
        &=
        x
        \Big(
            \dd\dd'y - \dd'\dd y
        \Big)
        +
        \Big(
            \dd\dd'x - \dd'\dd x 
        \Big)
        y
        \\
        &=
        x\Big((\dd\dd' - \dd'\dd)y\Big)
        +
        \Big((\dd\dd' - \dd'\dd)x\Big)y
        \\
        &=
        x\Big(\lb{\dd\dd'}y\Big)
        +
        \Big(\lb{\dd\dd'}x\Big)y.
    \end{align*}
    So $\Der \scrA$ is a subalgebra of $\glalg(\scrA)$.
\end{proof}

We have a special representation of \textit{any} Lie algebra, which is given by its action on itself.

\begin{definition}
    The \defstyle{adjoint representation} of a Lie algebra $\frkg$ is the mapping
    \begin{align*}
        \ad_\frkg:
        \frkg 
        &\to
        \Der \frkg
        \\
        x 
        &\mapsto 
        \ad_\frkg x
    \end{align*}
    where $\ad_\frkg x$ is defined to be the linear map
    \begin{align*}
        \ad_\frkg x: 
        \frkg 
        &\to 
        \frkg \\
        y 
        &\mapsto
        \lb{xy}.
    \end{align*}
    We will write $\ad x$ for $\ad_\frkg x$ unless there is any ambiguity.

    As a set, we define $\ad \frkg \coloneq \ad_\frkg(\frkg) \subseteq \glalg(\frkg)$.
\end{definition}

\begin{proposition}
    $\ad x$ is a derivation.
\end{proposition}
\begin{proof}
    We start with the Jacobi identity \ref{ax:LBJacobiIdentity}
    \[
        \lb{x\lb{yz}} + \lb{y\lb{zx}} + \lb{z\lb{xy}}
        =
        0,
    \]
    which, using the anticommutation relations $\lb{y\lb{zx}} = -\lb{y\lb{xz}}$ and $\lb{z\lb{xy}} = -\lb{\lb{xy}z}$, is equivalent to
    \[
        \lb{x\lb{yz}}
        =
        \lb{y\lb{xz}} + \lb{\lb{xy}{z}}.
    \]
    But this is saying that
    \[
        \ad x \rightharpoonup \lb{yz}
        =
        \lb{y, \ad x \rightharpoonup z}
        +
        \lb{\ad x \rightharpoonup y, z}
    \]
    which is exactly the defining identity for derivations.
\end{proof}


\subsection{Abstract Lie algebras}

\begin{definition}
    Let $\frkg$ be a Lie algebra, and fix some basis $\{x_1,\ldots,x_n\}$ of $\frkg$.
    We define $\frkg$'s \defstyle{structure constants} $a^k_{ij}$ relative to this basis to be the basis coefficients of the Lie brackets of basis elements--- the numbers such that
    \[
        \lb{x_ix_j}
        =
        \sum_{k=1}^n
        a^k_{ij}
        x_k.
    \]
\end{definition}

\begin{definition}
    An \defstyle{abelian} Lie algebra $\frkg$ is a Lie algebra with trivial bracket--- $\lb{xy} = 0$ for all $x,y \in \frkg$.
\end{definition}

\begin{proposition}
    Let $V$ be a vector space with basis $x_1,\ldots,x_n$, and let $a_{ij}^k$ be an array of structure coefficients.
    Then, the bracket defined by $a_{ij}^k$ gives $V$ a Lie algebra structure if and only if
    \[
        \begin{cases}
            a_{ii}^k = 0 \\
            a_{ij}^k + a_{ji}^k = 0 \\
            \sum_k
            a_{ij}^ka_{kl}^m
            + a_{jl}^ka_{ki}^m
            + a_{l}^ka_{kij}^m
            =
            0
        \end{cases}
    \]
    for any values of $i,j,k,l,m$.
\end{proposition}

We will classify all the Lie algebras of dimensions $1$ and $2$.

\begin{proposition}
    There are only two Lie algebras of dimension two up to isomorphism:
    \begin{enumerate}[label=(\alph*)]
        \item 
            The abelian two-dimensonal Lie algebra,
        \item 
            and the Lie algebra with basis $(x,y)$ and product $[x,y] = x$.
    \end{enumerate}
\end{proposition}

\begin{proof}
    If $\frkg$ is nonabelian, then $\lb{xy} = ax + by$, where at least one of $a, b$ is nonzero.
    Without loss of generality, let $a$ be nonzero.
    Then
    \[
        \lb{\lb{xy}y}
        =
        \lb{ax + by,y}
        =
        a\lb{xy}.
    \]
    Now put $u = \lb{xy}$ and $v = a^{-1}y$.
    Then
    \[
        \lb{uv}
        =
        \lb{\lb{xy},(a^{-1}y)}
        =
        \lb{xy}
        =
        u.
    \]
\end{proof}

\section{Ideals and homomorphisms}

\subsection{Ideals}

\begin{definition}
    A subspace $\frki$ of a Lie algebra $\frkg$ is called an \defstyle{ideal} of $\frkg$ if $\lb{xy} \in \frki$ for all $x \in \frkg$ and $y \in \frki$.
\end{definition}

The \defstyle{sum} and the \defstyle{bracket} of the ideals $\frki, \frkj$ are defined in the obvious way:
\[
    \frki + \frkj
    \coloneq
    \left\{
        x + y : x \in \frki, y \in \frkj
    \right\},\quad
    \lb{\frki,\frkj}
    \coloneq
    \left\{
        \sum_{i=0}^rc_i\lb{x_iy_i}
        :
        c_i \in \KK, x_i \in \frki, y_i \in \frkj
    \right\}.
\]

\begin{definition}
    The \defstyle{quotient of a Lie algebra} $\frkg$ by an ideal $\frki$, denoted $\frkg/\frki$, is defined to be the quotient of $\frkg$ as a vector space by $\frki$ as a subspace, equipped with the product
    \[
        \lb{x+\frki,y+\frki}
        \coloneq
        \lb{xy} + \frki.
    \]
\end{definition}

\begin{proposition}
    $\frkg/\frki$ is a Lie algebra.
\end{proposition}
\begin{proof}
    These are all easy to check.
    \begin{align*}
        \lb{ax+by+\frki,z+\frki}
        &=
        \Big(\lb{ax+by,z}\Big) + \frki
        \\\
        &=
        \Big(
            a\lb{x,z}  
            +
            b\lb{y,z}  
        \Big)
        + \frki
        \\
        &=
        \Big(
            a\lb{x,z} + \frki
        \Big)
        +
        \Big(
            b\lb{y,z} + \frki 
        \Big)
        \\
        &=
        a\lb{x+\frki,z+\frki} + b\lb{y+\frki,z+\frki}.
    \end{align*}
    \[
        \lb{x+\frki,x+\frki}
        =
        \lb{xx} + \frki
        =
        0 + \frki
    \]
\end{proof}

\newcommand{\barphi}{\ensuremath{\overline{\phi}}}

\subsection{Homomorphisms}

There is a natural definition of a Lie algebra homomorphism--- it's a map that respects brackets.

\begin{definition}
    \label{def:LieAlgHom}
    Let $\frkg$ and $\frkh$ be two Lie algebras.
    We say that a map $\phi: \frkg \to \frkh$ is a \defstyle{Lie algebra homomorphism} if it is a linear map for which
    \[
        \phi\Big(\lb{xy}\Big)
        =
        \lb{\phi(x)\phi(y)}
    \]
    for all $x,y \in \frkg$. 
    A \defstyle{Lie algebra isomorphism} is a Lie algebra homomorphism that is also an isomorphism of vector spaces.
\end{definition}


\begin{definition}
    A \defstyle{representation} of a Lie algebra $\frkg$ is a Lie algebra homomorphism $\frkg \to \glalg(V)$ where $V$ is some vector space.
\end{definition}

\subsection{Isomorphism theorems}

\begin{theorem}[Lie algebra isomorphism theorems]
    Let $\frkg$ and $\frkh$ be Lie algerbas.
    \begin{enumerate}[label=(\alph*)]
        \item \label{thm:FirstIsomorphismThm}
            If $\phi: \frkg \to \frkh$ is a homomorphism, then $\frkg / \ker \phi \simeq \im \phi$.
            If $\frki \subseteq \ker \phi$ is an ideal of $\frkg$, there exists a unique homomorphism $\barphi: \frkg/\frki \to \frkh$ that makes the following diagram commute:
            \[
                \begin{tikzcd}
                    \frkg \arrow[r, "\phi"] \arrow[d, "\pi"'] & \frkh \\ 
                                                             \frkg/\frki \arrow[ur, "\barphi"']
                \end{tikzcd}
            \]
        \item 
            If $\frka$ and $\frkb$ are ideals of $\frkg$ such that $\frkb \subseteq \frka$, then $\frka/\frkb$ is an ideal of $\frkg/\frkb$ and there is a natural isomorphism
            \[
                (\frkg/\frkb)/(\frka/\frkb)
                \simeq
                \frkg/\frka.
            \]
        \item 
            If $\frka, \frkb$ are ideals of $\frkg$, there is a natural isomorphism
            \[
                (\frka + \frkb)/\frkb
                \simeq
                \frka/(\frka \cap \frkb).
            \]
    \end{enumerate}
\end{theorem}

\begin{proof}
    \begin{enumerate}[label=(\alph*)]
        \item 
            The map
            \begin{align*}
                \barphi:
                \frkg/\ker\phi
                &\to
                \im \phi
                \\
                x + \ker \phi
                &\mapsto
                \phi(x)
            \end{align*}
            is the desired isomorphism $\frkg / \ker \phi \simeq \im \phi$.
            We verify that it is well defined: let $x + \ker \phi = x' + \ker \phi$.
            Then there exists $k, k' \in \ker \phi$ such that $x + k = x' + k'$, and we have that
            \[
                \phi(x)
                =
                \phi(x + k)
                =
                \phi(x + k')
                =
                \phi(x'),
            \]
            so $\barphi$ is a well-defined function on the cosets in $\frkg / \ker \phi$.

            Next, we check that it respects brackets:
            \begin{align*}
                \barphi
                \Big(
                    \lb{x+\ker\phi, y +\ker\phi}
                \Big)
                &=
                \barphi
                \Big(
                    \lb{xy} + \ker\phi
                \Big)
                \\
                &=
                \phi\Big(\lb{xy}\Big)
                \\
                &=
                \lb{\phi(x)\phi(y)}
                \\
                &=
                \lb{
                    \barphi\Big(x + \ker\phi\Big),
                    \barphi\Big(y + \ker\phi\Big)
                }.
            \end{align*}
            Then, it is a homomorphism.
            To show that it is an isomorphism, we note that it has a trival kernel, trivially:
            \[
                \ker\barphi
                =
                \{x + \ker \phi : x + \ker\phi = \ker \phi\}
                =
                \{0 + \ker \phi\}.
            \]
            Now, let $\frki$ be an ideal of $\frkg$ contained in $\ker \phi$.
            We define in a similar way
            \begin{align*}
                \barphi:
                \frkg/\frki
                &\to
                \im \phi
                \\
                x + \frki
                &\mapsto
                \phi(x),
            \end{align*}
            and via a similar argument as above, this map is well-defined. 
            It is moreover clear that $\barphi \circ \pi = \phi$ and that it is the only such homomorphism that has these properties.
        \item 
            Let $\frka$ and $\frkb$ be ideals of $\frkg$ such that $\frkb \subseteq \frka$.
            We define the map
            \begin{align*}
                \phi:
                \frkg/\frkb
                &\to
                \frkg/\frka
                \\
                x + \frkb
                &\mapsto
                x + \frka.
            \end{align*}
            This map is surjective.
            The kernel of this map is all the cosets $a + \frkb$, namely the ideal $\frka/\frkb$.
            Then, by \ref{thm:FirstIsomorphismThm}, 
            \[
                (\frkg/\frkb)(\frka/\frkb)
                =
                (\frkg/\frkb)/\ker\phi
                \simeq
                \im\phi
                =
                \frkg/\frka.
            \]
        \item 
            Let $\frka$ and $\frkb$ be ideals of $\frkg$.
            Define the map
            \begin{align*}
                \phi:
                \frka 
                &\to 
                (\frka+\frkb)/(\frkb)
                \\
                a
                &\mapsto
                a + \frkb.
            \end{align*}
            This map is surjective, as, if $(a + b) + \frkb \in (\frka+\frkb)/(\frkb)$, then
            \[
                \phi(a)
                =
                a + \frkb
                =
                a + (b + \frkb)
                =
                (a + b) + \frkb.
            \]
            Moreover, since
            \[
                \ker \phi
                =
                \frka \cap \frkb
            \]
            we have that, by \ref{thm:FirstIsomorphismThm} again,
            \[
                (\frka+\frkb)/\frkb
                =
                \im \phi
                \simeq
                \frka/\ker\phi
                =
                \frka/(\frka \cap \frkb).
            \]
    \end{enumerate}
\end{proof}

\begin{theorem}
    The adjoint representation $\ad: \frkg \to \glalg(\frkg)$ is a representation of $\frkg$.
\end{theorem}

\begin{proof}
    $\ad$ is evidently linear.
    Next, we just check that it is a homomorphism:
    \begin{align*}
        \lb{\ad x, \ad y} \rightharpoonup z
        &=
        \Big(\ad x \ad y - \ad y \ad x\Big) \rightharpoonup z 
        \\
        &=
        \Big(\ad x \ad y \rightharpoonup z\Big) - \Big(\ad y \ad x \rightharpoonup z\Big) 
        \\
        &=
        \Big(
            \ad x \rightharpoonup \lb{yz}
        \Big)
        - 
        \Big(
            \ad y \rightharpoonup \lb{xz}
        \Big)
        \\
        &=
        \lb{x\lb{yz}} - \lb{y\lb{xz}}
        \\
        &=
        \lb{x\lb{yz}} + \lb{y\lb{zx}}
        \\
        &=
        \lb{\lb{xy}z}
        \\
        &=
        \ad \lb{xy} \rightharpoonup z.
    \end{align*}
\end{proof}

\begin{corollary}
    Any simple Lie algebra is isomorphic to a linear Lie algebra.
\end{corollary}

\begin{proof}
    Let $\frkg$ be a Lie algebra.
    We have that
    \[
        \ker \ad 
        =
        \Big\{
            x \in \frkg: \ad x = 0
        \Big\}
        =
        \Big\{
            x \in \frkg: \lb{xy} = 0 \text{ for all } y \in \frkg
        \Big\}
        =
        Z(\frkg).
    \]
    Hence, if $\frkg$ is simple, i.e if $Z(\frkg) = 0$, then $\ad$ has a trivial kernel, so it is an isomorphism. 
\end{proof}

\section{Automorphisms}

\begin{definition}
    A \defstyle{automorphism} of a Lie algebra $\frkg$ is an isomorphism $\frkg \to \frkg$.
\end{definition}

\begin{proposition}
    Let $V$ be a vector space and let $g \in \GL(V)$.
    Then the map
    \[
        x \mapsto gxg^{-1}
    \]
    is an automorphism of $\glalg(V)$.
\end{proposition}
\begin{proof}
    The aforementioned map is a vector space isomorphism, with explicit inverse
    \[
        x \mapsto g^{-1}xg
    \]
    and it is a homomorphism, as
    \begin{align*}
        g\lb{xy}g^{-1}
        &=
        g\Big(xy - yx\Big)g^{-1}
        \\
        &=
        \Big(gxyg^{-1}\Big)
        -
        \Big(gyxg^{-1}\Big)
        \\
        &=
        \Big(gxg^{-1}gyg^{-1}\Big)
        -
        \Big(gyg^{-1}gxg^{-1}\Big)
        \\
        &=
        \lb{gxg^{-1},gyg^{-1}}.
    \end{align*}
\end{proof}

\section{Solvable and nilpotent Lie algebras}

\subsection{The derived series, solvability}

\begin{definition}
    The \defstyle{derived series} of a Lie algebra $\frkg$ is a sequence of ideals $\frkg^{(0)}, \frkg^{(1)}, \ldots$ defined
    \[
        \begin{cases}
            \frkg^{(0)} \coloneq \frkg \\
            \frkg^{(i)} \coloneq \lb{\frkg^{(i-1)}\frkg^{(i-1)}} \\
        \end{cases}.
    \]
\end{definition}

In other words, $\frkg^{(i)}$ is all those elements of $\frkg$ which can be written as linear combinations of $i$ ``full binary trees'' of brackets in $\frkg$.

\begin{definition}
    A Lie algebra $\frkg$ is said to be \defstyle{solvable} if $\frkg^{(n)} = 0$ for some $n$.
\end{definition}

For example, abelian Lie algebras are solvable, whereas simple Lie algebras are never solvable.

\begin{proposition}
    The Lie algebra of upper triangular matrices $\talg_n(\KK)$ is solvable.
\end{proposition}
\begin{proof}
    We use the following definition of an upper triangular matrix:
    \[
        (a_{ij}) \text{ is upper triangular }
        \iff
        a_{ij} = 0 \text{ if } j - i < 0.
    \]
    Let $(a_{ij})$ and $(b_{ij})$ be two upper triangular matrices, and let $j - i < 1$, then
    \begin{align*}
        (ab - ba)_{ij}
        &=
        (ab)_{ij}
        -
        (ba)_{ij}
        \\
        &=
        \sum_{k=1}^n
        a_{ik}b_{kj}
        -
        \sum_{k=1}^n
        b_{ik}a_{kj}
        \\
        &=
        \left(
            \sum_{k=1}^{i-1}
            a_{ik}b_{kj}
            +
            \sum_{k=i}^j
            a_{ik}b_{kj}
            +
            \sum_{k=j+1}^n
            a_{ik}b_{kj}
        \right)
        -
        \sum_{k=1}^n
        b_{ik}a_{kj}
        \\
        &=
        \left(
            \sum_{k=1}^{i-1}
            0 \cdot b_{kj}
            +
            \sum_{k=i}^j
            a_{ik}b_{kj}
            +
            \sum_{k=j+1}^n
            a_{ik} \cdot 0
        \right)
        -
        \sum_{k=1}^n
        b_{ik}a_{kj}
        \\
        &=
        \sum_{k=i}^j
        a_{ik}b_{kj}
        -
        \sum_{k=1}^n
        b_{ik}a_{kj}
        \\
        &=
        \sum_{k=i}^j
        a_{ik}b_{kj}
        -
        \sum_{k=i}^j
        b_{ik}a_{kj}
        \\
        &=
        \sum_{k=i}^j
        (a_{ik}b_{kj} - b_{ik}a_{kj})
        \\
        &=
        \begin{cases}
            0 & \text{if }j < i \\
            a_{jj}b_{jj} - b_{jj}a_{jj} & \text{if }j = i \\
        \end{cases}
        \\
        &=
        0.
    \end{align*}
    Hence, $(ab - ba)$ is \textit{strictly} upper triangular, so $\lb{ab} \in \nalg$.
    Then $\talg^{(1)} = \lb{\talg\talg} \subseteq \nalg$.

    Now suppose that, for some $l \geq 0$,
    \[
        (a_{ij}) \in \nalg^{(l)} 
        \implies
        a_{ij} = 0 \text{ if } j - i < m.
    \]
    Then, we can do a similar, in fact easier calculation to show that if $(a_{ij}), (b_{ij}) \in \talg^{(m)}$ and $j-i < 2m$.
    \[
        (ab - ba)_{ij}
        =
        \sum_{k=i+m}^{j-m}
        (a_{ik}b_{kj}-b_{ik}a_{kj})
        =
        0.
    \]
    Hence, we have shown that
    \[
        (a_{ij}) \in \talg^{(l+1)} 
        \implies
        a_{ij} = 0 \text{ if } j - i < 2m.
    \]
    Combined with our initial conditions, we have shown in general that
    \[
        (a_{ij}) \in \talg^{(l)} 
        \implies
        a_{ij} = 0 \text{ if } j - i < 2^l.
    \]
    Clearly, if $l$ is large enough, $(a_{ij})$ is forced to be the zero matrix.
    Hence $\nalg$ is solvable, as $\nalg^{(l)} = 0$ for some positive integer $l$.
    Then $\talg$ is also solvable, as $\talg^{(l+1)} \subseteq \nalg^{(l)} = 0$.
\end{proof}

\begin{theorem}
    Let $\frkg$ be a Lie algebra.
    \begin{enumerate}[label=(\alph*)]
        \item 
            If $\frkg$ is solvable, then so are all subalgebras and homomorphic images of $\frkg$.
        \item 
            If $\frki$ is a solvable ideal of $\frkg$ such that $\frkg/\frki$ is also solvable, then $\frkg$ is solvable.
        \item 
            If $\frki$, $\frkj$ are solvable ideals of $\frkg$, then so is $\frki + \frkj$.
    \end{enumerate}
\end{theorem}

\begin{proof}
    The first statement of (a) follows if we show that
    \[
        \frkh^{(i)} \subseteq \frkg^{(i)}
    \]
    for any subalgebra $\frkh$ of $\frkg$--- this is an easy induction.
    Similarly, the second statement of (a) follows from
    \[
        (\phi\frkg)^{(i)} 
        = 
        \phi\Big(\frkg^{(i)}\Big)
    \]
    for any homomorphism $\phi$.
    This is another easy induction.

    For (b), we stack together $\frkg/\frki$ and $\frki$'s solvability--- the former being solvable means that $\frkg^{(n)} \subseteq \frki$ for large enough $n$, but that means that $\frkg^{(i)}$ is a subalgebra of $\frki$, for which $\frki^{(m)} = 0$ for large enough $m$, so we can ``push in'' $\frkg$ further, namely
    \[
        \frkg^{(n+m)}
        =
        \Big(\frkg^{(n)}\Big)^{(m)}
        \subseteq
        \frki^{(m)}
        =
        0.
    \]


\end{proof}

\subsection{The descending central series, nilpotency}

\begin{definition}
    The \defstyle{descending central series} of a Lie algebra $\frkg$ is a sequence of ideals $\frkg^0, \frkg^1, \ldots$ defined to be
    \[
        \begin{cases}
            \frkg^0 \coloneq \frkg \\
            \frkg^i \coloneq \lb{\frkg\frkg^{i-1}} \\
        \end{cases}.
    \]
\end{definition}

\begin{definition}
    A Lie algebra $\frkg$ is said to be \defstyle{nilpotent} if $\frkg^n = 0$ for some $n$.
\end{definition}

\begin{proposition}
    All nilpotent Lie algebras are solvable.
\end{proposition}

\begin{definition}
    Let $\frkg$ be a Lie algebra.
    We say that $x \in \frkg$ is \defstyle{ad-nilpotent} if $(\ad x)^n = 0$ for some $n$.
\end{definition}

\begin{theorem}
    Let $\frkg$ be a Lie algebra.
    \begin{enumerate}[label=(\alph*)]
        \item 
        \item 
        \item 
    \end{enumerate}
\end{theorem}

\subsection{Engel's theorem}

We will prove \defstyle{Engel's theorem}.

\begin{theorem}[Engel]
    Let $\frkg$ be a Lie algebra.
    Then the following are equivalent:
    \begin{enumerate}[label=(\roman*)]
        \item 
            $\frkg$ is nilpotent.
        \item 
            All the elments of $\frkg$ are $\ad$-nilpotent.
    \end{enumerate}
\end{theorem}

We will prove the following equivalent theorem:

\begin{theorem}
    Let $\frkg$ be a subalgebra of $\glalg(V)$, where $V$ has positive dimension.
    If $\frkg$ consists only of nilpotent transformations, then there exists a nonzero vector $v \in V$ so that $\frkg \rightharpoonup v = 0$.
\end{theorem}

\begin{proof}
    We induct on $\dim \frkg$.

    The $\dim \frkg = 0$ case is trivial--- $\frkg$ will only contain the zero transformation.

    The $\dim \frkg = 1$ case is also easy.
    Let $x \in \frkg$ be nonzero and nilpotent.
    Then  we can find a nonzero vector $v \in V$ so that $x \rightharpoonup v = 0$, and so $\frkg \rightharpoonup v = \KK x \rightharpoonup v = 0$.

    Now supppose $\dim \frkg > 1$.
    Let $\frkh$ be a proper subalgebra of $\frkg$ of positive dimension.
    Then,
    \[
        \ad \frkg/\frkh
        \coloneq
        \Big\{
            \ad_{\frkg/\frkh}(x + \frkh)
            :
            x \in \frkg
        \Big\}
    \]
    is a Lie algebra--- it is the homomorphic image of $\frkg$ under the composition
    \[
        \begin{tikzcd}
            \frkg \arrow[r, "\pi", two heads] & \frkg/\frkh \arrow[r, "\ad", two heads] & \ad \frkg/\frkh. 
        \end{tikzcd}
    \]
    Moreover, 
    \[
        \dim \frkg > \dim \frkg/\frkh \geq \dim \ad \frkg/\frkh,
    \]
    as $\frkh$ has positive dimension.
    By the inductive hypothesis, we may find a nonzero vector $x + \frkh \in \frkg/\frkh$ such that
    \[
        \ad \frkg/\frkh \rightharpoonup (x + \frkh) 
        = 
        0 + \frkh = \frkh.
    \]
    This means that
    \begin{align*}
        \lb{hx} + \frkh
        &=
        \lb{h + \frkh, x + \frkh}
        \\
        &=
        \ad_{\frkg/\frkh} (h + \frkh) \rightharpoonup (x + \frkh) 
        \\
        &= 
        \frkh
    \end{align*}
    for all $h \in \frkh$, so $x \in N_\frkg(\frkh)$.

    But $x + \frkh$ being nonzero in $\frkg/\frkh$ means exactly that $x \notin \frkh$, so $\frkh \subsetneq N_\frkg(\frkh)$.
    We will use this fact to produce a nontrivial maximal ideal of $\frkg$.

    We are always able to find a proper subalgebra of positive dimension--- choose the span of any single element in $\frkg$. 
    Then, there must exist maximal proper subalgebras. 
    Let $\frkh$ be maximal now. 
    Then we have that $N_\frkg(\frkh) = \frkg$, as otherwise $N_\frkg(\frkh)$ is a larger proper subalgebra of $\frkg$.

    Hence, $\frkh$ is an ideal of $\frkg$.
    We will show that it has codimension one.
    Suppose it has codimension at least two.
    Then, we can pull back a one-dimensional subalgebra of the quotient $\frkg / \frkh$ along the projection map and obtain a proper subalgebra of $\frkg$ that properly contains $\frkh$, which is impossible.

    Now, consider the subspace $W = \{ v \in V : \frkh \rightharpoonup v = 0 \}$ of $V$.
    Since $\frkh$ is an ideal of $\frkg$, $\frkg$ stabilizes $W$--- for all $g \in \frkg$, $h \in \frkh$, and $w \in W$, we have that
    \begin{align*}
        h \rightharpoonup g \rightharpoonup w
        &=
        hg \rightharpoonup w
        \\
        &= 
        \Big(gh - \lb{gh}\Big) \rightharpoonup w
        \\
        &=
        \Big( g \rightharpoonup \underbrace{h \rightharpoonup w}_{=0} \Big) + \Big(\underbrace{\lb{hg}}_{\in \frkh} \rightharpoonup w\Big)
        \\
        &=
        \Big( g \rightharpoonup 0 \Big) + 0
        \\
        &=
        0,
    \end{align*}
    hence $\frkg \rightharpoonup W = W$.

    Then, if we pick $g \in \frkg$ and restrict it to $W$, we have a nilpotent endomorphism of $W$, hence $g$ has an eigenvector $v$ in $W$.

    Then, $(\frkh + \KK g) \rightharpoonup v = 0$, completing the theorem.
\end{proof}

Now, we can prove Engel's theorem:

\begin{proof}[Proof of Engel's theorem]
    As before, the $\dim \frkg = 0$ and $\dim \frkg = 1$ cases are trivial.
    So, we induct on $\dim \frkg$.

    Let $\frkg$ be a Lie algebra whose elements are all ad-nilpotent.

    Then $\ad \frkg$ is a subalgebra of $\glalg(\frkg)$ consisting of nilpotent transformations, hence there exists a nonzero vector $x \in \frkg$ such that $\ad \frkg \rightharpoonup x = 0$.

    But, from the definition of $\ad$, this means that $\lb{\frkg x} = 0$, hence $x \in Z(\frkg)$, so $Z(\frkg)$ has positive dimension, and $\dim \frkg / Z(\frkg) < \dim \frkg$.

    Now, we want to show that $\frkg/Z(\frkg)$ consists of ad-nilpotent elements.
    This follows from the observation that
    \begin{align*}
        \ad \Big(x + Z(\frkg)\Big) \rightharpoonup \Big(y + Z(\frkg)\Big)
        &= 
        \lb{x + Z(\frkg), y + Z(\frkg)} 
        \\
        &= \lb{xy} + Z(\frkg) 
        \\
        &= (\ad x \rightharpoonup y) + Z(\frkg),
    \end{align*}
    hence it easily follows that $\ad \Big(x + Z(\frkg)\Big)$ is nilpotent given that $\ad x$ is nilpotent.
    
    Then, by the induction hypothesis, $\frkg/Z(\frkg)$ is a nilpotent Lie algebra.

    By Theorem, $\frkg$ is a nilpotent Lie algebra, completing the proof.
\end{proof}

\begin{corollary}
\end{corollary}

\section{Semisimple Lie algebras}

\subsection{Lie's theorem}

Similar to Engel's theorem, which concerned \textit{nilpotent} Lie algebras, we have \defstyle{Lie's theorem}, which concerns \textit{solvable} Lie algebras.

\begin{theorem}[Lie's theorem]
    Let $\frkg$ be a solvable subalgebra of $\glalg(V)$.
    Then $\frkg$ stabilizes some flag in $V$.
\end{theorem}

In other words, relative to some basis of $V$, the matrix representation of all elements of $\frkg$ are upper triangular.

Again, we will prove it by proving an equivalent formulation in terms of the existence of a common eigenvector.

\begin{theorem}
    Let $\frkg$ be a solvable subalgebra of $\glalg(V)$.
    Then there exists $v \in V$ that is an eigenvector for all $x \in \frkg$.
\end{theorem}

\subsection{Cartan's criterion}

\begin{theorem}[Cartan's criterion]
\end{theorem}

\subsection{Killing form}

\begin{definition}
    The \defstyle{Killing form}
\end{definition}

\subsection{\texorpdfstring{$\frkg$}{g}-modules}

\begin{definition}
    Let $\frkg$ be a Lie algebra.
    A \defstyle{$\frkg$-module} is a vector space $V$ equipped with a \textit{scaling map}
    \begin{align*}
        \wc . \wc:
        \frkg \times V
        &\to
        V
        \\
        (x, v)
        &\mapsto
        x.v
    \end{align*}
    which satisfies the following axioms:
    \begin{enumerate}[label=(M\arabic*)]
        \item \label{ax:ModuleLeftDistribute}
            $(ax + by).v  = ax.v + by.v$,
        \item \label{ax:ModuleRightDistribute}
            $x.(av + bw) = ax.v + bx.w$,
        \item \label{ax:ModuleBracket}
            $\lb{xy}.v = x.y.v - y.x.v$.
    \end{enumerate}

\end{definition}

\begin{proposition}
    $\frkg$-modules are in one-to-one correspondence with representations of $\frkg$.
\end{proposition}

\begin{proof}
    Let $V$ be a vector space, and let $\frkg$ be a Lie algebra.
    We will demonstrate a correspondence between $\frkg$-module structures on $V$ and representations of $\frkg$ in $\glalg(V)$.

    Let $\phi: \frkg \to \glalg(V)$ be a representation of $\frkg$.

    Define a $\frkg$-module structure on $V$ by
    \[
        x.v
        \coloneq
        \phi(x) \rightharpoonup v.
    \]
    Then, \ref{ax:ModuleLeftDistribute} and \ref{ax:ModuleRightDistribute} follow easily from the fact that $\phi(x) \in \glalg(V)$.

    Then, the fact that $\phi$ is a Lie algebra homomorphism shows \ref{ax:ModuleBracket}, as
    \begin{align*}
        \lb{xy}.v
        &=
        \phi\Big(\lb{xy}\Big) \rightharpoonup v
        \\
        &=
        \lb{\phi(x)\phi(y)} \rightharpoonup v
        \\
        &=
        \Big(
            \phi(x)\phi(y) - \phi(y)\phi(x)
        \Big)
        \rightharpoonup v
        \\
        &=
        \Big(
            \phi(x) \rightharpoonup \phi(y) \rightharpoonup v
        \Big) 
        -
        \Big(
            \phi(y) \rightharpoonup \phi(x) \rightharpoonup v
        \Big) 
        \\
        &=
        x.y.v - y.x.v.
    \end{align*}
    Conversely, suppose that $V$ has a $\frkg$-module structure.
    Then for all $x \in \frkg$ we can define $\phi(x) \in \End V$ by
    \[
        \phi(x) \rightharpoonup v
        \coloneq
        x.v.
    \]
\end{proof}


\begin{theorem}[Schur's lemma]
\end{theorem}

\subsection{Weyl's theorem}

\begin{theorem}[Weyl's theorem]
    If $\frkg$ is semisimple Lie algebra, then any representation of $\frkg$ is completely reducible.
\end{theorem}

\section{Representations of \texorpdfstring{$\slalg_2$}{sl2} and the root space decomposition}

\begin{theorem}
\end{theorem}

\section{Root systems}

\begin{definition}
\end{definition}

\section{Appendix}

\subsection{Definitions}

\begin{definition}
    \label{def:Iverson}
    Let $\psi$ be some statement that can be evaluated to be true or false.
    The \defstyle{Iverson bracket} of $\psi$ is
    \[
        [\psi]^? \coloneq \begin{cases}
            1, & \text{if }\psi\text{ is true} \\
            0, & \text{otherwise}.
        \end{cases}
    \]
    a function of the free variables of $\psi$.
\end{definition}

\subsection{Some linear algebra}

I never really got a chance to learn much foundational \textit{abstract} linear algebra.
Learning this material was a great way for me to brush up on a lot of this stuff, so here's a short dump of some important results.

\subsubsection{Definitions}

\begin{definition}
    The \defstyle{endomorphism ring} $\End V$ of the vector space $V$ is the collection of all linear maps from $V$ to itself.

    If $T \in \End V$ and $v \in V$, we will write $T \rightharpoonup v$ to denote $T(v)$.
\end{definition}

\begin{definition}
    Let $\KK$ be a field.
    The \defstyle{$\mathbf{n}\times\mathbf{n}$ matrix ring} $\Mat_n(\KK)$ is defined to be the ring whose underlying set is $\KK^{n \times n}$ with pointwise scaling and addition, and with product given by matrix multiplication.
\end{definition}

\begin{definition}
    Let $V$ be a vector space over the field $\KK$.
    The \defstyle{dual space} $V^\ast$ of $V$ is the collection of all linear maps $V \to \KK$.
\end{definition}

\subsubsection{Rank-nullity}

\begin{theorem}[\defstyle{Rank-nullity}]
    \label{thm:RankNullity}
    Let $x \in \End V$.
    xhen
    \[
        \rank x + \nullity x
        =
        \dim V,
    \]
    where
    \[
        \rank x
        \coloneq
        \dim \im x
        ,\qquad
        \nullity x
        \coloneq
        \dim \ker x.
    \]
\end{theorem}

\begin{proof}
    Let $n = \dim V$, $r = \rank x$ and let $\ell = \nullity x$.

    Let $\bfp=(p_1,p_2,\ldots,p_\ell)$ be a basis for $\ker x$.

    We may extend this into a basis of $V$ by adjoining more vectors $\bfq=(q_{\ell+1},\ldots,q_n)$, so that $(\bfp,\bfq) = (p_1,\ldots,p_\ell,q_{\ell+1},\ldots,q_n)$ is a basis of $V$.

    Then, we claim that
    \[
        x \rightharpoonup \bfq
        =
        \Big(
            x \rightharpoonup q_{\ell+1},
            \cdots
            x \rightharpoonup q_n,
        \Big)
    \]
    is a basis for $\im x$.
    We first show that it spans $\im x$: let $v \in V$, then $v = a_1p_1 + \cdots + a_\ell p_\ell + a_{\ell+1} q_{\ell+1} + \cdots + a_nq_n$.

    So
    \begin{align*}
        x \rightharpoonup v
        &=
        x \rightharpoonup 
        \Big(
            a_1p_1 + \cdots + a_\ell p_\ell + a_{\ell+1} q_{\ell+1} + \cdots + a_n q_n
        \Big)
        \\
        &=
        \underbrace{
            \Big(
                x \rightharpoonup a_1p_1 + \cdots + a_\ell p_\ell
            \Big)
        }_{=0}
        + 
        \Big(
            x \rightharpoonup a_{\ell+1} q_{\ell+1} + \cdots + a_n q_n
        \Big)
        \\
        &=
        x \rightharpoonup 
        \Big(
            a_{\ell+1} q_{\ell+1} + \cdots + a_n q_n
        \Big)
        \\
        &=
        a_{\ell+1}\Big(
            x \rightharpoonup q_{\ell+1}
        \Big)
        + \cdots
        + a_n\Big(
            x \rightharpoonup q_n
        \Big).
    \end{align*}
    Hence $x \rightharpoonup v$ is in the span of $x \rightharpoonup \bfq$.
    Next, we show that it is linearly independent--- suppose that there existed $a_{\ell+1}, \ldots, a_n$ such that
    \[
        a_{\ell+1}\Big(
            x \rightharpoonup q_{\ell+1}
        \Big)
        + \cdots
        + a_n\Big(
            x \rightharpoonup q_n
        \Big)
        \neq
        0.
    \]
    But this means that
    \[
        x \rightharpoonup 
        \Big(
            a_{\ell+1} q_{\ell+1} + \cdots + a_n q_n
        \Big)
        \neq
        0,
    \]
    and so the vector $a_{\ell+1} q_{\ell+1} + \cdots + a_n q_n$ is in the kernel of $x$, however it is not in the kernel of $x$ because it is not in the span of $\bfp$, a contradiction.

    Hence $x \rightharpoonup \bfq$ is linearly independent, completing our assertion that it is a basis of $\im x$.

    Then $r = \dim \im x = n - \ell$, and so
    \[
        r + \ell 
        = 
        n,
    \]
    which proves the theorem.
\end{proof}

\begin{corollary}
    Let $x \in \End V$.
    The following are equivalent:
    \begin{enumerate}[label=(\alph*)]
        \item
            $x$ is injective.
        \item
            $x$ is surjective.
        \item
            $x$ is bijective.
    \end{enumerate}
\end{corollary}

\begin{proof}
    We have the easily verifiable propositions:
    \[
        \dim \ker x = 0 \iff x \text{ is injective}
    \]
    \[
        \dim \im x = \dim V \iff x \text{ is surjective}
    \]
    And, by rank nullity,
    \[
        \dim \ker x = 0
        \iff
        \dim \im x = \dim V,
    \]
    hence $x$ is injective if and only if it is surjective.
\end{proof}

\subsubsection{The matrix representation}

We recall the definition of a tensor product:

\begin{definition}
    Let $V$ and $W$ be two $\KK$-vector spaces with bases $\bfv = (v_1,\ldots,v_n)$ and $\bfw = (w_1,\ldots,w_m)$ respectively.

    The \defstyle{tensor product of vector spaces} $V \otimes W$, is the $\KK$-vector space with basis
    \[
        \Big\{
            v_i \otimes w_j: 1 \leq i \leq n, 1 \leq j \leq m
        \Big\}.
    \]
\end{definition}

As a structure, there isn't really ``anything happening'' with this construction.
The following definition makes 

\begin{definition}
    Let $V, W$ be $\KK$-vector spaces as before.

    Let $v = a_1v_1 + \cdots + a_nv_n \in V$ and $w = b_1w_1 + \cdots + b_mw_m \in W$.
    The \defstyle{tensor product of vectors} $v \otimes w$ is defined 
    \[
        v \otimes w
        =
        \left(
            \sum_{i=1}^n
            a_iv_i
        \right)
        \otimes
        \left(
            \sum_{j=1}^n
            b_jw_j
        \right)
        \coloneq
        \sum_{i=1}^n
        \sum_{j=1}^m
        a_ib_j (v_i \otimes w_j).
    \]
    This defines a map $i: V \times W \to V \otimes W$ given by $(v, w) \mapsto v \otimes w$.
\end{definition}

Together, this pair of constructions satisfies a \textit{universal property}:

\begin{theorem}
    If $U$ and $V$ are two $\KK$-vector spaces, then any bilinear map $f: U \times V \to W$ factors through $\otimes: U \times V \to U \otimes V$--- there exists a unique linear map $\overline{f}$ that makes the following diagram commute:
    \[
        \begin{tikzcd}
            U \times V \arrow[rd, "f"] \arrow[d, "i"'] \\ 
            U \otimes V \arrow[r, "\overline{f}"'] & W
        \end{tikzcd}
    \]
\end{theorem}

\begin{proof}
    Fix bases $\bfu = (u_1, \ldots, u_n)$ and $\bfv = (v_1, \ldots v_n)$ of $U$ and $V$.

    Let $u = a_1u_1 + \cdots + a_nu_n$ and $v = b_1v_1 + \cdots + b_mv_m$.

    Then, by bilinearity,
    \begin{align*}
        f(u, v)
        &=
        f\Big(
            a_1u_1 + \cdots + a_nu_n,
            b_1v_1 + \cdots + b_mv_m
        \Big)
        \\
        &=
        \sum_{i=1}^n
        a_i
        \cdot
        f\Big(
            u_i,
            b_1v_1 + \cdots + b_mv_m
        \Big)
        \\
        &=
        \sum_{i=1}^n
        \sum_{j=1}^m
        a_i
        b_j
        \cdot
        f(u_i,v_j).
    \end{align*}
    Hence, $f$ is completely determined by its values $f(u_i,v_j)$ where $1 \leq i \leq n$, $1 \leq j \leq m$.
    Conversely, any array $w_{ij} \in V$ defines a bilinear map by putting $(u_i,v_j) \mapsto w_{ij}$.

    Pick some $i,j$.
    If $f = \overline{f} \circ i$, it must be that
    \[
        f(u_i, v_j)
        =
        \Big(\overline{f} \circ i\Big)(u_i, v_j)
        =
        \overline{f}(u_i \otimes v_j).
    \]
\end{proof}

\begin{definition}
    Let $U, V$ be $\KK$-vector spaces, and let $f: U \to U$ and $g: V \to V$ be linear maps.
    We define the \defstyle{tensor product of linear maps} $f \otimes g$ to be the map
    \begin{align*}
        f \otimes g:
        U \otimes V
        &\to
        U \otimes V
        \\
        \sum_i
        u_i \otimes v_i
        &\mapsto
        \sum_i
        f(u_i) \otimes g(v_i).
    \end{align*}
\end{definition}

\begin{definition}
    Let $V$ be a vector space over $\KK$ and fix a basis $\bfv = (v_1,\ldots,v_n)$ of $V$ with a dual basis $\bfv^\ast = (v^1, \ldots, v^n)$ of the dual space $V^\ast$.

    By abuse of notation, we define the corresponding elements
    \[
        \bfv 
        \coloneq 
        \sum_{i=1}^n 
        e^i \otimes v_i 
        \in (\KK^n)^\ast \otimes V
        ,\qquad
        \bfv^\ast 
        \coloneq 
        \sum_{i=1}^n v^i \otimes e_i 
        \in V^\ast \otimes \KK^n
    \]
    for the basis $\bfv$ and dual basis $\bfv^\ast$.
\end{definition}

\begin{definition}
    Let $U, V, W$ be vector spaces with bases $\bfu$, $\bfv$, $\bfw$, then we define a product
    \begin{align*}
        (U^\ast \otimes V) \times (V^\ast \otimes W)
        &\to
        U^\ast \otimes W
        \\
        (u^i \otimes v_j)(v^k \otimes w_l)
        &\mapsto
        v^kv_j
        (u^i \otimes w_l).
    \end{align*}
\end{definition}

\begin{theorem}
    Let $V$ be a vector space over $\KK$ of dimension $n$.
    Then
    \[
        \End V
        \simeq
        V^\ast \otimes V
        \simeq
        M_n(\KK).
    \]
\end{theorem}

\begin{proof}
    Fix a basis $\bfv$ and dual basis $\bfv^\ast$ of $V$.

    Now, if $U$ is a vector space and $x \in \End V$, it has a linear action on $U^\ast \otimes V$ given by $u^i \otimes v_j \mapsto u^i \otimes xv_j$.

    Now, we claim that the map $T \mapsto \bfv^\ast T \bfv$ is the desired isomorphism between $\End V$ and $V^\ast \otimes V$.

    Then, the map $v^i \otimes v_j \mapsto e_{ij}$ provides the isomorphism between $V^\ast \otimes V$ and $M_n(\KK)$.
\end{proof}

\subsubsection{Change of basis}

\begin{proposition}
    If $\bfv$ and $\bfw$ are two bases of $V$, then
    \[
        \bfv\bfw^\ast \in \KK^n \otimes \KK^n
    \]
    encodes the change of basis matrix expressing coordinates in $\bfv$ as coordinates in $\bfw$.

    Similarly,
    \[
        \bfw^\ast\bfv \in V^\ast \otimes V
    \]
    encodes the linear map $v_i \mapsto w_i$.
\end{proposition}
\begin{proof}
    Define the array $S_{ij}$ to be the numbers for which
    \[
        v_i
        =
        \sum_{j=1}^n
        S_{ij}w_j.
    \]
    Then
    \begin{align*}
       \bfv \bfw^\ast
       &=
       \left(
           \sum_{i=1}^n
           e^i \otimes v_i
       \right)
       \left(
           \sum_{j=1}^n
           w^j \otimes e_j
       \right)
       \\
       &=
       \sum_{i=1}^n
       \sum_{j=1}^n
       w^j(v_i)
       (e_i \otimes e_j)
       \\
       &=
       \sum_{i=1}^n
       \sum_{j=1}^n
       S_{ij}
       (e_i \otimes e_j).
    \end{align*}

    Now, consider the map $T \in \End V$ given by $w_i \mapsto v_i$.
    Then
    \begin{align*}
        \bfw^\ast T \bfw
        &=
        \sum_{i=1}^n
        w^i \otimes (T \rightharpoonup w_i)
        \\
        &=
        \sum_{i=1}^n
        w^i \otimes v_i
        \\
        &=
        \sum_{i=1}^n
        \sum_{j=1}^n
        \delta_{ij}
        (w^i \otimes v_j)
        \\
        &=
        \sum_{i=1}^n
        \sum_{j=1}^n
        (e^i \rightharpoonup e_j)
        (w^i \otimes v_j)
        \\
        &=
        \left(
            \sum_{i=1}^n
            w^i \otimes e_i
        \right)
        \left(
            \sum_{j=1}^n
            e^j \otimes v_j
        \right)
        \\
        &=
        \bfw^\ast\bfv.
    \end{align*}
\end{proof}



\subsubsection{Trace}

\begin{definition}
    Let $V$ be a vector space with basis $\bfv = (v_1,\ldots,v_n)$
    The \defstyle{trace} $\tr x$ of an endomorphism $x \in \End V$ of $V$ is defined to be the sum
    \[
        \sum_{i=1}^n 
        v^i\Big(x(v_i)\Big).
    \]
\end{definition}

\begin{proposition}
    $\tr x = \sum_{i=1}^n (\bfv^\ast x \bfv)_{ii}$
\end{proposition}

\begin{theorem}
    The trace is a linear operator, i.e if $x, y \in \End V$ and $a,b \in \KK$,
    \[
        \tr(ax + by)
        =
        a\tr x + b \tr y.
    \]
\end{theorem}
\begin{proof}
    \begin{align*}
        \tr (ax + by)
        &=
        \sum_{i=1}^n 
        v^i\Big((ax + by)(v_i)\Big)
        \\
        &=
        \sum_{i=1}^n 
        v^i\Big(ax(v_i) + by(v_i)\Big)
        \\
        &=
        \sum_{i=1}^n 
        av^i\Big(x(v_i)\Big)
        + bv^i\Big(y(v_i)\Big)
        \\
        &=
        a
        \sum_{i=1}^n 
        v^i\Big(x(v_i)\Big)
        + 
        b
        \sum_{i=1}^n 
        v^i\Big(y(v_i)\Big)
        \\
        &=
        a \tr x + b \tr y.
    \end{align*}
\end{proof}

\begin{theorem}
    Let $V$ be a vector space.

    For all $x, y \in \End V$, $\tr(xy) = \tr(yx)$.
\end{theorem}

\begin{proof}
    Fix a basis $\bfv = (v_1, \ldots, v_n)$ of $V$.
    \begin{align*}
        \tr (xy)
        &=
        \sum_{i=1}^n 
        v^i \rightharpoonup xy \rightharpoonup v_i
        \\
        &=
        \sum_{i=1}^n 
        \sum_{j=1}^n
        \Big(
            v^i \rightharpoonup x \rightharpoonup v_j
        \Big) 
        \Big(
            v^j \rightharpoonup y \rightharpoonup v_i
        \Big)
        \\
        &=
        \sum_{j=1}^n
        \sum_{i=1}^n 
        \Big(
            v^j \rightharpoonup y \rightharpoonup v_i
        \Big)
        \Big(
            v^i \rightharpoonup x \rightharpoonup v_j
        \Big) 
        \\
        &=
        \sum_{i=1}^n 
        v^i \rightharpoonup yx \rightharpoonup v_i
        \\
        &=
        \tr (yx).
    \end{align*}
\end{proof}

\begin{theorem}\label{thm:TraceIsBasisIndependent}
    The trace of a linear operator $x \in \End V$ is basis invariant--- its value is independent of the basis used to compute it.
\end{theorem}

\begin{proof}
    Let $\bfv$ and $\bfw$ be two bases of $\End V$.
    Then
    \[
        \tr (\bfv^\ast\bfw x \bfw^\ast\bfv)
        =
        \tr (\bfw^\ast \bfv \bfv^\ast \bfw x)
        =
        \tr x.
    \]
\end{proof}

\end{document}
